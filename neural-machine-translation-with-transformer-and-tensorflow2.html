<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="本文向讀者介紹近年深度學習在機器翻譯任務上的一些重要模型及概念。我們也會實際使用 TensorFlow 2 來一步步實作一個可以將英文句子翻譯成中文的 Transformer。Transformer 是一個基於注意力機制的強大神經網路架構，透過實作並瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等任務之上並達到卓越的成果。" />
        <meta name="keywords" content="自然語言處理, NLP, Tensorflow" />
        <meta name="tags" content="自然語言處理" />
        <meta name="tags" content="NLP" />
        <meta name="tags" content="Tensorflow" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="/theme/images/background/Tour_de_babel.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="/neural-machine-translation-with-transformer-and-tensorflow2.html" />
        <meta property="og:title" content="淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯" />
        <meta property="og:description" content="本文向讀者介紹近年深度學習在機器翻譯任務上的一些重要模型及概念。我們也會實際使用 TensorFlow 2 來一步步實作一個可以將英文句子翻譯成中文的 Transformer。Transformer 是一個基於注意力機制的強大神經網路架構，透過實作並瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等任務之上並達到卓越的成果。" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="/theme/css/toc-nav.css">

    <link rel="stylesheet" type="text/css" href="/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="/theme/js/modernizr.js"></script>
    <script src="/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯">淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯</a><ul><li><a class="toc-href" href="#Experiment" title="Experiment">Experiment</a></li><li><a class="toc-href" href="#Gdrive" title="Gdrive">Gdrive</a></li><li><a class="toc-href" href="#Setup-input-pipeline" title="Setup input pipeline">Setup input pipeline</a></li><li><a class="toc-href" href="#Positional-encoding" title="Positional encoding">Positional encoding</a></li><li><a class="toc-href" href="#Masking" title="Masking">Masking</a><ul><li><a class="toc-href" href="#TODO" title="TODO">TODO</a></li></ul></li><li><a class="toc-href" href="#Scaled-dot-product-attention_1" title="Scaled dot product attention">Scaled dot product attention</a></li><li><a class="toc-href" href="#Multi-head-attention" title="Multi-head attention">Multi-head attention</a></li><li><a class="toc-href" href="#Point-wise-feed-forward-network" title="Point wise feed forward network">Point wise feed forward network</a></li><li><a class="toc-href" href="#Encoder-and-decoder" title="Encoder and decoder">Encoder and decoder</a><ul><li><a class="toc-href" href="#Encoder-layer" title="Encoder layer">Encoder layer</a></li><li><a class="toc-href" href="#Decoder-layer" title="Decoder layer">Decoder layer</a></li><li><a class="toc-href" href="#Encoder" title="Encoder">Encoder</a></li><li><a class="toc-href" href="#Decoder" title="Decoder">Decoder</a></li></ul></li><li><a class="toc-href" href="#Create-the-Transformer_1" title="Create the Transformer">Create the Transformer</a></li><li><a class="toc-href" href="#Set-hyperparameters" title="Set hyperparameters">Set hyperparameters</a><ul><li><a class="toc-href" href="#Setup-experiment-path" title="Setup experiment path">Setup experiment path</a></li></ul></li><li><a class="toc-href" href="#Optimizer_1" title="Optimizer">Optimizer</a></li><li><a class="toc-href" href="#Loss-and-metrics" title="Loss and metrics">Loss and metrics</a></li><li><a class="toc-href" href="#Training-and-checkpointing" title="Training and checkpointing">Training and checkpointing</a><ul><li><a class="toc-href" href="#Chinese-font-setup-for-matplotlib" title="Chinese font setup for matplotlib">Chinese font setup for matplotlib</a></li><li><a class="toc-href" href="#TODO_1" title="TODO">TODO</a></li></ul></li><li><a class="toc-href" href="#Tensorboard_1" title="Tensorboard">Tensorboard</a></li><li><a class="toc-href" href="#TODO_2" title="TODO">TODO</a></li><li><a class="toc-href" href="#Evaluate" title="Evaluate">Evaluate</a></li><li><a class="toc-href" href="#Summary" title="Summary">Summary</a></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

            <div id="leftNavImage_transformer" class="overlay" style="background-color:white" onclick="closeLeftNavImage('leftNavImage_transformer')">
              <div class="overlay-content">
                <img src="/theme/images/left-nav/transformer.jpg">
              </div>
            </div>


    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(/theme/images/background/Tour_de_babel.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                            <a href="/tag/nlp.html" rel="tag">NLP</a>
                            <a href="/tag/tensorflow.html" rel="tag">Tensorflow</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="/neural-machine-translation-with-transformer-and-tensorflow2.html" title="">
                            淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-06-03 (Mon)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>



    <!--custom images with icon shown on left nav-->
            <div id="left-nav-image-wrap-transformer">
                <a title="顯示/隱藏 Transformer 架構圖">
                    <i class="im im-picture-o" aria-hidden="true" onclick="toggleLeftNavImage('leftNavImage_transformer')"></i>
                </a>
            </div>


</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        那時，全世界的語言都一樣。人們說：『來吧，我們要建一座塔，塔頂通天，為了揚我們的名，免得我們被分散到世界各地。』耶和華說：『看哪！他們成爲一樣的人民、用同樣的語言。如今既蓋起塔來，以後就沒有他們無法完成的事情了。我們下去！在那裏變亂他們的口音，使他們的言語彼此不通。』
                        <br/>
<span style="float:right;margin-right: 1.5rem">─ 《創世記》第十一章</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是聖經中著名的<a href="https://zh.wikipedia.org/wiki/%E5%B7%B4%E5%88%A5%E5%A1%94">巴別塔</a>橋段，用來解釋為何當今世界上有那麼多種語言。當年的上帝或許過於杞人憂天，但憑藉著過往的研究以及<a href="https://zh.wikipedia.org/zh-hant/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度學習（Deep Learning）</a>技術，近年<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">機器翻譯（Machine Translation）</a>系統如 <a href="https://translate.google.com">Google 翻譯</a>的表現令人歎為觀止。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/images/transformer/google-translate.jpg">
<source src="/images/transformer/google-translate.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        以往被視為非常困難的中 -&gt; 英翻譯已有不錯的水準
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>機器翻譯的研究之所以如此重要且迷人，是因為它將有機會讓任何人不受語言的限制，獲得世界上任何他或她想要的資訊與知識。</p>
<p>在這篇文章裡頭，我們會先簡單回顧深度學習在機器翻譯裡頭的一些重要模型及概念，接著在這些基礎之上實際使用最新的 <a href="https://www.tensorflow.org/">TensorFlow 2</a> 來實作一個可以將英文句子翻譯成中文的神經網路架構：<a href="https://arxiv.org/abs/1706.03762">Transformer</a>。</p>
<p>因為本文講述的內容牽涉到許多進階的深度學習概念，在說明之餘我也會盡量附上最適切的相關連結供你參考。</p>
<p>另外強烈推薦使用桌筆電閱讀，因為本文篇幅很長且有大量 <a href="https://www.python.org/">Python</a> 程式碼。</p>
<p>好啦！前言已盡，讓我們正式開始吧！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>TOC</p>
<ul>
<li>MT intro<ul>
<li>pbmt</li>
<li>seq2seq</li>
<li>seq2seq with attention</li>
<li>切入上面這個 transformer 的圖: self-attention</li>
<li>vis 介紹 intution</li>
<li>MIT tools</li>
</ul>
</li>
<li>Transformer 實作</li>
<li>Tensor2tensor 套用 Transformer</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>先想 input / output 是什麼，再實作</p>
<p>Components and implementation order</p>
<ul>
<li>~Masking (Masked Multi-Head Attention 起頭)~<ul>
<li>~padding mask~</li>
<li>~look ahead mask~</li>
</ul>
</li>
<li>~Scaled Dot-Product Attention~</li>
<li>~Multi-Head Attention~</li>
<li>~Feed Forward~</li>
<li>~Encoder block~<ul>
<li>~Residual Connection &amp; Layer Normalization~</li>
</ul>
</li>
<li>Decoder block</li>
<li>Encoder</li>
<li>Decoder</li>
<li>Positional Encoding</li>
<li>Transformer</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table align="left" class="tfo-notebook-buttons table table-striped table-responsive">
<td>
<a href="https://www.tensorflow.org/alpha/tutorials/text/transformer" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
    View on TensorFlow.org</a>
</td>
<td>
<a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb" target="_blank">
<img src="https://www.tensorflow.org/images/colab_logo_32px.png"/>
    Run in Google Colab</a>
</td>
<td>
<a href="https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb" target="_blank">
<img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png"/>
    View source on GitHub</a>
</td>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial trains a <a class="external" href="https://arxiv.org/abs/1706.03762">Transformer model</a> to translate Portuguese to English. This is an advanced example that assumes knowledge of <a href="text_generation.ipynb">text generation</a> and <a href="nmt_with_attention.ipynb">attention</a>.</p>
<p>The core idea behind the Transformer model is <em>self-attention</em>&mdash;the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections <em>Scaled dot product attention</em> and <em>Multi-head attention</em>.</p>
<p>A transformer model handles variable-sized input using stacks of self-attention layers instead of <a href="text_classification_rnn.ipynb">RNNs</a> or <a href="../images/intro_to_cnns.ipynb">CNNs</a>. This general architecture has a number of advantages:</p>
<ul>
<li>It make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8">StarCraft units</a>).</li>
<li>Layer outputs can be calculated in parallel, instead of a series like an RNN.</li>
<li>Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see <a href="https://arxiv.org/pdf/1903.03878.pdf">Scene Memory Transformer</a> for example).</li>
<li>It can learn long-range dependencies. This is a challenge in many sequence tasks.</li>
</ul>
<p>The downsides of this architecture are:</p>
<ul>
<li>For a time-series, the output for a time-step is calculated from the <em>entire history</em> instead of only the inputs and current hidden-state. This <em>may</em> be less efficient.   </li>
<li>If the input <em>does</em> have a  temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words. </li>
</ul>
<p>After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.</p>
<p><img alt="Attention heatmap" src="https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png" width="800"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">clear_output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tf-nightly-gpu-2.0-preview
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 TF2 裡頭 <code>tf.logging</code> 被 deprecated，我們可以直接用 <code>logging</code>  模組：
ref: <a href="https://www.tensorflow.org/alpha/guide/effective_tf2">https://www.tensorflow.org/alpha/guide/effective_tf2</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="s2">"error"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'2.0.0-dev20190527'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Experiment">Experiment<a class="anchor-link" href="#Experiment">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># corpus_generator = ['使用 Transformer 做機器翻譯很猛']</span>
<span class="c1"># tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_generator, target_vocab_size=2**13, max_subword_length=1)</span>

<span class="c1"># sample_string = '使用 Transformer 做機器翻譯很猛'</span>

<span class="c1"># tokenized_string = tokenizer_zh.encode(sample_string)</span>
<span class="c1"># print ('Tokenized string is {}'.format(tokenized_string))</span>

<span class="c1"># original_string = tokenizer_zh.decode(tokenized_string)</span>
<span class="c1"># print ('The original string: {}'.format(original_string))</span>

<span class="c1"># assert original_string == sample_string</span>

<span class="c1"># for ts in tokenized_string:</span>
<span class="c1">#   print ('{} ----&gt; {}'.format(ts, tokenizer_zh.decode([ts])))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span>
<span class="c1">#     (zh.numpy() for zh, _ in train_examples), target_vocab_size=2**17)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># sample_string = '使用 Transformer 做機器翻譯很猛'</span>

<span class="c1"># tokenized_string = tokenizer_zh.encode(sample_string)</span>
<span class="c1"># print ('Tokenized string is {}'.format(tokenized_string))</span>

<span class="c1"># original_string = tokenizer_zh.decode(tokenized_string)</span>
<span class="c1"># print ('The original string: {}'.format(original_string))</span>

<span class="c1"># assert original_string == sample_string</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># for ts in tokenized_string:</span>
<span class="c1">#   print ('{} ----&gt; {}'.format(ts, tokenizer_zh.decode([ts])))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gdrive">Gdrive<a class="anchor-link" href="#Gdrive">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_to_gdrive</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="s2">"nmt"</span>

<span class="k">if</span> <span class="n">save_to_gdrive</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">google.colab</span> <span class="k">import</span> <span class="n">drive</span>
    <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/content/gdrive'</span><span class="p">)</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"/content/gdrive/My Drive"</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">)</span>
    

<span class="n">en_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"en_vocab"</span><span class="p">)</span>
<span class="n">zh_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"zh_vocab"</span><span class="p">)</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"checkpoints"</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">download_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"tensorflow-datasets/downloads"</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Save result to </span><span class="si">{output_dir}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code

Enter your authorization code:
&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;
Mounted at /content/gdrive
Save result to /content/gdrive/My Drive/nmt
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span>
  <span class="n">en_vocab_file</span><span class="p">,</span>
  <span class="n">zh_vocab_file</span><span class="p">,</span>
  <span class="n">checkpoint_path</span><span class="p">,</span>
  <span class="n">log_dir</span><span class="p">,</span>
  <span class="n">download_dir</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/en_vocab',
 '/content/gdrive/My Drive/nmt/zh_vocab',
 '/content/gdrive/My Drive/nmt/checkpoints',
 '/content/gdrive/My Drive/nmt/logs',
 '/content/gdrive/My Drive/nmt/tensorflow-datasets/downloads']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup-input-pipeline">Setup input pipeline<a class="anchor-link" href="#Setup-input-pipeline">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Use <a href="https://www.tensorflow.org/datasets">TFDS</a> to load the <a href="https://github.com/neulab/word-embeddings-for-nmt">Portugese-English translation dataset</a> from the <a href="https://www.ted.com/participate/translate">TED Talks Open Translation Project</a>.</p>
<p>This dataset contains approximately 50000 training examples, 1100 validation examples, and 2000 test examples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>dataset reference: <a href="https://www.tensorflow.org/datasets/datasets#wmt19_translate">https://www.tensorflow.org/datasets/datasets#wmt19_translate</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # 顯示 subset 用</span>
<span class="c1"># tmp_builder = tfds.builder("wmt19_translate/zh-en")</span>
<span class="c1"># tmp_builder.subsets</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">translate</span><span class="o">.</span><span class="n">wmt</span><span class="o">.</span><span class="n">WmtConfig</span><span class="p">(</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">"0.0.2"</span><span class="p">,</span>
    <span class="n">language_pair</span><span class="o">=</span><span class="p">(</span><span class="s2">"zh"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">),</span>
    <span class="n">subsets</span><span class="o">=</span><span class="p">{</span>
        <span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span> <span class="p">[</span><span class="s2">"newscommentary_v14"</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">builder</span><span class="p">(</span><span class="s2">"wmt_translate"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># builder.download_and_prepare()</span>
<span class="n">builder</span><span class="o">.</span><span class="n">download_and_prepare</span><span class="p">(</span><span class="n">download_dir</span><span class="o">=</span><span class="n">download_dir</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>reference:</p>
<ul>
<li><a href="https://github.com/tensorflow/datasets/blob/master/docs/splits.md">https://github.com/tensorflow/datasets/blob/master/docs/splits.md</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_perc</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">drop_prec</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">train_perc</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">split</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">subsplit</span><span class="p">([</span><span class="n">drop_prec</span><span class="p">,</span> <span class="n">train_perc</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">split</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(NamedSplit('train')(tfds.percent[0:9]),
 NamedSplit('train')(tfds.percent[9:99]),
 NamedSplit('train')(tfds.percent[99:100]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">info</span>
<span class="n">examples</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;,
 &lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;,
 &lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">examples</span>
<span class="c1"># train_examples, val_examples = examples</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">sample_examples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">en</span><span class="p">,</span> <span class="n">zh</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">:</span>
    <span class="n">num_en_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    <span class="n">num_zh_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">zh</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_zh_words</span> <span class="o">&lt;</span> <span class="mi">16</span> <span class="ow">or</span> <span class="n">num_zh_words</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>
      <span class="k">continue</span>
    
    <span class="k">if</span> <span class="n">num_en_words</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">num_en_words</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
      <span class="k">continue</span>
  
    <span class="n">sample_examples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">),</span>
        <span class="n">zh</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span>
    <span class="p">))</span>
    <span class="n">c</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="n">num_samples</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fix sentence for consistency</span>
<span class="n">sample_examples</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">'This year will be another difficult one for China.'</span><span class="p">,</span> <span class="s1">'今年对中国来说又将是艰难的一年。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'It is the only such case in the world.'</span><span class="p">,</span> <span class="s1">'它是世界上此类案例中唯一的一个。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'When that happens, a recession typically follows.'</span><span class="p">,</span> <span class="s1">'一旦出现这种情况衰退通常就会随之而来。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Right-wing populists such as Trump engage in identity politics.'</span><span class="p">,</span>
  <span class="s1">'特朗普等右翼民粹主义者采取身份政治。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Given the reaction in financial markets, they have succeeded.'</span><span class="p">,</span>
  <span class="s1">'鉴于金融市场的反应，他们已经成功了。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'The first lesson is that markets are not self-correcting.'</span><span class="p">,</span>
  <span class="s1">'第一个教训是，市场不会自动纠正错误。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'And they are gateways to Europe, Africa, and Asia.'</span><span class="p">,</span> <span class="s1">'它们也是通往欧洲、非洲和亚洲的门户。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Will the energy reforms produce another false dawn?'</span><span class="p">,</span> <span class="s1">'能源改革会再一次只是昙花一现吗？'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'In economic terms, they have had considerable success.'</span><span class="p">,</span>
  <span class="s1">'在经济方面，它们取得了相当大的成功。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'But even this comparison involves a selection bias.'</span><span class="p">,</span>
  <span class="s1">'但是，就连这个比较也带有一定的选择偏误。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'The shifting economic operating environment bolsters these opportunities.'</span><span class="p">,</span>
  <span class="s1">'不断变化的经济运行环境支撑了这些机遇。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'That memory is discouraging political innovation today.'</span><span class="p">,</span>
  <span class="s1">'如今，这段记忆阻碍人们进行政治创新。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'They are just buying real estate because they need it.'</span><span class="p">,</span>
  <span class="s1">'他们购置不动产时出于实际的需要。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Sixth, the authors admiringly cite the British experience.'</span><span class="p">,</span>
  <span class="s1">'第六，作者们以敬仰之情援引了英国的经验。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Are banks, markets, or regulators to blame?'</span><span class="p">,</span> <span class="s1">'我们应该归罪于银行、市场或监管者么？'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Moreover, Ukraine has strong investment and consumption growth.'</span><span class="p">,</span>
  <span class="s1">'此外，乌克兰投资和消费增长强劲。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Another factor that has been overlooked is the Chinese consumer.'</span><span class="p">,</span>
  <span class="s1">'中国消费者是一直被忽视的另外一个因素。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Europe&rsquo;s leaders must imbue their citizens with renewed hope.'</span><span class="p">,</span>
  <span class="s1">'欧洲领导人必须让民众看到新的希望。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Moreover, discrimination in India often begins in the family.'</span><span class="p">,</span>
  <span class="s1">'此外，印度的歧视往往从家庭就开始了。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Mueller obviously hopes to &ldquo;flip&rdquo; both Flynn and Manafort.'</span><span class="p">,</span>
  <span class="s1">'米勒显然希望策反弗林和马纳福特。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Others continue to advocate for national reconciliation.'</span><span class="p">,</span>
  <span class="s1">'也有人继续宣传民族和解才能解决问题。'</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create a custom subwords tokenizer from the training dataset.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_perc</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>90</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">en_vocab_file</span> <span class="o">=</span> <span class="n">en_vocab_file</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_perc</span><span class="p">)</span>
<span class="n">zh_vocab_file</span> <span class="o">=</span> <span class="n">zh_vocab_file</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_perc</span><span class="p">)</span>
<span class="p">[</span>
    <span class="n">en_vocab_file</span><span class="p">,</span> 
    <span class="n">zh_vocab_file</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/en_vocab_90',
 '/content/gdrive/My Drive/nmt/zh_vocab_90']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>因為我們實際上是讓模型產生一個中文字的分佈，開小一點的詞彙可以訓練快一點。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>
try:
    tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)
    print(f"Vocab file loaded from {en_vocab_file}")
except:
    tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(
        (en.numpy() for en, _ in train_examples), target_vocab_size=2**13)
    
    tokenizer_en.save_to_file(en_vocab_file)
    print(f"Build from corpus and saved to {en_vocab_file}")
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Vocab file loaded from /content/gdrive/My Drive/nmt/en_vocab_90
CPU times: user 48.3 ms, sys: 1.93 ms, total: 50.2 ms
Wall time: 662 ms
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>
try:
    tokenizer_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)
    print(f"Vocab file loaded from {zh_vocab_file}")
except:
    tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (zh.numpy() for _, zh in train_examples), target_vocab_size=2**17, max_subword_length=1)
    
    tokenizer_zh.save_to_file(zh_vocab_file)
    print(f"Build from corpus and saved to {zh_vocab_file}")
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Vocab file loaded from /content/gdrive/My Drive/nmt/zh_vocab_90
CPU times: user 27.2 ms, sys: 1.87 ms, total: 29.1 ms
Wall time: 530 ms
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(8159, 4849)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="s1">'Transformer is awesome.'</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'Tokenized string is </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'The original string: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Tokenized string is [2728, 466, 9, 3354, 145, 1355, 7949]
The original string: Transformer is awesome.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>2728 ----&gt; Trans
466 ----&gt; former 
9 ----&gt; is 
3354 ----&gt; aw
145 ----&gt; es
1355 ----&gt; ome
7949 ----&gt; .
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="n">sample_examples</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'Tokenized string is </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'The original string: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Tokenized string is [260, 25, 24, 16, 4, 28, 176, 553, 49, 6, 1108, 269, 1, 7, 25, 3]
The original string: 今年对中国来说又将是艰难的一年。
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>260 ----&gt; 今
25 ----&gt; 年
24 ----&gt; 对
16 ----&gt; 中
4 ----&gt; 国
28 ----&gt; 来
176 ----&gt; 说
553 ----&gt; 又
49 ----&gt; 将
6 ----&gt; 是
1108 ----&gt; 艰
269 ----&gt; 难
1 ----&gt; 的
7 ----&gt; 一
25 ----&gt; 年
3 ----&gt; 。
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Add a start and end token to the input and target.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
  <span class="n">lang1</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang1</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">lang2</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
  
  <span class="k">return</span> <span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">40</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">filter_max_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">,</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Operations inside <code>.map()</code> run in graph mode and receive a graph tensor that do not have a numpy attribute. The <code>tokenizer</code> expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a <code>tf.py_function</code>, which receives an eager tensor having a numpy attribute that contains the string value.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tf_encode</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">encode</span><span class="p">,</span> <span class="p">[</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span>
<span class="c1"># cache the dataset to memory to get a speedup while reading from it.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">padded_shapes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>


<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">padded_shapes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">en_batch</span><span class="p">,</span> <span class="n">zh_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">en_batch</span><span class="p">,</span> <span class="n">zh_batch</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>W0527 12:29:02.031296 139886818944768 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0527 12:29:02.034073 139886818944768 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0527 12:29:02.045863 139886810552064 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0527 12:29:02.047582 139886810552064 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0527 12:29:02.076257 139886810552064 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;tf.Tensor: id=1250590, shape=(128, 40), dtype=int64, numpy=
 array([[8159,  424,  778, ...,    0,    0,    0],
        [8159,  129,   47, ...,    0,    0,    0],
        [8159, 5097,  565, ...,    0,    0,    0],
        ...,
        [8159,  609,  614, ...,    0,    0,    0],
        [8159,   86,  586, ...,    0,    0,    0],
        [8159,   40,   41, ...,    0,    0,    0]])&gt;,
 &lt;tf.Tensor: id=1250591, shape=(128, 40), dtype=int64, numpy=
 array([[4849,   10,   66, ...,  427,    3, 4850],
        [4849,   69,  227, ...,    0,    0,    0],
        [4849,    9,  160, ...,    0,    0,    0],
        ...,
        [4849,   35,    6, ...,    0,    0,    0],
        [4849,   10,    7, ...,    0,    0,    0],
        [4849,   35,   10, ...,    0,    0,    0]])&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Positional-encoding">Positional encoding<a class="anchor-link" href="#Positional-encoding">&para;</a></h2><p>Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.</p>
<p>The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the <em>similarity of their meaning and their position in the sentence</em>, in the d-dimensional space.</p>
<p>See the notebook on <a href="https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb">positional encoding</a> to learn more about it. The formula for calculating the positional encoding is as follows:</p>
<p>$$\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$
$$\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angle_rates</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                          <span class="n">d_model</span><span class="p">)</span>
  
  <span class="c1"># apply sin to even indices in the array; 2i</span>
  <span class="n">sines</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
  <span class="c1"># apply cos to odd indices in the array; 2i+1</span>
  <span class="n">cosines</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">sines</span><span class="p">,</span> <span class="n">cosines</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  
  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-notebook"</span><span class="p">)</span>
<span class="c1"># plt.style.use("ggplot")</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'RdBu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Depth'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Position'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 50, 512)
</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAecAAAFfCAYAAAB0lARMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Masking">Masking<a class="anchor-link" href="#Masking">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value <code>0</code> is present: it outputs a <code>1</code> at those locations, and a <code>0</code> otherwise.</p>
<p>實際上在 Transformer 裡頭有兩種 masks：</p>
<ul>
<li>padding mask</li>
<li>look ahead mask</li>
</ul>
<p>前者是讓 Transformer 用來識別 sequence 實際的內容到哪裡；後者則是讓 Decoder 在生成輸出序列的時候，不會不小心看到未來的句子。</p>
<p>mask 序列裡頭那些值為 1 的位置即為未來會被遮蓋住的位置。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
  <span class="n">seq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  
  <span class="c1"># add extra dimensions so that we can add the padding</span>
  <span class="c1"># to the attention logits.</span>
  <span class="k">return</span> <span class="n">seq</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, 1, seq_len)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而這邊之所以要新增兩個 dimensions 是因為在 Multi-Head Attention 裡頭一組 attention logits 的維度為 <code>(batch_size, num_heads, seq_len, seq_len)</code>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
                 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
                 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor: id=1250607, shape=(3, 1, 1, 5), dtype=float32, numpy=
array([[[[0., 0., 1., 1., 1.]]],


       [[[0., 0., 0., 1., 1.]]],


       [[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensor 裡頭為 <code>1</code> 的位置的值在之後的計算就會忽略。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.</p>
<p>This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">mask</span>  <span class="c1"># (seq_len, seq_len)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">temp</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor: id=1250623, shape=(3, 3), dtype=float32, numpy=
array([[0., 1., 1.],
       [0., 0., 1.],
       [0., 0., 0.]], dtype=float32)&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>第一維度代表處理次數，而可以看得出來，第二維度從上到下就代表 Decoder 在產生翻譯時每次能看到的位置。</p>
<p>第一個時間點只能看到第一個位置，以此類推。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="TODO">TODO<a class="anchor-link" href="#TODO">&para;</a></h3><p>增加 ene-to-end 例子</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dec_target_padding_mask = create_padding_mask(tar)</span>
<span class="c1"># print(dec_target_padding_mask)</span>
<span class="c1"># look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])</span>
<span class="c1"># print(look_ahead_mask)</span>
<span class="c1"># tf.maximum(dec_target_padding_mask, look_ahead_mask)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scaled-dot-product-attention_1">Scaled dot product attention<a class="anchor-link" href="#Scaled-dot-product-attention">&para;</a></h2><p>在實作 Multi-head 之前，先讓我們實作基本的 Attention 機制。</p>
<p>注意力機制基本上可以想成資料庫比對。給定一個查詢 Q，我們去看該 Q 跟所有 K 的匹配程度，接著以此匹配程度對實際的 V 做加權平均，得到最後的 Repr.</p>
<p>$$Attention(Q, K, V) = softmax({QK^T \over \sqrt{d_{k}}})V $$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="scaled_dot_product_attention" src="https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png" width="500"/></p>
<p>The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:</p>
<p>$$\Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k}}) V} $$</p>
<p>The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.</p>
<p>For example, consider that <code>Q</code> and <code>K</code> have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of <code>dk</code>. Hence, <em>square root of <code>dk</code></em> is used for scaling (and not any other number) because the matmul of <code>Q</code> and <code>K</code> should have a mean of 0 and variance of 1, so that we get a gentler softmax.</p>
<p>The mask is multiplied with <em>-1e9 (close to negative infinity).</em> This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install pysnooper
<span class="kn">import</span> <span class="nn">pysnooper</span>
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>

<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
  <span class="sd">"""Calculate the attention weights.</span>
<span class="sd">  q, k, v must have matching leading dimensions.</span>
<span class="sd">  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.</span>
<span class="sd">  The mask has different shapes depending on its type(padding or look ahead) </span>
<span class="sd">  but it must be broadcastable for addition.</span>
<span class="sd">  </span>
<span class="sd">  Args:</span>
<span class="sd">    q: query shape == (..., seq_len_q, depth)</span>
<span class="sd">    k: key shape == (..., seq_len_k, depth)</span>
<span class="sd">    v: value shape == (..., seq_len_v, depth_v)</span>
<span class="sd">    mask: Float tensor with shape broadcastable </span>
<span class="sd">          to (..., seq_len_q, seq_len_k). Defaults to None.</span>
<span class="sd">    </span>
<span class="sd">  Returns:</span>
<span class="sd">    output, attention_weights</span>
<span class="sd">  """</span>

  <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>
  
  <span class="c1"># scale matmul_qk</span>
  <span class="n">dk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">scaled_attention_logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

  <span class="c1"># add the mask to the scaled tensor.</span>
  <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">scaled_attention_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

  <span class="c1"># softmax is normalized on the last axis (seq_len_k) so that the scores</span>
  <span class="c1"># add up to 1.</span>
  <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_attention_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (..., seq_len_v, depth_v)</span>

  <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很大的負值丟入 Softmax 函式以後會接近 0 ，則如果我們想把後三個位置遮住丟入 softmax 的話，則 mask 應該要是 [..., 0, 1, 1, 1] （要被遮住的位置的 mask 值為 1），再乘上 -1e9 以後加入 scaled_attention_logits 即可讓後三個位置經過 softmax 出來的值為 0</p>
<p>As the softmax normalization is done on K, its values decide the amount of importance given to Q.</p>
<p>The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_out</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Attention weights are:'</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_attn</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Output is:'</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">temp_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 3)</span>

<span class="n">temp_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span>   <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span>  <span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 2)</span>

<span class="c1"># This `query` aligns with the second `key`,</span>
<span class="c1"># so the second `value` is returned.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>

<span class="c1"># print_out(temp_q, temp_k, temp_v)</span>
<span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns with a repeated key (third and fourth), </span>
<span class="c1"># so all associated values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns equally with the first and second key, </span>
<span class="c1"># so their values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pass all the queries together.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (3, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor(
[[0.  0.  0.5 0.5]
 [0.  1.  0.  0. ]
 [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)
Output is:
tf.Tensor(
[[550.    5.5]
 [ 10.    0. ]
 [  5.5   0. ]], shape=(3, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-head-attention">Multi-head attention<a class="anchor-link" href="#Multi-head-attention">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="multi-head attention" src="https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png" width="500"/></p>
<p>Multi-head attention consists of four parts:</p>
<ul>
<li>Linear layers and split into heads.</li>
<li>Scaled dot-product attention.</li>
<li>Concatenation of heads.</li>
<li>Final linear layer.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.</p>
<p>The <code>scaled_dot_product_attention</code> defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using <code>tf.transpose</code>, and <code>tf.reshape</code>) and put through a final <code>Dense</code> layer.</p>
<p>Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    
    <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
  <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">"""Split the last dimension into (num_heads, depth).</span>
<span class="sd">    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="sd">    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_k, depth)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_v, depth)</span>
    
    <span class="c1"># scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)</span>
    <span class="c1"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    
    <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len_v, num_heads, depth)</span>

    <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> 
                                  <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len_v, d_model)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len_v, d_model)</span>
        
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create a <code>MultiHeadAttention</code> layer to try out. At each location in the sequence, <code>y</code>, the <code>MultiHeadAttention</code> runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>  <span class="c1"># (batch_size, encoder_sequence, d_model)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">temp_mha</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Point-wise-feed-forward-network">Point wise feed forward network<a class="anchor-link" href="#Point-wise-feed-forward-network">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>  <span class="c1"># (batch_size, seq_len, dff)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
  <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_ffn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Encoder-and-decoder">Encoder and decoder<a class="anchor-link" href="#Encoder-and-decoder">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="transformer" src="https://www.tensorflow.org/images/tutorials/transformer/transformer.png" width="600"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The transformer model follows the same general pattern as a standard <a href="nmt_with_attention.ipynb">sequence to sequence with attention model</a>.</p>
<ul>
<li>The input sentence is passed through <code>N</code> encoder layers that generates an output for each word/token in the sequence.</li>
<li>The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder-layer">Encoder layer<a class="anchor-link" href="#Encoder-layer">&para;</a></h3><p>Each encoder layer consists of sublayers:</p>
<ol>
<li>Multi-head attention (with padding mask) </li>
<li>Point wise feed forward networks. </li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.</p>
<p>The output of each sublayer is <code>LayerNorm(x + Sublayer(x))</code>. The normalization is done on the <code>d_model</code> (last) axis. There are N encoder layers in the transformer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    
    <span class="k">return</span> <span class="n">out2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder_layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span> <span class="o">=</span> <span class="n">sample_encoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 43, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder-layer">Decoder layer<a class="anchor-link" href="#Decoder-layer">&para;</a></h3><p>Each decoder layer consists of sublayers:</p>
<ol>
<li>Masked multi-head attention (with look ahead mask and padding mask)</li>
<li>Multi-head attention (with padding mask). V (value) and K (key) receive the <em>encoder output</em> as inputs. Q (query) receives the <em>output from the masked multi-head attention sublayer.</em></li>
<li>Point wise feed forward networks</li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is <code>LayerNorm(x + Sublayer(x))</code>. The normalization is done on the <code>d_model</code> (last) axis.</p>
<p>There are N decoder layers in the transformer.</p>
<p>As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>
 
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
    <span class="c1"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>

    <span class="n">attn1</span><span class="p">,</span> <span class="n">attn_weights_block1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="n">attn2</span><span class="p">,</span> <span class="n">attn_weights_block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span><span class="p">(</span>
        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">attn2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    
    <span class="k">return</span> <span class="n">out3</span><span class="p">,</span> <span class="n">attn_weights_block1</span><span class="p">,</span> <span class="n">attn_weights_block2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder_layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_decoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">sample_encoder_layer_output</span><span class="p">,</span> 
    <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder">Encoder<a class="anchor-link" href="#Encoder">&para;</a></h3><p>The <code>Encoder</code> consists of:</p>
<ol>
<li>Input Embedding</li>
<li>Positional Encoding</li>
<li>N encoder layers</li>
</ol>
<p>The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
               <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    
    
    <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
  
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># adding embedding and position encoding.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">)</span>

<span class="n">sample_encoder_output</span> <span class="o">=</span> <span class="n">sample_encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">)),</span> 
                                       <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">sample_encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>(64, 62, 512)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder">Decoder<a class="anchor-link" href="#Decoder">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Decoder</code> consists of:</p>
<ol>
<li>Output Embedding</li>
<li>Positional Encoding</li>
<li>N decoder layers</li>
</ol>
<p>The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> 
               <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
                                             <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>
      
      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">'decoder_layer</span><span class="si">{}</span><span class="s1">_block1'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block1</span>
      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">'decoder_layer</span><span class="si">{}</span><span class="s1">_block2'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block2</span>
    
    <span class="c1"># x.shape == (batch_size, target_seq_len, d_model)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">sample_decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">)),</span> 
                              <span class="n">enc_output</span><span class="o">=</span><span class="n">sample_encoder_output</span><span class="p">,</span> 
                              <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                              <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="p">[</span><span class="s1">'decoder_layer2_block2'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-the-Transformer_1">Create the Transformer<a class="anchor-link" href="#Create-the-Transformer">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
               <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                           <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                           <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">):</span>

    <span class="n">enc_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, inp_seq_len, d_model)</span>
    
    <span class="c1"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
    <span class="n">dec_output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
        <span class="n">tar</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">)</span>
    
    <span class="n">final_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
    
    <span class="k">return</span> <span class="n">final_output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>

<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">))</span>
<span class="n">temp_target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">))</span>

<span class="n">fn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_transformer</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">temp_target</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                               <span class="n">enc_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                               <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">dec_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">fn_out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 26, 8000])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-hyperparameters">Set hyperparameters<a class="anchor-link" href="#Set-hyperparameters">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To keep this example small and relatively fast, the values for <em>num_layers, d_model, and dff</em> have been reduced.</p>
<p>The values used in the base model of transformer were; <em>num_layers=6</em>, <em>d_model = 512</em>, <em>dff = 2048</em>. See the <a href="https://arxiv.org/abs/1706.03762">paper</a> for all the other versions of the transformer.</p>
<p>Note: By changing the values below, you can get the model that achieved state of the art on many tasks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">dff</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(8161, 4851)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Setup-experiment-path">Setup experiment path<a class="anchor-link" href="#Setup-experiment-path">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">run_id</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"</span><span class="si">{num_layers}</span><span class="s2">layers_</span><span class="si">{d_model}</span><span class="s2">d_</span><span class="si">{num_heads}</span><span class="s2">heads_</span><span class="si">{dff}</span><span class="s2">dff_</span><span class="si">{train_perc}</span><span class="s2">train_perc"</span>
<span class="n">run_id</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'6layers_256d_8heads_1024dff_90train_perc'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span>
    <span class="n">checkpoint_path</span><span class="p">,</span>
    <span class="n">log_dir</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc',
 '/content/gdrive/My Drive/nmt/logs/6layers_256d_8heads_1024dff_90train_perc']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Optimizer_1">Optimizer<a class="anchor-link" href="#Optimizer">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Use the Adam optimizer with a custom learning rate scheduler according to the formula in the <a href="https://arxiv.org/abs/1706.03762">paper</a>.</p>
<p>$$\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}$$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> 
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-whitegrid"</span><span class="p">)</span>
<span class="c1"># plt.style.use("ggplot")</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_learning_rate_schedule</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temp_learning_rate_schedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Learning Rate"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Train Step"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 0, 'Train Step')</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAFYCAYAAAAlTUT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># temp_learning_rate_schedule.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-and-metrics">Loss and metrics<a class="anchor-link" href="#Loss-and-metrics">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.</p>
<p>這邊 <code>reduction</code> 設成 <code>none</code> 是因為我們會套用 mask 以後自己 sum up</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在這邊每個中文字就是一個分類，因此我們可以使用 Cross entropy</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">'none'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>
  
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'train_loss'</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'train_accuracy'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)</span>
<span class="c1"># pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)</span>
<span class="c1"># loss_object(real, pred)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tf.keras.losses.SparseCategoricalCrossentropy(</span>
<span class="c1">#     from_logits=True)(real, pred)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-and-checkpointing">Training and checkpointing<a class="anchor-link" href="#Training-and-checkpointing">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                          <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="c1"># Encoder padding mask</span>
  <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
  
  <span class="c1"># Used in the 2nd attention block in the decoder.</span>
  <span class="c1"># This padding mask is used to mask the encoder outputs.</span>
  <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
  
  <span class="c1"># Used in the 1st attention block in the decoder.</span>
  <span class="c1"># It is used to pad and mask future tokens in the input received by </span>
  <span class="c1"># the decoder.</span>
  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tar</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">dec_target_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">tar</span><span class="p">)</span>
  <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dec_target_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">inp</span><span class="p">,</span> <span class="n">tar</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;tf.Tensor: id=1255678, shape=(3, 4), dtype=float32, numpy=
 array([[1., 2., 3., 0.],
        [2., 2., 5., 6.],
        [5., 1., 0., 0.]], dtype=float32)&gt;,
 &lt;tf.Tensor: id=1255679, shape=(3, 3), dtype=float32, numpy=
 array([[2., 5., 1.],
        [3., 4., 0.],
        [1., 0., 0.]], dtype=float32)&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create_padding_mask(inp)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dec_target_padding_mask = create_padding_mask(tar)</span>
<span class="c1"># print(dec_target_padding_mask)</span>
<span class="c1"># look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])</span>
<span class="c1"># print(look_ahead_mask)</span>
<span class="c1"># tf.maximum(dec_target_padding_mask, look_ahead_mask)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every <code>n</code> epochs.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Latest checkpoint restored!!'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Start from scratch!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Start from scratch!
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">last_epoch</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"-"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">last_epoch</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. <code>tar_real</code> is that same input shifted by 1: At each location in <code>tar_input</code>, <code>tar_real</code> contains the  next token that should be predicted.</p>
<p>For example, <code>sentence</code> = "SOS A lion in the jungle is sleeping EOS"</p>
<p><code>tar_inp</code> =  "SOS A lion in the jungle is sleeping"</p>
<p><code>tar_real</code> = "A lion in the jungle is sleeping EOS"</p>
<p>The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.</p>
<p>During training this example uses teacher-forcing (like in the <a href="./text_generation.ipynb">text generation tutorial</a>). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.</p>
<p>As the transformer predicts each word, <em>self-attention</em> allows it to look at the previous words in the input sequence to better predict the next word.</p>
<p>To prevent the model from peaking at the expected output the model uses a look-ahead mask.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們會使用 teacher forcing 來訓練模型。</p>
<ul>
<li>teach forcing 比較穩定的 video link: </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="n">tar_inp</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">tar_real</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
  
  <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">)</span>
  
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">,</span> 
                                 <span class="kc">True</span><span class="p">,</span> 
                                 <span class="n">enc_padding_mask</span><span class="p">,</span> 
                                 <span class="n">combined_mask</span><span class="p">,</span> 
                                 <span class="n">dec_padding_mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

  <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>    
  <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
  
  <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="n">train_accuracy</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>英文是我們的來源語言，而我們目標則是將其轉成（簡體）中文。因此 <code>inp</code> 為 <code>en</code>，<code>tar</code> 為 <code>zh</code>。</p>
<ul>
<li>因為我們在訓練的時候輸入輸出句子都有加上 <bos>，在產生一個新句子的時候也要加上去</bos></li>
<li>因為不像是天龍八部生成器我們希望機器每次都能產生新的結果，NMT 是做 Conditional Generation，因此我們不是 sample 機器預測的中文字分佈而是取 <code>argmax</code></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">):</span>
  <span class="n">start_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">end_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
  
  <span class="c1"># inp sentence is english, hence adding the start and end token</span>
  <span class="n">inp_sentence</span> <span class="o">=</span> <span class="n">start_token</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">end_token</span>
  <span class="n">encoder_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  
  <span class="c1"># as the target is chinese, the first word to the transformer should be the</span>
  <span class="c1"># chinese start token.</span>
  <span class="n">decoder_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span>
        <span class="n">encoder_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
  
    <span class="c1"># predictions.shape == (batch_size, seq_len, vocab_size)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">encoder_input</span><span class="p">,</span> 
                                                 <span class="n">output</span><span class="p">,</span>
                                                 <span class="kc">False</span><span class="p">,</span>
                                                 <span class="n">enc_padding_mask</span><span class="p">,</span>
                                                 <span class="n">combined_mask</span><span class="p">,</span>
                                                 <span class="n">dec_padding_mask</span><span class="p">)</span>
    
    <span class="c1"># select the last word from the seq_len dimension</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, vocab_size)</span>

    <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="c1"># return the result if the predicted_id is equal to the end token</span>
    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predicted_id</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
    
    <span class="c1"># concatentate the predicted_id to the output which is given to the decoder</span>
    <span class="c1"># as its input.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">predicted_id</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chinese-font-setup-for-matplotlib">Chinese font setup for matplotlib<a class="anchor-link" href="#Chinese-font-setup-for-matplotlib">&para;</a></h3><p>reference</p>
<ul>
<li><a href="https://www.jianshu.com/p/fc9a502ad243">https://www.jianshu.com/p/fc9a502ad243</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget -O /usr/share/fonts/truetype/liberation/simhei.ttf <span class="err">\</span>
    <span class="s2">"http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf"</span>
<span class="c1"># !test NotoSans-unhinted.zip || wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSans-unhinted.zip</span>
<span class="c1"># !test -d fonts || mkdir fonts</span>
<span class="c1"># !unzip -d fonts NotoSans-unhinted.zip</span>
<span class="c1"># !cp fonts/NotoSans* /usr/share/fonts/truetype/liberation/</span>
<span class="c1"># clear_output()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>--2019-05-27 12:29:18--  http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf
Resolving d.xiazaiziti.com (d.xiazaiziti.com)... 67.198.189.58
Connecting to d.xiazaiziti.com (d.xiazaiziti.com)|67.198.189.58|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10050868 (9.6M) [application/octet-stream]
Saving to: &lsquo;/usr/share/fonts/truetype/liberation/simhei.ttf&rsquo;

/usr/share/fonts/tr 100%[===================&gt;]   9.58M  1.98MB/s    in 4.8s    

2019-05-27 12:29:23 (1.98 MB/s) - &lsquo;/usr/share/fonts/truetype/liberation/simhei.ttf&rsquo; saved [10050868/10050868]

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="c1"># zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/NotoSans-Regular.ttf')</span>
<span class="n">zhfont</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">font_manager</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s1">'/usr/share/fonts/truetype/liberation/simhei.ttf'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-whitegrid"</span><span class="p">)</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="TODO_1">TODO<a class="anchor-link" href="#TODO">&para;</a></h3><p>改成 2 * 4</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>
<span class="k">def</span> <span class="nf">plot_attention_weights</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
  
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  
  <span class="c1"># for pretty rendering intermidate result</span>
  <span class="k">if</span> <span class="n">max_len_tar</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:</span><span class="n">max_len_tar</span><span class="p">]</span>
  
  <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  
  <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">head</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># plot the attention weights</span>
    <span class="k">if</span> <span class="n">max_len_tar</span><span class="p">:</span>
      <span class="n">attn_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">][:</span><span class="n">max_len_tar</span><span class="p">,</span> <span class="p">:])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attn_map</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>  <span class="c1"># (inp_seq_len, tar_seq_len)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">attn_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attn_map</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
    <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"fontproperties"</span><span class="p">:</span> <span class="n">zhfont</span><span class="p">}</span>
    
    <span class="k">if</span> <span class="n">max_len_tar</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_len_tar</span><span class="p">:</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_len_tar</span><span class="p">))</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_len_tar</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)))</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">],</span> 
                       <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>    
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span>
        <span class="p">[</span><span class="s1">'&lt;start&gt;'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">'&lt;end&gt;'</span><span class="p">],</span> 
        <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Head </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">head</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
  
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

  
  <span class="k">if</span> <span class="ow">not</span> <span class="n">save_file_name</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_file_name</span><span class="p">)</span>
    
  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">''</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">result</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  
  <span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">])</span>  
  <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Input: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Predicted translation: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
  
  <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
    <span class="n">plot_attention_weights</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">plot</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">predicted_sentence</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_layer_block</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"decoder_layer</span><span class="si">{num_layers}</span><span class="s2">_block2"</span>
<span class="n">plot_layer_block</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'decoder_layer6_block2'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">[</span><span class="mi">14</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
          <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Input: Are banks, markets, or regulators to blame?
Predicted translation: 侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪侪
</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABMAAAAHMCAYAAAAzsNRKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inp_sentence = "America"</span>
<span class="c1"># result, attention_weights = evaluate(inp_sentence)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def translate(sentence):</span>
<span class="c1">#   result, attention_weights = evaluate(sentence)</span>
  
<span class="c1">#   predicted_sentence = tokenizer_zh.decode([i for i in result </span>
<span class="c1">#                                             if i &lt; tokenizer_zh.vocab_size])  </span>

<span class="c1">#   print('Input: {}'.format(sentence))</span>
<span class="c1">#   print('Predicted translation: {}'.format(predicted_sentence))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># predicted_sentence = tokenizer_zh.decode([i for i in result </span>
<span class="c1">#                                             if i &lt; tokenizer_zh.vocab_size])  </span>

<span class="c1"># print('Input: {}'.format(inp_sentence))</span>
<span class="c1"># print('Predicted translation: {}'.format(predicted_sentence))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tensorboard_1">Tensorboard<a class="anchor-link" href="#Tensorboard">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> tensorboard
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span><span class="nb">kill</span> <span class="m">6658</span>
<span class="o">%</span><span class="k">tensorboard</span> --logdir /content/gdrive/My\ Drive/nmt/logs
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>/bin/bash: line 0: kill: (6658) - No such process
</pre>
</div>
</div>
<div class="output_area">
<div class="output_html rendered_html output_subarea ">
<div id="root"></div>
<script>
      (function() {
        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};
        window.TENSORBOARD_ENV["IN_COLAB"] = true;
        document.querySelector("base").href = "https://localhost:6006";
        function fixUpTensorboard(root) {
          const tftb = root.querySelector("tf-tensorboard");
          // Disable the fragment manipulation behavior in Colab. Not
          // only is the behavior not useful (as the iframe's location
          // is not visible to the user), it causes TensorBoard's usage
          // of `window.replace` to navigate away from the page and to
          // the `localhost:<port>` URL specified by the base URI, which
          // in turn causes the frame to (likely) crash.
          tftb.removeAttribute("use-hash");
        }
        function executeAllScripts(root) {
          // When `script` elements are inserted into the DOM by
          // assigning to an element's `innerHTML`, the scripts are not
          // executed. Thus, we manually re-insert these scripts so that
          // TensorBoard can initialize itself.
          for (const script of root.querySelectorAll("script")) {
            const newScript = document.createElement("script");
            newScript.type = script.type;
            newScript.textContent = script.textContent;
            root.appendChild(newScript);
            script.remove();
          }
        }
        function setHeight(root, height) {
          // We set the height dynamically after the TensorBoard UI has
          // been initialized. This avoids an intermediate state in
          // which the container plus the UI become taller than the
          // final width and cause the Colab output frame to be
          // permanently resized, eventually leading to an empty
          // vertical gap below the TensorBoard UI. It's not clear
          // exactly what causes this problematic intermediate state,
          // but setting the height late seems to fix it.
          root.style.height = `${height}px`;
        }
        const root = document.getElementById("root");
        fetch(".")
          .then((x) => x.text())
          .then((html) => void (root.innerHTML = html))
          .then(() => fixUpTensorboard(root))
          .then(() => executeAllScripts(root))
          .then(() => setHeight(root, 800));
      })();
    </script>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">last_epoch</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"restore for checkpoints, skip."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>
  <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
  
<span class="n">track_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"China, India, and others have enjoyed continuing economic growth."</span><span class="p">,</span>
    <span class="s2">"When that happens, a recession typically follows."</span><span class="p">,</span>
    <span class="s2">"In fact, central banks typically only have very fuzzy measures."</span><span class="p">,</span>
    <span class="s2">"Given the reaction in financial markets, they have succeeded."</span><span class="p">,</span>
    <span class="s2">"Moreover, discrimination in India often begins in the family."</span><span class="p">,</span>
    <span class="s2">"Europe&rsquo;s leaders must imbue their citizens with renewed hope."</span><span class="p">,</span>
    <span class="s2">"Are banks, markets, or regulators to blame?"</span>
<span class="p">]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">last_epoch</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">s_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">track_sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="s2">"s_</span><span class="si">{:02d}</span><span class="s2">_attn_epoch_</span><span class="si">{:02d}</span><span class="s2">.png"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"restore for checkpoints, skip."</span><span class="p">)</span>
  
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>50</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">last_epoch</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  
  <span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  
  <span class="c1"># inp -&gt; english, tar -&gt; chinese</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>
    
<span class="c1">#     if batch % 100 == 0:</span>
<span class="c1">#       print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(</span>
<span class="c1">#           epoch + 1, batch, train_loss.result(), train_accuracy.result()))</span>
      
  <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">'Saving checkpoint for epoch </span><span class="si">{}</span><span class="s1"> at </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">ckpt_save_path</span><span class="p">))</span>
    

  <span class="c1"># tensorboard</span>
  <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s2">"train_loss"</span><span class="p">,</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s2">"train_acc"</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Epoch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
                                                <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> 
                                                <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Time taken for 1 epoch: </span><span class="si">{}</span><span class="s1"> secs</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    
  <span class="c1"># intermidate translation</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">en</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">translation</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  
  <span class="c1"># attention vis by time</span>
  <span class="k">for</span> <span class="n">s_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">track_sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="s2">"s_</span><span class="si">{:02d}</span><span class="s2">_attn_epoch_</span><span class="si">{:02d}</span><span class="s2">.png"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s_idx</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving checkpoint for epoch 1 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-1
Epoch 1 Loss 3.9216 Accuracy 0.0970
Time taken for 1 epoch: 1025.4106004238129 secs

Saving checkpoint for epoch 2 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-2
Epoch 2 Loss 2.5735 Accuracy 0.2225
Time taken for 1 epoch: 577.9302551746368 secs

Saving checkpoint for epoch 3 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-3
Epoch 3 Loss 2.2195 Accuracy 0.2610
Time taken for 1 epoch: 572.8449900150299 secs

Saving checkpoint for epoch 4 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-4
Epoch 4 Loss 2.0447 Accuracy 0.2810
Time taken for 1 epoch: 575.2676854133606 secs

Saving checkpoint for epoch 5 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-5
Epoch 5 Loss 1.9109 Accuracy 0.2980
Time taken for 1 epoch: 573.6397109031677 secs

Saving checkpoint for epoch 6 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-6
Epoch 6 Loss 1.8018 Accuracy 0.3124
Time taken for 1 epoch: 576.986958026886 secs

Saving checkpoint for epoch 7 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-7
Epoch 7 Loss 1.7228 Accuracy 0.3231
Time taken for 1 epoch: 574.5642266273499 secs

Saving checkpoint for epoch 8 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-8
Epoch 8 Loss 1.6618 Accuracy 0.3318
Time taken for 1 epoch: 571.6106472015381 secs

Saving checkpoint for epoch 9 at /content/gdrive/My Drive/nmt/checkpoints/6layers_256d_8heads_1024dff_90train_perc/ckpt-9
Epoch 9 Loss 1.6111 Accuracy 0.3387
Time taken for 1 epoch: 569.6304190158844 secs

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TODO_2">TODO<a class="anchor-link" href="#TODO">&para;</a></h2><ul>
<li>看 decoder 的 self-attention</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluate">Evaluate<a class="anchor-link" href="#Evaluate">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following steps are used for evaluation:</p>
<ul>
<li>Encode the input sentence using the Portuguese tokenizer (<code>tokenizer_pt</code>). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.</li>
<li>The decoder input is the <code>start token == tokenizer_en.vocab_size</code>.</li>
<li>Calculate the padding masks and the look ahead masks.</li>
<li>The <code>decoder</code> then outputs the predictions by looking at the <code>encoder output</code> and its own output (self-attention).</li>
<li>Select the last word and calculate the argmax of that.</li>
<li>Concatentate the predicted word to the decoder input as pass it to the decoder.</li>
<li>In this approach, the decoder predicts the next word based on the previous words it predicted.</li>
</ul>
<p>Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def evaluate(inp_sentence):</span>
<span class="c1">#   start_token = [tokenizer_zh.vocab_size]</span>
<span class="c1">#   end_token = [tokenizer_zh.vocab_size + 1]</span>
  
<span class="c1">#   # inp sentence is portuguese, hence adding the start and end token</span>
<span class="c1">#   inp_sentence = start_token + tokenizer_zh.encode(inp_sentence) + end_token</span>
<span class="c1">#   encoder_input = tf.expand_dims(inp_sentence, 0)</span>
  
<span class="c1">#   # as the target is english, the first word to the transformer should be the</span>
<span class="c1">#   # english start token.</span>
<span class="c1">#   decoder_input = [tokenizer_en.vocab_size]</span>
<span class="c1">#   output = tf.expand_dims(decoder_input, 0)</span>
    
<span class="c1">#   for i in range(MAX_LENGTH):</span>
<span class="c1">#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(</span>
<span class="c1">#         encoder_input, output)</span>
  
<span class="c1">#     # predictions.shape == (batch_size, seq_len, vocab_size)</span>
<span class="c1">#     predictions, attention_weights = transformer(encoder_input, </span>
<span class="c1">#                                                  output,</span>
<span class="c1">#                                                  False,</span>
<span class="c1">#                                                  enc_padding_mask,</span>
<span class="c1">#                                                  combined_mask,</span>
<span class="c1">#                                                  dec_padding_mask)</span>
    
<span class="c1">#     # select the last word from the seq_len dimension</span>
<span class="c1">#     predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)</span>

<span class="c1">#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)</span>
    
<span class="c1">#     # return the result if the predicted_id is equal to the end token</span>
<span class="c1">#     if tf.equal(predicted_id, tokenizer_en.vocab_size+1):</span>
<span class="c1">#       return tf.squeeze(output, axis=0), attention_weights</span>
    
<span class="c1">#     # concatentate the predicted_id to the output which is given to the decoder</span>
<span class="c1">#     # as its input.</span>
<span class="c1">#     output = tf.concat([output, predicted_id], axis=-1)</span>

<span class="c1">#   return tf.squeeze(output, axis=0), attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def plot_attention_weights(attention, sentence, result, layer):</span>
<span class="c1">#   fig = plt.figure(figsize=(8, 16))</span>
  
<span class="c1">#   sentence = tokenizer_en.encode(sentence)</span>
  
<span class="c1">#   attention = tf.squeeze(attention[layer], axis=0)</span>
  
<span class="c1">#   for head in range(attention.shape[0]):</span>
<span class="c1">#     ax = fig.add_subplot(4, 2, head+1)</span>
    
<span class="c1">#     # plot the attention weights</span>
<span class="c1">#     ax.matshow(attention[head][:-1, :], cmap='viridis')</span>

<span class="c1">#     fontdict = {'fontsize': 10, "fontproperties": zhfont}</span>
    
<span class="c1">#     ax.set_xticks(range(len(sentence)+2))</span>
<span class="c1">#     ax.set_yticks(range(len(result)))</span>
    
<span class="c1">#     ax.set_ylim(len(result)-1.5, -0.5)</span>
        
<span class="c1">#     ax.set_xticklabels(</span>
<span class="c1">#         ['&lt;start&gt;']+[tokenizer_en.decode([i]) for i in sentence]+['&lt;end&gt;'], </span>
<span class="c1">#         fontdict=fontdict, rotation=90)</span>
    
<span class="c1">#     ax.set_yticklabels([tokenizer_zh.decode([i]) for i in result </span>
<span class="c1">#                         if i &lt; tokenizer_zh.vocab_size], </span>
<span class="c1">#                        fontdict=fontdict)</span>
    
<span class="c1">#     ax.set_xlabel('Head {}'.format(head+1))</span>
  
<span class="c1">#   plt.tight_layout()</span>
<span class="c1">#   plt.show()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can pass different layers and attention blocks of the decoder to the <code>plot</code> parameter.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&para;</a></h2><p>In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.</p>
<p>Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create <a href="https://arxiv.org/abs/1810.04805">BERT</a> and train state of the art models. Futhermore, you can implement beam search to get better predictions.</p>
</div>
</div>
</div>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                        <a href="/tag/nlp.html" rel="tag">NLP</a>
                        <a href="/tag/tensorflow.html" rel="tag">Tensorflow</a>
                    </span>

                </p>



                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary " style="color: #FFFFFF">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__next">
                            <a href="/generate-anime-using-cartoongan-and-tensorflow2.html" rel="next">
                                <span>Next Post</span>
                                用 CartoonGAN 及 TensorFlow 2 生成新海誠與宮崎駿動畫
                            </a>
                        </div>
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>



    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="/theme/images/logo.png" alt="Homepage"></a>
                </div>

                <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>                </ul>
            </div>
        </div>

        <div class="row footer-bottom">
            <div class="col-twelve">
                <div class="copyright">
                    <span>Powered by <a href="http://getpelican.com/" target="_blank">Pelican</a></span>
                    <span>© Copyright Hola 2017</span>
                    <span>Design by <a href="https://www.styleshout.com/" target="_blank">styleshout</a></span>
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>
        </div> <!-- end footer-bottom -->
    </footer> <!-- end footer -->


    <div id="preloader">
        <div id="loader"></div>
    </div>

        <!-- Javascript
    ================================================== -->
    <script src="/theme/js/jquery-3.2.1.min.js"></script>
    <script src="/theme/js/plugins.js"></script>
    <script src="/theme/js/main.js"></script>
    <script type='text/javascript' src='/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script>


</body>
</html>