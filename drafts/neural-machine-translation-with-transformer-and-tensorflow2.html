<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="本文向讀者介紹近年深度學習在機器翻譯任務上的一些重要模型及概念。我們也會實際使用 TensorFlow 2 來實作一個可以將英文句子翻譯成中文的 Transformer。Transformer 是一個近年十分熱門、基於自注意機制的 Seq2Seq 模型。透過實作並瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等 ML 任務之上並達到卓越成果。" />
        <meta name="keywords" content="自然語言處理, NLP, Tensorflow" />
        <meta name="tags" content="自然語言處理" />
        <meta name="tags" content="NLP" />
        <meta name="tags" content="Tensorflow" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/Tour_de_babel.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/drafts/neural-machine-translation-with-transformer-and-tensorflow2.html" />
        <meta property="og:title" content="淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯" />
        <meta property="og:description" content="本文向讀者介紹近年深度學習在機器翻譯任務上的一些重要模型及概念。我們也會實際使用 TensorFlow 2 來實作一個可以將英文句子翻譯成中文的 Transformer。Transformer 是一個近年十分熱門、基於自注意機制的 Seq2Seq 模型。透過實作並瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等 ML 任務之上並達到卓越成果。" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯">淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯</a><ul><li><a class="toc-href" href="#基礎知識" title="基礎知識">基礎知識</a></li><li><a class="toc-href" href="#機器翻譯近代史" title="機器翻譯近代史">機器翻譯近代史</a><ul><li><a class="toc-href" href="#統計機器翻譯：基於短語的翻譯" title="統計機器翻譯：基於短語的翻譯">統計機器翻譯：基於短語的翻譯</a></li><li><a class="toc-href" href="#神經機器翻譯：Encoder-Decoder-模型" title="神經機器翻譯：Encoder-Decoder 模型">神經機器翻譯：Encoder-Decoder 模型</a></li><li><a class="toc-href" href="#Encoder-Decoder-模型-+-注意機制" title="Encoder-Decoder 模型 + 注意機制">Encoder-Decoder 模型 + 注意機制</a></li><li><a class="toc-href" href="#Transformer：Seq2Seq-模型-+-自注意機制" title="Transformer：Seq2Seq 模型 + 自注意機制">Transformer：Seq2Seq 模型 + 自注意機制</a></li></ul></li><li><a class="toc-href" href="#Transformer-實作時間_1" title="Transformer 實作時間">Transformer 實作時間</a></li><li><a class="toc-href" href="#Experiment" title="Experiment">Experiment</a></li><li><a class="toc-href" href="#Gdrive" title="Gdrive">Gdrive</a></li><li><a class="toc-href" href="#Setup-input-pipeline" title="Setup input pipeline">Setup input pipeline</a></li><li><a class="toc-href" href="#Positional-encoding" title="Positional encoding">Positional encoding</a></li><li><a class="toc-href" href="#Masking" title="Masking">Masking</a><ul><li><a class="toc-href" href="#TODO" title="TODO">TODO</a></li></ul></li><li><a class="toc-href" href="#Scaled-dot-product-attention_1" title="Scaled dot product attention">Scaled dot product attention</a></li><li><a class="toc-href" href="#Multi-head-attention" title="Multi-head attention">Multi-head attention</a></li><li><a class="toc-href" href="#Point-wise-feed-forward-network" title="Point wise feed forward network">Point wise feed forward network</a></li><li><a class="toc-href" href="#Encoder-and-decoder" title="Encoder and decoder">Encoder and decoder</a><ul><li><a class="toc-href" href="#Encoder-layer" title="Encoder layer">Encoder layer</a></li><li><a class="toc-href" href="#Decoder-layer" title="Decoder layer">Decoder layer</a></li><li><a class="toc-href" href="#Encoder" title="Encoder">Encoder</a></li><li><a class="toc-href" href="#Decoder" title="Decoder">Decoder</a></li></ul></li><li><a class="toc-href" href="#Create-the-Transformer_1" title="Create the Transformer">Create the Transformer</a></li><li><a class="toc-href" href="#Set-hyperparameters" title="Set hyperparameters">Set hyperparameters</a><ul><li><a class="toc-href" href="#Setup-experiment-path" title="Setup experiment path">Setup experiment path</a></li></ul></li><li><a class="toc-href" href="#Optimizer_1" title="Optimizer">Optimizer</a></li><li><a class="toc-href" href="#Loss-and-metrics" title="Loss and metrics">Loss and metrics</a></li><li><a class="toc-href" href="#Training-and-checkpointing" title="Training and checkpointing">Training and checkpointing</a><ul><li><a class="toc-href" href="#Chinese-font-setup-for-matplotlib" title="Chinese font setup for matplotlib">Chinese font setup for matplotlib</a></li><li><a class="toc-href" href="#TODO_1" title="TODO">TODO</a></li></ul></li><li><a class="toc-href" href="#Tensorboard_1" title="Tensorboard">Tensorboard</a></li><li><a class="toc-href" href="#TODO_2" title="TODO">TODO</a></li><li><a class="toc-href" href="#Evaluate" title="Evaluate">Evaluate</a></li><li><a class="toc-href" href="#Summary" title="Summary">Summary</a></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

            <div id="leftNavImage_transformer" class="overlay" style="background-color:white" onclick="closeLeftNavImage('leftNavImage_transformer')">
              <div class="overlay-content">
                <img src="https://leemeng.tw/theme/images/left-nav/transformer.jpg">
              </div>
            </div>


    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/Tour_de_babel.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                            <a href="https://leemeng.tw/tag/nlp.html" rel="tag">NLP</a>
                            <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">Tensorflow</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/drafts/neural-machine-translation-with-transformer-and-tensorflow2.html" title="">
                            淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-06-03 (Mon)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>



    <!--custom images with icon shown on left nav-->
            <div id="left-nav-image-wrap-transformer">
                <a title="顯示/隱藏 Transformer 架構圖">
                    <i class="im im-picture-o" aria-hidden="true" onclick="toggleLeftNavImage('leftNavImage_transformer')"></i>
                </a>
            </div>


</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        那時，全世界的語言都一樣。人們說：『來吧，我們要建一座塔，塔頂通天，為了揚我們的名，免得我們被分散到世界各地。』耶和華說：『看哪！他們成爲一樣的人民、用同樣的語言。如今既蓋起塔來，以後就沒有他們無法完成的事情了。我們下去！在那裏變亂他們的口音，使他們的言語彼此不通。』
                        <br/>
<span style="float:right;margin-right: 1.5rem">─ 《創世記》第十一章</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是聖經中著名的<a href="https://zh.wikipedia.org/wiki/%E5%B7%B4%E5%88%A5%E5%A1%94">巴別塔</a>橋段，用來解釋為何當今世上有那麼多種語言。當年的上帝或許過於杞人憂天，但近年多虧了<a href="https://zh.wikipedia.org/zh-hant/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度學習</a>，<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">機器翻譯</a>的快速發展讓人不禁覺得，或許巴別塔很快就不再只是虛幻傳說了。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/google-translate.jpg">
<source src="https://leemeng.tw/images/transformer/google-translate.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        以往被視為非常困難的中 -&gt; 英翻譯如今在深度學習的加持下也有不錯的水準
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>機器翻譯的研究之所以如此重要且迷人，是因為它將有機會讓未來任何人都不受語言的限制，獲得世界上任何他或她想要的資訊與知識。</p>
<p>而在這篇文章裡頭，我們首先會花點時間來回顧一些機器翻譯領域裡頭重要的概念，接著在此基礎之上利用最新的 <a href="https://www.tensorflow.org/">TensorFlow 2</a> 來實作一個可以將英文句子翻譯成中文的神經網路架構：<a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/transformer-high-level-view.png"/>
</center>
<center>
                        利用 Transformer 將法文句子翻譯成英文
                        （<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是一個非常簡化的示意圖。Transformer 實際上是一種基於自注意機制的 <a href="https://youtu.be/ZjfjPzXw6og?t=3208">Seq2Seq 模型</a>，近年在<a href="https://paperswithcode.com/task/image-captioning">圖像描述</a>、<a href="https://zh.wikipedia.org/wiki/%E8%81%8A%E5%A4%A9%E6%A9%9F%E5%99%A8%E4%BA%BA">聊天機器人</a>、<a href="https://zh.wikipedia.org/zh-hant/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB">語音辨識</a>以及機器翻譯等各大領域大發異彩。但因為其相對複雜，到現在還是有種現象：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        懂 Transformer 的人用得很開心，不懂的人還是不懂。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>當然這並不僅限於 Transformer，因為深度學習及 AI 的研究範圍實在太廣了。但希望這篇文章能讓有興趣的人開始理解神經機器翻譯在做些什麼以及 Transformer 的內部架構與精神。（在<a href="#基礎知識">基礎知識</a>一節會看到我推薦你先掌握的基礎清單）</p>
<p>當我們完成本文並訓練出一個 Transformer 以後，除了可以英翻中以外，我們還能清楚地了解其是如何利用強大的<a href="https://www.youtube.com/watch?v=jd9DtlR90ak&amp;feature=youtu.be">注意機制</a>（我們在 <a href="#Encoder-Decoder-模型-+-注意機制">Encoder-Decoder 模型 + 注意機制</a>一節會仔細探討此概念）來做到精準且自然的翻譯。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/en-to-ch-attention-map.png"/>
</center>
<center>
                        Transformer 在將英文句子翻譯成中文時會「關注」需要注意的英文詞彙來生成對應的中文字詞
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>除了翻譯出來的中文正確無誤以外，從上圖你可以發現很有趣的現象。</p>
<p>給定左側的英文，Transformer 在生成其對應的中文翻譯時都會給每個英文詞彙不同的「注意程度」。小方格越亮則代表模型在生成某中文字時放越多的注意力在左側對應的英文詞彙上。</p>
<p>仔細看你會發現這個已經訓練好的 Transformer 在翻譯：</p>
<ul>
<li>「必」、「須」時會關注「must」</li>
<li>「希」、「望」時會關注「hope」</li>
<li>「公」、「民」時會關注「citizens」</li>
</ul>
<p>乍看之下好像稀鬆平常，但事實上我們在訓練模型時並不會告訴它這些詞彙之間的對應關係或是任何語言學的知識。我們就只是餵給它多組相同意思的中英句子，並讓它自己學會怎麼做翻譯。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        好黑魔法，不學嗎？
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在英翻中的情境下，神經網路要做的事情就是讀入左側的英文句子，接著生成右側的中文句子（繁中對英文的翻譯資料集稀少，此文將以簡體為例）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/en-zh-training-sentences.jpg"/>
</center>
<center>
                        訓練資料是多組相同語義的成對中英句子（當然仍需前處理）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="基礎知識">基礎知識<a class="anchor-link" href="#基礎知識">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我在文中會盡量言簡意賅地介紹所有你需要了解的相關知識，但就像在<a href="https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html">天龍八部</a>或是眾多武俠小說都有提過的重要準則：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        武功修習有先後順序，勿求一步登天。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>儘管在 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">2017 年就已被提出</a>，本文即將探討並實作的 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Transformer</a> 仍算是相當進階的神經網路架構。因此具備以下的基礎知識能幫助你更順利地理解本文內容：</p>
<ul>
<li>一點點<a href="https://demo.leemeng.tw/">卷積神經網路</a>的概念</li>
<li>理解<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">循環神經網路</a>的運算方式</li>
<li>基本的<a href="http://research.sinica.edu.tw/nlp-natural-language-processing-chinese-knowledge-information/">自然語言處理</a>知識</li>
<li>基本的<a href="https://youtu.be/uUrt8xgdMbs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW">線性代數</a>如矩陣相乘運算</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/nlp-intro.jpg"/>
</center>
<center>
                        中研院這篇文章清楚地說明了自然語言處理在中文上的研究與應用
                        （圖片來源：<a href="http://research.sinica.edu.tw/nlp-natural-language-processing-chinese-knowledge-information/" target="_blank">研之有物</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>希望這樣的要求沒把你嚇跑，因為實際上你可能會需要更多基礎、或者自己做點額外閱讀來完整理解本文內容。</p>
<p>我能給你的最好建議就是在需要了解更多細節時，參考這節內容或是文內說明概念時所附上的參考文獻。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        積極點，想知道什麼就點擊各圖片底下描述的「圖片來源」去進一步了解就對了。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>前言很長，但好戲才在後頭。如果你已經準備好進入神經機器翻譯的世界的話，現在就讓我們正式開始這趟旅程吧！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="機器翻譯近代史">機器翻譯近代史<a class="anchor-link" href="#機器翻譯近代史">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>鑑往知來。了解一點機器翻譯的歷史以及 Transformer 是怎麼跑出來的會對實作很有幫助。</p>
<p>機器翻譯（<strong>M</strong>achine <strong>T</strong>ranslation）本身的概念<a href="https://zh.wikipedia.org/zh-tw/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91#%E6%AD%B7%E5%8F%B2">最早可追溯到 17 世紀</a>。自從那開始，人們嘗試並研究了各式各樣的方法，寫了一大堆規則、蒐集了數以萬計的翻譯結果來嘗試自動化翻譯。隨著時代演進，我們有了：</p>
<ul>
<li>基於規則的機器翻譯 RBMT</li>
<li>基於範例的機器翻譯 EBMT</li>
<li>統計機器翻譯 SMT</li>
<li>近年的神經機器翻譯 NMT</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/mt-history.jpg"/>
</center>
<center>
                        近代機器翻譯發展簡史
                        （<a href="https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很多遠古時代的東西我們不會討論，而 NMT 當然是本文的重點。不過在那之前讓我們非常簡短地看一下 SMT。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="統計機器翻譯：基於短語的翻譯">統計機器翻譯：基於短語的翻譯<a class="anchor-link" href="#統計機器翻譯：基於短語的翻譯">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>機器翻譯的歷史很長，但一直要到 21 世紀初期<a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">統計機器翻譯（<strong>S</strong>tatistical <strong>M</strong>achine <strong>T</strong>ranslation，簡稱 SMT）</a>技術成熟以後，機器翻譯的品質才稍微使人滿意。其中最知名的例子當屬 <a href="https://ai.googleblog.com/2006/04/statistical-machine-translation-live.html">Google 在 2006 年發布的 SMT 翻譯系統</a>。</p>
<p>不限於 Google，當時不少最先進的 SMT 系統都採用了<a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Phrase-based_translation">基於短語的機器翻譯（Phrase-Based MT）</a> 演算法。PBMT 最大的特色是先將來源語言（Source Language）的句子切成短語或是詞彙，接著大致上獨立地將這些詞彙翻譯成目標語言（Target Language）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/pbmt.jpg"/>
</center>
<center>
                        基於短語的 SMT（Phrase-Based SMT）
                        （<a href="https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>PBMT 的翻譯結果相較於早年基於規則（Rule-Based）的手法已經進步很多，但仍然需要大量的<a href="https://zh.wikipedia.org/wiki/%E5%B9%B3%E8%A1%8C%E8%AF%AD%E6%96%99">平行語料</a>、對齊語料來取得較好的結果。且因為是以短語為單位在做翻譯，這些短語拼湊出來的句子仍然不夠自然。</p>
<p>如果你跟我一樣有用過早年的 Google 翻譯，應該還能隱約記得當年那些充斥著「機械感」的翻譯結果。</p>
<p>（如果你有當年 Google 翻譯結果的截圖的話歡迎提供）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="神經機器翻譯：Encoder-Decoder-模型">神經機器翻譯：Encoder-Decoder 模型<a class="anchor-link" href="#神經機器翻譯：Encoder-Decoder-模型">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>顧名思義，神經機器翻譯 NMT 即代表使用<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">類神經網路（Neural Network）</a>來做機器翻譯。</p>
<p>不管是英文、法文還是中文，一個自然語言的句子基本上可以被視為一個有時間順序的序列數據（Sequence Data）。而<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1">我們曾提過 RNN 很適合用來處理有時間關係的序列數據</a>。給定一個向量序列，RNN 就是回傳一個一樣長度的向量序列作為輸出。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/nlp-kaggle-intro/rnn-animate.gif"/>
</center>
<center>
                        RNN 很適合拿來處理具有時間順序的序列數據（下方的詞在丟入 RNN 前會被轉成詞向量）
                        （<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>當我們把來源語言以及目標語言的句子都視為一個獨立的序列以後，機器翻譯事實上就是一個<a href="https://youtu.be/ZjfjPzXw6og">序列生成（Sequence Generation）</a>任務：對一個輸入序列（來源語言）做些有意義的轉換與處理以後，輸出一個新的序列（目標語言）。</p>
<p>而在深度學習時代，我們一般會使用以 RNN 為基礎的 <a href="https://youtu.be/ZjfjPzXw6og?t=3208">Encoder-Decoder 架構（又被稱作 Sequence to Sequence / Seq2Seq 模型）</a>來做序列生成：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/seq2seq-animate.jpg">
<source src="https://leemeng.tw/images/transformer/seq2seq-animate.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        一個以 RNN 為基礎的 Encoder-Decoder / Seq2Seq 模型將法文翻譯成英文的步驟
                        （<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Seq2Seq 模型裡頭 Encoder 跟 Decoder 是各自獨立的 RNN。Encoder 把輸入的句子做處理後所得到的向量（圖中的 <code>Hidden State#3</code>）交給 Decoder 來生成目標語言。</p>
<p>你可以想像兩個語義相同的法英句子雖然使用的語言、語順不一樣，但因為它們有相同的語義，Encoder 在將整個<strong>法文</strong>句子濃縮成一個嵌入空間（Embedding Space）中的向量後，Decoder 能利用隱含在該向量中的語義資訊來重新生成具有相同意涵的<strong>英文</strong>句子。</p>
<p>這樣的模型就像是在模擬人類做翻譯的<a href="https://zh.wikipedia.org/zh-tw/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91#%E7%BF%BB%E8%AD%AF%E6%B5%81%E7%A8%8B">兩個主要過程</a>：</p>
<ul>
<li>（Encoder）解譯來源文字的文意</li>
<li>（Decoder）重新編譯該文意至目標語言</li>
</ul>
<p>當然人類在做翻譯時有更多步驟、也會考慮更多東西，但 Seq2Seq 模型的表現已經很不錯了。</p>
<p>有些人閱讀到這裡可能會問：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        如果我們利用 Seq2Seq 模型將多種語言的句子都轉換到某個嵌入空間裡頭，該空間會長成什麼樣子呢？是相同語言的句子靠得比較近，還是不同語言但擁有同語義的句子會靠得比較近呢？
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是一個很好的研究問題。</p>
<p>而如果我們試著把這個問題圖像化，則結果可能長得像這樣：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/multi-lang-emb.jpg"/>
</center>
<center>
                        大哉問：神經網路將句子轉換完所形成的向量空間比較靠近左邊還是右邊？
                        （<a href="https://youtu.be/ulLx2iPTIcs?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;t=1035" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>圖中的點代表不同句子，不同顏色則代表不同語言。如果結果是左邊，代表神經網路並沒有創出一個「語義」空間，而只是把不同語言都投射到該嵌入空間裡頭的不同位置，接著才在該空間裡進行不同語言之間的轉換（中轉英、英轉法 etc.）。</p>
<p>我們比較想要的是右邊的情況：無關語言，只要句子的語義接近，彼此的距離就相近的語義空間。</p>
<p>而 <a href="https://aclweb.org/anthology/Q17-1024">Google 在 2016 年的研究結果</a>發現，在此空間裡頭語言相異但擁有同語義的句子之間的距離 <code>d1</code>，要比同語言但不同語義的句子之間的距離 <code>d2</code> 要小得多（即  <code>d1 &lt;&lt; d2</code>）。</p>
<p>換句話說，在此空間中同語義的句子會靠得比較近，我們實際得到的空間比較像右邊。</p>
<p>而如果我們將這些句子做 <a href="https://distill.pub/2016/misread-tsne/">t-SNE</a> ，甚至可以得到這樣的結果：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/gnmt-multilingual.jpg">
<source src="https://leemeng.tw/images/transformer/gnmt-multilingual.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        在 Seq2Seq 模型創造出來的「語義」空間裡頭，不同語言但同語義的句子彼此相當接近
                        （<a href="https://projector.tensorflow.org/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>此研究告訴我們，只要對自然語言做正確的轉換，就能將語言相異但同語義的句子都轉換成彼此距離相近的語義向量，並以此做出好的翻譯。</p>
<p>以下是我隨意挑選出來的一組句子，它們在該空間裡的距離相近：</p>
<div class="highlight"><pre><span></span>英文：
From low-cost pharmacy brand moisturizers to high-priced cosmetics brand moisturizers, competition is fierce.

日文：
低価格の薬品ブランドの保湿剤から高価な百貨店の化粧品ブランドのためには, 競争が激しい

韓文：
싸구려백화점화장품브랜드 moisturizers 에 저렴한약국브랜드 moisturizers 에서 , 경쟁이큰있습니다
</pre></div>
<p>這些句子都代表著類似的意思：「從低價的保濕劑到高價的化妝品牌，競爭都十分激烈」。</p>
<p>如果你想進一步了解這個視覺化結果，可以閱讀 <a href="https://youtu.be/ulLx2iPTIcs?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;t=789">Google Brain 的詳細解說</a>或是上 <a href="https://projector.tensorflow.org/">Embedding Projector</a> 自己試看看。</p>
<p>另外值得注意的是，機器翻譯本身是一種<a href="https://youtu.be/ZjfjPzXw6og?t=2816">有條件的序列生成任務（Conditional Sequence Generation）</a>：給定一個特定的輸入句子（文字序列），依此條件輸出另外一個句子（文字序列）。這跟在<a href="https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html">讓 AI 寫點金庸</a>一文中會隨機生成天龍八部文章的<a href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B">語言模型（Language Model）</a>是有所差異的：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/lstm-sequence-generation.jpg">
<source src="https://leemeng.tw/images/transformer/lstm-sequence-generation.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        隨機序列生成的例子：一個以 LSTM 實作的簡單語言模型
                        （<a href="https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>一般來說，語言模型可以在不給定任何輸入的情況下生成非常隨機的文字序列；但針對機器翻譯這種有條件的序列生成任務，我們通常希望給定相同輸入，輸出的結果越穩定越好（或是每次都一模一樣）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder-Decoder-模型-+-注意機制">Encoder-Decoder 模型 + 注意機制<a class="anchor-link" href="#Encoder-Decoder-模型-+-注意機制">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>好啦，你現在應該已經了解如何使用 Seq2Seq 模型來做 NMT 了，不過現在讓我們再次複習其運作方式。這次我們把用 RNN 實作的 Encoder / Decoder 在每個時間點做的事情從左到右一字排開：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="{filename}images/transformer/seq2seq-unrolled-no-attention.jpg">
<source src="https://leemeng.tw/images/transformer/seq2seq-unrolled-no-attention.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        以 RNN 為基礎的 Seq2Seq 模型做 NMT 的流程
                        （<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>基本款的 Seq2Seq 模型表現得不錯，但其實有可以改善的地方。你有看出來了嗎？上圖的輸入句子只有 3 個詞彙，但如果輸入句子很長呢？</p>
<p>我們前面曾提過 Seq2Seq 模型裡的一個重要假設是 Encoder 能把輸入句子的語義 / 文本脈絡全都壓縮成<strong>一個</strong>固定維度的語義向量。之後 Decoder 只要利用該向量裡頭的資訊就能重新生成具有相同意義，但不同語言的句子。</p>
<p>但你可以想像當我們只有一個向量的時候，是不太可能把一個很長的句子的所有資訊打包起來的。</p>
<p>這時候怎麼辦呢？</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        與其只把 Encoder 處理完句子產生的最後「一個」向量交給 Decoder 並要求其從中萃取整句資訊，不如將 Encoder 在處理每個詞彙後所生成的「所有」輸出向量都交給 Decoder，讓 Decoder 自己決定在生成新序列的時候要把「注意」放在 Encoder 的哪些輸出向量上面。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這事實上就是<a href="https://www.youtube.com/watch?v=jd9DtlR90ak&amp;feature=youtu.be">注意機制（Attention Mechanism）</a>的中心思想：提供更多資訊給 Decoder，並透過類似資料庫存取的概念，令其自行學會該怎麼提取資訊。兩篇核心論文分別在 <a href="https://arxiv.org/abs/1409.0473">2014 年 9 月</a>及 <a href="https://arxiv.org/abs/1508.04025">2015 年 8 月</a>釋出，概念不難但威力十分強大。</p>
<p>以下就是將注意機制加到 Seq2Seq 模型後的結果：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/seq2seq-unrolled-with-attention.jpg">
<source src="https://leemeng.tw/images/transformer/seq2seq-unrolled-with-attention.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        注意機制讓 Decoder 在生成新序列時能查看 Encoder 裡所有可能有用的語義向量
                        （<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你可以拉回去跟沒有注意機制的 Seq2Seq 模型比較一下差異。</p>
<p>現在你會看到 Encoder 把處理完每個詞彙所產生的向量都交給 Decoder 了。且透過注意機制，Decoder 在生成新序列的每個元素時都能<strong>動態地</strong>考慮自己要看哪些 Encoder 的向量（還有決定從中該擷取多少資訊），因此這種架構又被稱作<a href="https://youtu.be/ZjfjPzXw6og?t=3528">動態的條件序列生成（Dynamic Conditional Generation）</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/seq2seq_detail.jpg">
<source src="https://leemeng.tw/images/transformer/seq2seq_detail.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        法翻英時，Decoder 在生成每個英文詞彙時都在 Encoder 的每個輸出向量上放不同的注意程度
                        （<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>構想並實際證明其有效的研究者們十分厲害，且其概念也挺符合人類直覺的，對吧？</p>
<p>為了方便讀者理解，上面動畫實際上隱藏了一些細節：</p>
<ul>
<li>呈現算好的注意程度而不是計算過程</li>
<li>Encoder / 跟 Decoder 的實際架構</li>
</ul>
<p>既然是深度學習，Encoder / Decoder 一般來說都是由多個 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a> / <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a> 等 RNN Layers 所疊起來的。而注意機制在這種情境下實際的運作方式如下：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/attention_mechanism_luong.jpg"/>
</center>
<center>
                        英翻法情境下，Decoder 在第一個時間點進行的注意機制
                        （<a href="https://github.com/tensorflow/nmt#background-on-the-attention-mechanism" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>左右兩邊分別是 Encoder 與 Decoder ，縱軸則是多層的神經網路區塊 / 層。</p>
<p>雖然上張動畫是法翻英（這邊是英翻法），但該動畫也是以一樣的概念將圖中的注意權重（attention weights ）視覺化出來（注意權重和為 1）。</p>
<p>現在讓我們看一下注意機制實際的計算步驟。在 Decoder 的每個時間點，我們都會進行注意機制以讓 Decoder 從 Encoder 取得語境資訊：</p>
<ol>
<li>拿 Decoder 當下的紅色隱狀態向量 <code>ht</code> 跟 Encoder 所有藍色隱狀態向量 <code>hs</code> 做比較，利用 <code>score</code> 函式計算出 <code>ht</code> 對每個 <code>hs</code> 的注意程度</li>
<li>以此注意程度為權重，<strong>加權平均</strong>所有 Encoder 隱狀態 <code>hs</code> 以取得上下文向量 <code>context vector</code></li>
<li>將此上下文向量與 Decoder 隱狀態結合成一個注意向量 <code>attention vector</code> 並作為該時間的輸出</li>
<li>該注意向量會作為 Decoder 下個時間點的輸入</li>
</ol>
<p>定義 <code>score</code> 函式的方式不少，現在就先讓我們假設有這麼一個函式。</p>
<p>至此為止，你應該已經能夠看懂注意機制的計算公式：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/attention-equation.jpg"/>
</center>
<center>
                        注意機制前 3 步驟的數學式子
                        （<a href="https://github.com/tensorflow/nmt#background-on-the-attention-mechanism" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而之所以稱為注意權重（attention weights），是因為注意機制可以被視為是一個學習來源語言和目標語言<strong>每一個單詞之間關係</strong>的小型神經網路，而這些權重是該神經網路的參數。</p>
<p>我們在<a href="#TODO">後面的章節</a>會實際看到，在訓練還沒開始前，這些權重都是隨機且無意義的。是透過訓練，神經網路才知道該為這些權重賦予多少值。</p>
<p>你也會發現我在文中提及多次的「注意程度」就是這裡的「注意權重」，而前者是一種擬人化的說法。你可以想像這些權重值讓當下的 Decoder 曉得該放多少關注在 Encoder 個別的隱狀態身上，並依此從它們身上取得上下文資訊（步驟 2）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        而事實上神經網路並沒有意識，因此也不會有感知層次上的「注意」。它學到的是讓注意機制產生最好結果的「參數權重」，而不是我們人類想像的「注意程度」。只有人類可以賦予神經網路裡頭的計算意義。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>有點扯遠了，畢竟這裡應該沒有人文學系的讀者。</p>
<p>讓我們拉回注意機制。</p>
<p>將此機制加入 Seq2Seq 模型後，NMT 系統的翻譯水準再次起飛。Google 在 2016 年推出的 <a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html">Google Neural Machine Translation system（GNMT）</a> 是一個知名的案例。除了注意機制以外，GNMT <a href="https://arxiv.org/abs/1609.08144">在 Encoder 跟 Decoder 都採用了多達 8 層的 LSTM 神經網路</a>，讓更多人見識到深度學習的威力。</p>
<p>跟 Google 10 年前推出的 PBMT 系統比起來，翻譯錯誤率平均下降了 60 %。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/nmt-model-fast.jpg">
<source src="https://leemeng.tw/images/transformer/nmt-model-fast.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        利用注意力機制的 GNMT 讓 Decoder 在生成「Knowledge」時能放注意力在 Encoder 處理完「知」與「識」的兩個輸出向量 e0 &amp; e1
                        （<a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>上圖為 GNMT 做中翻英的過程。Encoder 跟 Decoder 之間的線條代表注意力（Attention），線條越粗代表下面的 Decoder 在生成某英文字時越關注上方的某些中文字。模型自己學會在翻譯時該看來源句子中的哪些資訊，很聰明，不是嗎？</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>因為其卓越的翻譯品質，在 GNMT 推出的那段時間，搭配注意機制的 Seq2Seq 模型基本上就是拿來做 NMT 系統的不二人選。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/nmt-vs-pbmt.png"/>
</center>
<center>
                        NMT、PBMT 以及人類在中英翻譯時的結果比較
                        （<a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>話說當年 Google 導入 GNMT 時釋出了 8 個語言之間的對應翻譯，<a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">涵蓋了約 1/3 的世界人口以及超過 35 % 的 Google 翻譯查詢</a>，是機器翻譯發展的一個重要里程碑。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Transformer：Seq2Seq-模型-+-自注意機制">Transformer：Seq2Seq 模型 + 自注意機制<a class="anchor-link" href="#Transformer：Seq2Seq-模型-+-自注意機制">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>好酒沉甕底，萬眾矚目的時刻來了。</p>
<p>你現在可能在想：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        Seq2Seq 模型搭配注意機制感覺已經很猛了，難道還有什麼可以改善的嗎？
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>答案是肯定的 Yes。</p>
<p>不過這次問題不是出在 Encoder 跟 Decoder 中間交換的資訊不夠，也不是 Seq2Seq 架構本身有什麼問題，問題是出在我們是用 <strong>RNN</strong> 來實作 Encoder 以及 Decoder。</p>
<p><a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">循環神經網路 RNN</a> 時常被拿來處理序列數據，但其運作方式存在著一個困擾研究者已久的問題：無法有效地平行運算。以一個有 4 個元素的輸入序列為例：</p>
<div class="highlight"><pre><span></span>[a1, a2, a3, a4]
</pre></div>
<p>要獲得最後一個時間點的輸出向量 <code>b4</code> 得把整個輸入序列跑過一遍才行：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/rnn-vs-self-attn-layer.jpg"/>
</center>
<center>
                        自注意層可以做到跟雙向 RNN 一樣的事情，還可以平行運算
                        （<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/abs/1706.03762">Google 在 2017 年 6 月的一篇論文：Attention Is All You Need</a> 裡參考了注意機制，提出了<strong>自</strong>注意機制（Self-Attention mechanism）。這個機制不只跟 RNN 一樣可以處理序列數據，還可以平行運算。</p>
<p>以剛剛的輸入序列 <code>a[]</code> 為例：</p>
<div class="highlight"><pre><span></span>[a1, a2, a3, a4]
</pre></div>
<p>一個自注意層（Self-Attention Layer）可以利用矩陣運算在等同於 RNN 的一個時間點內就回傳所有 <code>bi</code> ，且每個 <code>bi</code> 都包含了整個輸入序列的資訊。相比之下，RNN 為了要取得序列中最後一個元素的輸出 <code>b4</code> 得經過 4 個時間點依序看過 <code>[a1, a2, a3, a4]</code> 。</p>
<p>在給定一個輸入序列的情境下，自注意機制的基本精神就是：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        在建立序列中每個元素的 repr. 時，同時去「注意」並擷取同個序列中其他元素的語義資訊。接著將這些語義資訊合併成上下文資訊並當作自己的 repr. 回傳。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>repr. 為 representation 縮寫。</p>
<p>雖然我們一直強調自注意機制的平行能力，如果你還記得我們在<a href="#Encoder-Decoder-模型-+-注意機制">上一節</a>講述的注意機制，就會發現在 Seq2Seq 架構裡頭自注意機制跟注意機制講的根本是同樣一件事情：</p>
<ul>
<li>注意機制讓 Decoder 在生成輸出元素的 repr. 時關注 Encoder 的輸出序列，從中獲得上下文資訊</li>
<li>自注意機制讓 Encoder 在生成輸入元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊</li>
<li>自注意機制讓 Decoder 在生成輸出元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你應該能看出點 pattern 了：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        注意機制跟自注意機制都是透過關注周遭來將上下文資訊匯總到某元素的 representation 裡頭，只是關注的對象（序列）有所不同罷了。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>透過新設計的自注意機制以及原有的注意機制，<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need 論文</a>作者們打造了一個完全不需使用 RNN 的 Seq2Seq 模型：Transformer。以下是非常簡化的版本，現在先讓我們「注意」看看哪邊用到了（自）注意機制：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/Transformer_decoder.png"/>
</center>
<center>
                        在 Transformer 裡頭共有 3 個地方用到（自）注意機制
                        （<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 Transformer 裡頭，Decoder 利用注意機制關注 Encoder（Encoder-Decoder Attention），而 Encoder 跟 Decoder 各自利用自注意機制關注自己處理的序列（Self-Attention）。無法平行運算的 RNN 完全消失，名符其實的 Attention is all you need.</p>
<p>以下則是 Transformer 實際上將英文句子翻譯到法文的過程：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/images/transformer/transformer-nmt-encode-decode.jpg">
<source src="https://leemeng.tw/images/transformer/transformer-nmt-encode-decode.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        用 Transformer 將英文句子翻譯到法文的例子
                        （<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>以 Transformer 實作的 NMT 系統基本上可以分為 6 個步驟：</p>
<ol>
<li>Encoder 為輸入序列裡的每個詞彙產生初始的 repr. （即詞向量），以空圈表示</li>
<li>利用自注意機制將序列中所有詞彙的語義資訊各自匯總成每個詞彙的 repr.，以實圈表示</li>
<li>Encoder 重複 N 次自注意機制，讓每個詞彙的 repr. 持續修正以完整納入上下文語義</li>
<li>跟一般的<a href="#Encoder-Decoder-%E6%A8%A1%E5%9E%8B-+-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6"> Seq2Seq 模型</a>相同，Decoder 利用注意機制關注 Encoder 的所有輸出並將其資訊納入當前生成元素的 repr.</li>
<li>Decoder 在生成每個法文字時也運用了自注意機制，關注自己之前已生成的元素，將其語義也納入之後生成的元素</li>
<li>Decoder 重複 N 次（自）注意機制以讓當前元素完整包含整體語義</li>
</ol>
<p>上面動畫的 N 為 3，這值可作變動。</p>
<p>如果你看懂這張圖的資訊流動以及運算方式，就等於瞭解 Transformer 的精神了，恭喜！</p>
<p>如果仍然有不明瞭的地方，可以多看幾遍動畫或是直接閱讀 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Google AI 部落格的原文</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/transformer/en-ge-bleu-comparison.png"/>
</center>
<center>
                        Transformer 釋出時與其他模型在英德翻譯資料集上的比較
                        （<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>自注意機制解開了 RNN 加在 GPU 上的拘束器。作者們用了 8 個 <a href="https://www.nvidia.com.tw/object/tesla-p100-tw.html">NVIDIA P100 GPU</a>，花了 3 天半訓練了一個 Transformer，而該模型在 <a href="http://statmt.org/wmt14/">WMT 2014</a>  英法 / 英德翻譯都取得了最高水準的成績。</p>
<p>跟其他模型相比，這訓練時間跟其創造的優異成績可以說是逆天的存在。自此「大注意時代」展開，該論文至今超過 1800 次引用，所有研究領域都被自注意機制相關的論文洗了一波。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/transformer/ramona-flwrs-1310216-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>沒趕上能開心洗論文的最佳時機也別傷心難過，對我們來說仍然有個十分重要的訊息：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        所有以 RNN 做過的研究，都可以用自注意機制來取代；所有用 Seq2Seq 架構實現過的應用，也都可以用 Transformer 來替換。效果可能更好，訓練速度更快。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這也是我決定寫這篇文章的理由之一。雖然本文是以機器翻譯的角度來介紹 Transformer，但事實上只要是能用 RNN 或 Seq2Seq 模型進行的研究領域，你都會看到已經有大量跟（自）注意機制或是 Transformer 有關的論文了：</p>
<ul>
<li>文本摘要（Text Summarization）</li>
<li>圖像描述（Image Captioning）</li>
<li>閱讀理解（Reading Comprehension）</li>
<li>語音辨識（Voice Recognition）</li>
<li>語言模型（Language Model）</li>
<li>聊天機器人（Chat Bot）</li>
<li>其他任何可以用 RNN 的潛在應用</li>
</ul>
<p>而現在還有很多基於 Transformer 的研究。</p>
<p>可以說不管你是對深度學習的什麼應用領域有興趣，了解（自）注意機制以及 Transformer 的運作方式幾乎都可以說是必經之路（如果你想走得夠前面的話）。</p>
<p>儘管理論跟實作之間還是存在差距，我相信你現在已經具備理解並實作 Transformer 的基礎知識了。</p>
<p>Let's get our hands dirty!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-實作時間_1">Transformer 實作時間<a class="anchor-link" href="#Transformer-實作時間">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>（選）<a href="https://www.tensorflow.org/alpha/tutorials/text/text_generation">以 RNN 實現文字生成</a></li>
<li>（選）<a href="https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention">以注意力機制實現 NMT</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>先想 input / output 是什麼，再實作</p>
<p>Components and implementation order</p>
<ul>
<li>~Masking (Masked Multi-Head Attention 起頭)~<ul>
<li>~padding mask~</li>
<li>~look ahead mask~</li>
</ul>
</li>
<li>~Scaled Dot-Product Attention~</li>
<li>~Multi-Head Attention~</li>
<li>~Feed Forward~</li>
<li>~Encoder block~<ul>
<li>~Residual Connection &amp; Layer Normalization~</li>
</ul>
</li>
<li>Decoder block</li>
<li>Encoder</li>
<li>Decoder</li>
<li>Positional Encoding</li>
<li>Transformer</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial trains a <a class="external" href="https://arxiv.org/abs/1706.03762">Transformer model</a> to translate Portuguese to English. This is an advanced example that assumes knowledge of <a href="text_generation.ipynb">text generation</a> and <a href="nmt_with_attention.ipynb">attention</a>.</p>
<p>The core idea behind the Transformer model is <em>self-attention</em>&mdash;the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections <em>Scaled dot product attention</em> and <em>Multi-head attention</em>.</p>
<p>A transformer model handles variable-sized input using stacks of self-attention layers instead of <a href="text_classification_rnn.ipynb">RNNs</a> or <a href="../images/intro_to_cnns.ipynb">CNNs</a>. This general architecture has a number of advantages:</p>
<ul>
<li>It make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8">StarCraft units</a>).</li>
<li>Layer outputs can be calculated in parallel, instead of a series like an RNN.</li>
<li>Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see <a href="https://arxiv.org/pdf/1903.03878.pdf">Scene Memory Transformer</a> for example).</li>
<li>It can learn long-range dependencies. This is a challenge in many sequence tasks.</li>
</ul>
<p>The downsides of this architecture are:</p>
<ul>
<li>For a time-series, the output for a time-step is calculated from the <em>entire history</em> instead of only the inputs and current hidden-state. This <em>may</em> be less efficient.   </li>
<li>If the input <em>does</em> have a  temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words. </li>
</ul>
<p>After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.</p>
<p><img alt="Attention heatmap" src="https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png" width="800"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">clear_output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tf-nightly-gpu-2.0-preview
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 TF2 裡頭 <code>tf.logging</code> 被 deprecated，我們可以直接用 <code>logging</code>  模組：
ref: <a href="https://www.tensorflow.org/alpha/guide/effective_tf2">https://www.tensorflow.org/alpha/guide/effective_tf2</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="s2">"error"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'2.0.0-dev20190531'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Experiment">Experiment<a class="anchor-link" href="#Experiment">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># corpus_generator = ['使用 Transformer 做機器翻譯很猛']</span>
<span class="c1"># tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_generator, target_vocab_size=2**13, max_subword_length=1)</span>

<span class="c1"># sample_string = '使用 Transformer 做機器翻譯很猛'</span>

<span class="c1"># tokenized_string = tokenizer_zh.encode(sample_string)</span>
<span class="c1"># print ('Tokenized string is {}'.format(tokenized_string))</span>

<span class="c1"># original_string = tokenizer_zh.decode(tokenized_string)</span>
<span class="c1"># print ('The original string: {}'.format(original_string))</span>

<span class="c1"># assert original_string == sample_string</span>

<span class="c1"># for ts in tokenized_string:</span>
<span class="c1">#   print ('{} ----&gt; {}'.format(ts, tokenizer_zh.decode([ts])))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span>
<span class="c1">#     (zh.numpy() for zh, _ in train_examples), target_vocab_size=2**17)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># sample_string = '使用 Transformer 做機器翻譯很猛'</span>

<span class="c1"># tokenized_string = tokenizer_zh.encode(sample_string)</span>
<span class="c1"># print ('Tokenized string is {}'.format(tokenized_string))</span>

<span class="c1"># original_string = tokenizer_zh.decode(tokenized_string)</span>
<span class="c1"># print ('The original string: {}'.format(original_string))</span>

<span class="c1"># assert original_string == sample_string</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># for ts in tokenized_string:</span>
<span class="c1">#   print ('{} ----&gt; {}'.format(ts, tokenizer_zh.decode([ts])))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gdrive">Gdrive<a class="anchor-link" href="#Gdrive">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_to_gdrive</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">dataset_in_gdrive</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># set to True for speed up without training</span>

<span class="n">output_dir</span> <span class="o">=</span> <span class="s2">"nmt"</span>

<span class="k">if</span> <span class="n">save_to_gdrive</span><span class="p">:</span>
  <span class="kn">from</span> <span class="nn">google.colab</span> <span class="k">import</span> <span class="n">drive</span>
  <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/content/gdrive'</span><span class="p">)</span>
  <span class="n">output_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"/content/gdrive/My Drive"</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">)</span>
    

<span class="n">en_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"en_vocab"</span><span class="p">)</span>
<span class="n">zh_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"zh_vocab"</span><span class="p">)</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"checkpoints"</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dataset_in_gdrive</span><span class="p">:</span>
  <span class="n">download_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"tensorflow-datasets/downloads"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">download_dir</span> <span class="o">=</span> <span class="s2">"tensorflow-datasets/downloads"</span>
    
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Save result to </span><span class="si">{output_dir}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount("/content/gdrive", force_remount=True).
Save result to /content/gdrive/My Drive/nmt
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span>
  <span class="n">en_vocab_file</span><span class="p">,</span>
  <span class="n">zh_vocab_file</span><span class="p">,</span>
  <span class="n">checkpoint_path</span><span class="p">,</span>
  <span class="n">log_dir</span><span class="p">,</span>
  <span class="n">download_dir</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/en_vocab',
 '/content/gdrive/My Drive/nmt/zh_vocab',
 '/content/gdrive/My Drive/nmt/checkpoints',
 '/content/gdrive/My Drive/nmt/logs',
 '/content/gdrive/My Drive/nmt/tensorflow-datasets/downloads']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup-input-pipeline">Setup input pipeline<a class="anchor-link" href="#Setup-input-pipeline">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Use <a href="https://www.tensorflow.org/datasets">TFDS</a> to load the <a href="https://github.com/neulab/word-embeddings-for-nmt">Portugese-English translation dataset</a> from the <a href="https://www.ted.com/participate/translate">TED Talks Open Translation Project</a>.</p>
<p>This dataset contains approximately 50000 training examples, 1100 validation examples, and 2000 test examples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>dataset reference: <a href="https://www.tensorflow.org/datasets/datasets#wmt19_translate">https://www.tensorflow.org/datasets/datasets#wmt19_translate</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # 顯示 subset 用</span>
<span class="c1"># tmp_builder = tfds.builder("wmt19_translate/zh-en")</span>
<span class="c1"># tmp_builder.subsets</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">translate</span><span class="o">.</span><span class="n">wmt</span><span class="o">.</span><span class="n">WmtConfig</span><span class="p">(</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">"0.0.2"</span><span class="p">,</span>
    <span class="n">language_pair</span><span class="o">=</span><span class="p">(</span><span class="s2">"zh"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">),</span>
    <span class="n">subsets</span><span class="o">=</span><span class="p">{</span>
        <span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span> <span class="p">[</span><span class="s2">"newscommentary_v14"</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">builder</span><span class="p">(</span><span class="s2">"wmt_translate"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># builder.download_and_prepare()</span>
<span class="n">builder</span><span class="o">.</span><span class="n">download_and_prepare</span><span class="p">(</span><span class="n">download_dir</span><span class="o">=</span><span class="n">download_dir</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>reference:</p>
<ul>
<li><a href="https://github.com/tensorflow/datasets/blob/master/docs/splits.md">https://github.com/tensorflow/datasets/blob/master/docs/splits.md</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_perc</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">drop_prec</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">train_perc</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">split</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">subsplit</span><span class="p">([</span><span class="n">drop_prec</span><span class="p">,</span> <span class="n">train_perc</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">split</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(NamedSplit('train')(tfds.percent[0:9]),
 NamedSplit('train')(tfds.percent[9:99]),
 NamedSplit('train')(tfds.percent[99:100]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">info</span>
<span class="n">examples</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;,
 &lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;,
 &lt;_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">examples</span>
<span class="c1"># train_examples, val_examples = examples</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">sample_examples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">en</span><span class="p">,</span> <span class="n">zh</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">:</span>
    <span class="n">num_en_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    <span class="n">num_zh_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">zh</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_zh_words</span> <span class="o">&lt;</span> <span class="mi">16</span> <span class="ow">or</span> <span class="n">num_zh_words</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>
      <span class="k">continue</span>
    
    <span class="k">if</span> <span class="n">num_en_words</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">num_en_words</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
      <span class="k">continue</span>
  
    <span class="n">sample_examples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">),</span>
        <span class="n">zh</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span>
    <span class="p">))</span>
    <span class="n">c</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="n">num_samples</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fix sentence for consistency</span>
<span class="n">sample_examples</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">'This year will be another difficult one for China.'</span><span class="p">,</span> <span class="s1">'今年对中国来说又将是艰难的一年。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'It is the only such case in the world.'</span><span class="p">,</span> <span class="s1">'它是世界上此类案例中唯一的一个。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'When that happens, a recession typically follows.'</span><span class="p">,</span> <span class="s1">'一旦出现这种情况衰退通常就会随之而来。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Right-wing populists such as Trump engage in identity politics.'</span><span class="p">,</span>
  <span class="s1">'特朗普等右翼民粹主义者采取身份政治。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Given the reaction in financial markets, they have succeeded.'</span><span class="p">,</span>
  <span class="s1">'鉴于金融市场的反应，他们已经成功了。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'The first lesson is that markets are not self-correcting.'</span><span class="p">,</span>
  <span class="s1">'第一个教训是，市场不会自动纠正错误。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'And they are gateways to Europe, Africa, and Asia.'</span><span class="p">,</span> <span class="s1">'它们也是通往欧洲、非洲和亚洲的门户。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Will the energy reforms produce another false dawn?'</span><span class="p">,</span> <span class="s1">'能源改革会再一次只是昙花一现吗？'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'In economic terms, they have had considerable success.'</span><span class="p">,</span>
  <span class="s1">'在经济方面，它们取得了相当大的成功。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'But even this comparison involves a selection bias.'</span><span class="p">,</span>
  <span class="s1">'但是，就连这个比较也带有一定的选择偏误。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'The shifting economic operating environment bolsters these opportunities.'</span><span class="p">,</span>
  <span class="s1">'不断变化的经济运行环境支撑了这些机遇。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'That memory is discouraging political innovation today.'</span><span class="p">,</span>
  <span class="s1">'如今，这段记忆阻碍人们进行政治创新。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'They are just buying real estate because they need it.'</span><span class="p">,</span>
  <span class="s1">'他们购置不动产时出于实际的需要。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Sixth, the authors admiringly cite the British experience.'</span><span class="p">,</span>
  <span class="s1">'第六，作者们以敬仰之情援引了英国的经验。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Are banks, markets, or regulators to blame?'</span><span class="p">,</span> <span class="s1">'我们应该归罪于银行、市场或监管者么？'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Moreover, Ukraine has strong investment and consumption growth.'</span><span class="p">,</span>
  <span class="s1">'此外，乌克兰投资和消费增长强劲。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Another factor that has been overlooked is the Chinese consumer.'</span><span class="p">,</span>
  <span class="s1">'中国消费者是一直被忽视的另外一个因素。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Europe&rsquo;s leaders must imbue their citizens with renewed hope.'</span><span class="p">,</span>
  <span class="s1">'欧洲领导人必须让民众看到新的希望。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Moreover, discrimination in India often begins in the family.'</span><span class="p">,</span>
  <span class="s1">'此外，印度的歧视往往从家庭就开始了。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Mueller obviously hopes to &ldquo;flip&rdquo; both Flynn and Manafort.'</span><span class="p">,</span>
  <span class="s1">'米勒显然希望策反弗林和马纳福特。'</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">'Others continue to advocate for national reconciliation.'</span><span class="p">,</span>
  <span class="s1">'也有人继续宣传民族和解才能解决问题。'</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create a custom subwords tokenizer from the training dataset.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_perc</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>90</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">en_vocab_file</span> <span class="o">=</span> <span class="n">en_vocab_file</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_perc</span><span class="p">)</span>
<span class="n">zh_vocab_file</span> <span class="o">=</span> <span class="n">zh_vocab_file</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_perc</span><span class="p">)</span>
<span class="p">[</span>
    <span class="n">en_vocab_file</span><span class="p">,</span> 
    <span class="n">zh_vocab_file</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/en_vocab_90',
 '/content/gdrive/My Drive/nmt/zh_vocab_90']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>因為我們實際上是讓模型產生一個中文字的分佈，開小一點的詞彙可以訓練快一點。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>
try:
    tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)
    print(f"Vocab file loaded from {en_vocab_file}")
except:
    tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(
        (en.numpy() for en, _ in train_examples), target_vocab_size=2**13)
    
    tokenizer_en.save_to_file(en_vocab_file)
    print(f"Build from corpus and saved to {en_vocab_file}")
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Vocab file loaded from /content/gdrive/My Drive/nmt/en_vocab_90
CPU times: user 40 ms, sys: 1.87 ms, total: 41.9 ms
Wall time: 285 ms
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>
try:
    tokenizer_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)
    print(f"Vocab file loaded from {zh_vocab_file}")
except:
    tokenizer_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (zh.numpy() for _, zh in train_examples), target_vocab_size=2**17, max_subword_length=1)
    
    tokenizer_zh.save_to_file(zh_vocab_file)
    print(f"Build from corpus and saved to {zh_vocab_file}")
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Vocab file loaded from /content/gdrive/My Drive/nmt/zh_vocab_90
CPU times: user 25.5 ms, sys: 77 &micro;s, total: 25.6 ms
Wall time: 311 ms
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(8159, 4849)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="s1">'Transformer is awesome.'</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'Tokenized string is </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'The original string: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Tokenized string is [2728, 466, 9, 3354, 145, 1355, 7949]
The original string: Transformer is awesome.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>2728 ----&gt; Trans
466 ----&gt; former 
9 ----&gt; is 
3354 ----&gt; aw
145 ----&gt; es
1355 ----&gt; ome
7949 ----&gt; .
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="n">sample_examples</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'Tokenized string is </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'The original string: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Tokenized string is [260, 25, 24, 16, 4, 28, 176, 553, 49, 6, 1108, 269, 1, 7, 25, 3]
The original string: 今年对中国来说又将是艰难的一年。
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>260 ----&gt; 今
25 ----&gt; 年
24 ----&gt; 对
16 ----&gt; 中
4 ----&gt; 国
28 ----&gt; 来
176 ----&gt; 说
553 ----&gt; 又
49 ----&gt; 将
6 ----&gt; 是
1108 ----&gt; 艰
269 ----&gt; 难
1 ----&gt; 的
7 ----&gt; 一
25 ----&gt; 年
3 ----&gt; 。
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Add a start and end token to the input and target.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
  <span class="n">lang1</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang1</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">lang2</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
  
  <span class="k">return</span> <span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">40</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">filter_max_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">,</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Operations inside <code>.map()</code> run in graph mode and receive a graph tensor that do not have a numpy attribute. The <code>tokenizer</code> expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a <code>tf.py_function</code>, which receives an eager tensor having a numpy attribute that contains the string value.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tf_encode</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">encode</span><span class="p">,</span> <span class="p">[</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span>
<span class="c1"># cache the dataset to memory to get a speedup while reading from it.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">padded_shapes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>


<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">padded_shapes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">en_batch</span><span class="p">,</span> <span class="n">zh_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">en_batch</span><span class="p">,</span> <span class="n">zh_batch</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>W0529 02:06:18.923115 140208100062976 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0529 02:06:18.926745 140208100062976 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0529 02:06:18.950205 140208100062976 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0529 02:06:18.954144 140208100062976 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
W0529 02:06:18.983365 140208108455680 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;tf.Tensor: id=1250590, shape=(128, 40), dtype=int64, numpy=
 array([[8159,  424,  778, ...,    0,    0,    0],
        [8159,  129,   47, ...,    0,    0,    0],
        [8159, 5097,  565, ...,    0,    0,    0],
        ...,
        [8159,  609,  614, ...,    0,    0,    0],
        [8159,   86,  586, ...,    0,    0,    0],
        [8159,   40,   41, ...,    0,    0,    0]])&gt;,
 &lt;tf.Tensor: id=1250591, shape=(128, 40), dtype=int64, numpy=
 array([[4849,   10,   66, ...,  427,    3, 4850],
        [4849,   69,  227, ...,    0,    0,    0],
        [4849,    9,  160, ...,    0,    0,    0],
        ...,
        [4849,   35,    6, ...,    0,    0,    0],
        [4849,   10,    7, ...,    0,    0,    0],
        [4849,   35,   10, ...,    0,    0,    0]])&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Positional-encoding">Positional encoding<a class="anchor-link" href="#Positional-encoding">&para;</a></h2><p>Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.</p>
<p>The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the <em>similarity of their meaning and their position in the sentence</em>, in the d-dimensional space.</p>
<p>See the notebook on <a href="https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb">positional encoding</a> to learn more about it. The formula for calculating the positional encoding is as follows:</p>
<p>$$\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$
$$\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angle_rates</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                          <span class="n">d_model</span><span class="p">)</span>
  
  <span class="c1"># apply sin to even indices in the array; 2i</span>
  <span class="n">sines</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
  <span class="c1"># apply cos to odd indices in the array; 2i+1</span>
  <span class="n">cosines</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">sines</span><span class="p">,</span> <span class="n">cosines</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  
  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-notebook"</span><span class="p">)</span>
<span class="c1"># plt.style.use("ggplot")</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'RdBu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Depth'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Position'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 50, 512)
</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAecAAAFfCAYAAAB0lARMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Masking">Masking<a class="anchor-link" href="#Masking">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value <code>0</code> is present: it outputs a <code>1</code> at those locations, and a <code>0</code> otherwise.</p>
<p>實際上在 Transformer 裡頭有兩種 masks：</p>
<ul>
<li>padding mask</li>
<li>look ahead mask</li>
</ul>
<p>前者是讓 Transformer 用來識別 sequence 實際的內容到哪裡；後者則是讓 Decoder 在生成輸出序列的時候，不會不小心看到未來的句子。</p>
<p>mask 序列裡頭那些值為 1 的位置即為未來會被遮蓋住的位置。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
  <span class="n">seq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  
  <span class="c1"># add extra dimensions so that we can add the padding</span>
  <span class="c1"># to the attention logits.</span>
  <span class="k">return</span> <span class="n">seq</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, 1, seq_len)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而這邊之所以要新增兩個 dimensions 是因為在 Multi-Head Attention 裡頭一組 attention logits 的維度為 <code>(batch_size, num_heads, seq_len, seq_len)</code>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
                 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
                 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor: id=1250607, shape=(3, 1, 1, 5), dtype=float32, numpy=
array([[[[0., 0., 1., 1., 1.]]],


       [[[0., 0., 0., 1., 1.]]],


       [[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensor 裡頭為 <code>1</code> 的位置的值在之後的計算就會忽略。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.</p>
<p>This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">mask</span>  <span class="c1"># (seq_len, seq_len)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">temp</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor: id=1250623, shape=(3, 3), dtype=float32, numpy=
array([[0., 1., 1.],
       [0., 0., 1.],
       [0., 0., 0.]], dtype=float32)&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>第一維度代表處理次數，而可以看得出來，第二維度從上到下就代表 Decoder 在產生翻譯時每次能看到的位置。</p>
<p>第一個時間點只能看到第一個位置，以此類推。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="TODO">TODO<a class="anchor-link" href="#TODO">&para;</a></h3><p>增加 ene-to-end 例子</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dec_target_padding_mask = create_padding_mask(tar)</span>
<span class="c1"># print(dec_target_padding_mask)</span>
<span class="c1"># look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])</span>
<span class="c1"># print(look_ahead_mask)</span>
<span class="c1"># tf.maximum(dec_target_padding_mask, look_ahead_mask)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scaled-dot-product-attention_1">Scaled dot product attention<a class="anchor-link" href="#Scaled-dot-product-attention">&para;</a></h2><p>在實作 Multi-head 之前，先讓我們實作基本的 Attention 機制。</p>
<p>注意力機制基本上可以想成資料庫比對。給定一個查詢 Q，我們去看該 Q 跟所有 K 的匹配程度，接著以此匹配程度對實際的 V 做加權平均，得到最後的 Repr.</p>
<p>$$Attention(Q, K, V) = softmax({QK^T \over \sqrt{d_{k}}})V $$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="scaled_dot_product_attention" src="https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png" width="500"/></p>
<p>The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:</p>
<p>$$\Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k}}) V} $$</p>
<p>The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.</p>
<p>For example, consider that <code>Q</code> and <code>K</code> have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of <code>dk</code>. Hence, <em>square root of <code>dk</code></em> is used for scaling (and not any other number) because the matmul of <code>Q</code> and <code>K</code> should have a mean of 0 and variance of 1, so that we get a gentler softmax.</p>
<p>The mask is multiplied with <em>-1e9 (close to negative infinity).</em> This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install pysnooper
<span class="kn">import</span> <span class="nn">pysnooper</span>
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>

<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
  <span class="sd">"""Calculate the attention weights.</span>
<span class="sd">  q, k, v must have matching leading dimensions.</span>
<span class="sd">  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.</span>
<span class="sd">  The mask has different shapes depending on its type(padding or look ahead) </span>
<span class="sd">  but it must be broadcastable for addition.</span>
<span class="sd">  </span>
<span class="sd">  Args:</span>
<span class="sd">    q: query shape == (..., seq_len_q, depth)</span>
<span class="sd">    k: key shape == (..., seq_len_k, depth)</span>
<span class="sd">    v: value shape == (..., seq_len_v, depth_v)</span>
<span class="sd">    mask: Float tensor with shape broadcastable </span>
<span class="sd">          to (..., seq_len_q, seq_len_k). Defaults to None.</span>
<span class="sd">    </span>
<span class="sd">  Returns:</span>
<span class="sd">    output, attention_weights</span>
<span class="sd">  """</span>

  <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>
  
  <span class="c1"># scale matmul_qk</span>
  <span class="n">dk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">scaled_attention_logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

  <span class="c1"># add the mask to the scaled tensor.</span>
  <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">scaled_attention_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

  <span class="c1"># softmax is normalized on the last axis (seq_len_k) so that the scores</span>
  <span class="c1"># add up to 1.</span>
  <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_attention_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (..., seq_len_v, depth_v)</span>

  <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很大的負值丟入 Softmax 函式以後會接近 0 ，則如果我們想把後三個位置遮住丟入 softmax 的話，則 mask 應該要是 [..., 0, 1, 1, 1] （要被遮住的位置的 mask 值為 1），再乘上 -1e9 以後加入 scaled_attention_logits 即可讓後三個位置經過 softmax 出來的值為 0</p>
<p>As the softmax normalization is done on K, its values decide the amount of importance given to Q.</p>
<p>The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_out</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Attention weights are:'</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_attn</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Output is:'</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">temp_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 3)</span>

<span class="n">temp_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span>   <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span>  <span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 2)</span>

<span class="c1"># This `query` aligns with the second `key`,</span>
<span class="c1"># so the second `value` is returned.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>

<span class="c1"># print_out(temp_q, temp_k, temp_v)</span>
<span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns with a repeated key (third and fourth), </span>
<span class="c1"># so all associated values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns equally with the first and second key, </span>
<span class="c1"># so their values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pass all the queries together.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (3, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor(
[[0.  0.  0.5 0.5]
 [0.  1.  0.  0. ]
 [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)
Output is:
tf.Tensor(
[[550.    5.5]
 [ 10.    0. ]
 [  5.5   0. ]], shape=(3, 2), dtype=float32)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-head-attention">Multi-head attention<a class="anchor-link" href="#Multi-head-attention">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="multi-head attention" src="https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png" width="500"/></p>
<p>Multi-head attention consists of four parts:</p>
<ul>
<li>Linear layers and split into heads.</li>
<li>Scaled dot-product attention.</li>
<li>Concatenation of heads.</li>
<li>Final linear layer.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.</p>
<p>The <code>scaled_dot_product_attention</code> defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using <code>tf.transpose</code>, and <code>tf.reshape</code>) and put through a final <code>Dense</code> layer.</p>
<p>Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    
    <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
  <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">"""Split the last dimension into (num_heads, depth).</span>
<span class="sd">    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="sd">    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_k, depth)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_v, depth)</span>
    
    <span class="c1"># scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)</span>
    <span class="c1"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    
    <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len_v, num_heads, depth)</span>

    <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> 
                                  <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len_v, d_model)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len_v, d_model)</span>
        
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create a <code>MultiHeadAttention</code> layer to try out. At each location in the sequence, <code>y</code>, the <code>MultiHeadAttention</code> runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>  <span class="c1"># (batch_size, encoder_sequence, d_model)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">temp_mha</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Point-wise-feed-forward-network">Point wise feed forward network<a class="anchor-link" href="#Point-wise-feed-forward-network">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>  <span class="c1"># (batch_size, seq_len, dff)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
  <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_ffn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Encoder-and-decoder">Encoder and decoder<a class="anchor-link" href="#Encoder-and-decoder">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="transformer" src="https://www.tensorflow.org/images/tutorials/transformer/transformer.png" width="600"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The transformer model follows the same general pattern as a standard <a href="nmt_with_attention.ipynb">sequence to sequence with attention model</a>.</p>
<ul>
<li>The input sentence is passed through <code>N</code> encoder layers that generates an output for each word/token in the sequence.</li>
<li>The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder-layer">Encoder layer<a class="anchor-link" href="#Encoder-layer">&para;</a></h3><p>Each encoder layer consists of sublayers:</p>
<ol>
<li>Multi-head attention (with padding mask) </li>
<li>Point wise feed forward networks. </li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.</p>
<p>The output of each sublayer is <code>LayerNorm(x + Sublayer(x))</code>. The normalization is done on the <code>d_model</code> (last) axis. There are N encoder layers in the transformer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    
    <span class="k">return</span> <span class="n">out2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder_layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span> <span class="o">=</span> <span class="n">sample_encoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 43, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder-layer">Decoder layer<a class="anchor-link" href="#Decoder-layer">&para;</a></h3><p>Each decoder layer consists of sublayers:</p>
<ol>
<li>Masked multi-head attention (with look ahead mask and padding mask)</li>
<li>Multi-head attention (with padding mask). V (value) and K (key) receive the <em>encoder output</em> as inputs. Q (query) receives the <em>output from the masked multi-head attention sublayer.</em></li>
<li>Point wise feed forward networks</li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is <code>LayerNorm(x + Sublayer(x))</code>. The normalization is done on the <code>d_model</code> (last) axis.</p>
<p>There are N decoder layers in the transformer.</p>
<p>As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>
 
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
    <span class="c1"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>

    <span class="n">attn1</span><span class="p">,</span> <span class="n">attn_weights_block1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="n">attn2</span><span class="p">,</span> <span class="n">attn_weights_block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span><span class="p">(</span>
        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">attn2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    
    <span class="k">return</span> <span class="n">out3</span><span class="p">,</span> <span class="n">attn_weights_block1</span><span class="p">,</span> <span class="n">attn_weights_block2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder_layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_decoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">sample_encoder_layer_output</span><span class="p">,</span> 
    <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder">Encoder<a class="anchor-link" href="#Encoder">&para;</a></h3><p>The <code>Encoder</code> consists of:</p>
<ol>
<li>Input Embedding</li>
<li>Positional Encoding</li>
<li>N encoder layers</li>
</ol>
<p>The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
               <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    
    
    <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
  
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># adding embedding and position encoding.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">)</span>

<span class="n">sample_encoder_output</span> <span class="o">=</span> <span class="n">sample_encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">)),</span> 
                                       <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">sample_encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>(64, 62, 512)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder">Decoder<a class="anchor-link" href="#Decoder">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Decoder</code> consists of:</p>
<ol>
<li>Output Embedding</li>
<li>Positional Encoding</li>
<li>N decoder layers</li>
</ol>
<p>The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> 
               <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
                                             <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>
      
      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">'decoder_layer</span><span class="si">{}</span><span class="s1">_block1'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block1</span>
      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">'decoder_layer</span><span class="si">{}</span><span class="s1">_block2'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block2</span>
    
    <span class="c1"># x.shape == (batch_size, target_seq_len, d_model)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">sample_decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">)),</span> 
                              <span class="n">enc_output</span><span class="o">=</span><span class="n">sample_encoder_output</span><span class="p">,</span> 
                              <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                              <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="p">[</span><span class="s1">'decoder_layer2_block2'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-the-Transformer_1">Create the Transformer<a class="anchor-link" href="#Create-the-Transformer">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
               <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                           <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                           <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">):</span>

    <span class="n">enc_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, inp_seq_len, d_model)</span>
    
    <span class="c1"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
    <span class="n">dec_output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
        <span class="n">tar</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">)</span>
    
    <span class="n">final_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
    
    <span class="k">return</span> <span class="n">final_output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>

<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">))</span>
<span class="n">temp_target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">))</span>

<span class="n">fn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_transformer</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">temp_target</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                               <span class="n">enc_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                               <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">dec_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">fn_out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 26, 8000])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-hyperparameters">Set hyperparameters<a class="anchor-link" href="#Set-hyperparameters">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To keep this example small and relatively fast, the values for <em>num_layers, d_model, and dff</em> have been reduced.</p>
<p>The values used in the base model of transformer were; <em>num_layers=6</em>, <em>d_model = 512</em>, <em>dff = 2048</em>. See the <a href="https://arxiv.org/abs/1706.03762">paper</a> for all the other versions of the transformer.</p>
<p>Note: By changing the values below, you can get the model that achieved state of the art on many tasks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">dff</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(8161, 4851)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Setup-experiment-path">Setup experiment path<a class="anchor-link" href="#Setup-experiment-path">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">run_id</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"</span><span class="si">{num_layers}</span><span class="s2">layers_</span><span class="si">{d_model}</span><span class="s2">d_</span><span class="si">{num_heads}</span><span class="s2">heads_</span><span class="si">{dff}</span><span class="s2">dff_</span><span class="si">{train_perc}</span><span class="s2">train_perc"</span>
<span class="n">run_id</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'4layers_256d_8heads_1024dff_90train_perc'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span>
    <span class="n">checkpoint_path</span><span class="p">,</span>
    <span class="n">log_dir</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>['/content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc',
 '/content/gdrive/My Drive/nmt/logs/4layers_256d_8heads_1024dff_90train_perc']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Optimizer_1">Optimizer<a class="anchor-link" href="#Optimizer">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Use the Adam optimizer with a custom learning rate scheduler according to the formula in the <a href="https://arxiv.org/abs/1706.03762">paper</a>.</p>
<p>$$\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}$$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> 
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-whitegrid"</span><span class="p">)</span>
<span class="c1"># plt.style.use("ggplot")</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_learning_rate_schedule</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temp_learning_rate_schedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Learning Rate"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Train Step"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 0, 'Train Step')</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAFYCAYAAAAlTUT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># temp_learning_rate_schedule.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-and-metrics">Loss and metrics<a class="anchor-link" href="#Loss-and-metrics">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.</p>
<p>這邊 <code>reduction</code> 設成 <code>none</code> 是因為我們會套用 mask 以後自己 sum up</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在這邊每個中文字就是一個分類，因此我們可以使用 Cross entropy</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">'none'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>
  
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'train_loss'</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'train_accuracy'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)</span>
<span class="c1"># pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)</span>
<span class="c1"># loss_object(real, pred)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tf.keras.losses.SparseCategoricalCrossentropy(</span>
<span class="c1">#     from_logits=True)(real, pred)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-and-checkpointing">Training and checkpointing<a class="anchor-link" href="#Training-and-checkpointing">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                          <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="c1"># Encoder padding mask</span>
  <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
  
  <span class="c1"># Used in the 2nd attention block in the decoder.</span>
  <span class="c1"># This padding mask is used to mask the encoder outputs.</span>
  <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
  
  <span class="c1"># Used in the 1st attention block in the decoder.</span>
  <span class="c1"># It is used to pad and mask future tokens in the input received by </span>
  <span class="c1"># the decoder.</span>
  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tar</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">dec_target_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">tar</span><span class="p">)</span>
  <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dec_target_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">inp</span><span class="p">,</span> <span class="n">tar</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(&lt;tf.Tensor: id=1255678, shape=(3, 4), dtype=float32, numpy=
 array([[1., 2., 3., 0.],
        [2., 2., 5., 6.],
        [5., 1., 0., 0.]], dtype=float32)&gt;,
 &lt;tf.Tensor: id=1255679, shape=(3, 3), dtype=float32, numpy=
 array([[2., 5., 1.],
        [3., 4., 0.],
        [1., 0., 0.]], dtype=float32)&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create_padding_mask(inp)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dec_target_padding_mask = create_padding_mask(tar)</span>
<span class="c1"># print(dec_target_padding_mask)</span>
<span class="c1"># look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])</span>
<span class="c1"># print(look_ahead_mask)</span>
<span class="c1"># tf.maximum(dec_target_padding_mask, look_ahead_mask)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every <code>n</code> epochs.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Latest checkpoint restored!!'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Start from scratch!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Start from scratch!
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">last_epoch</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"-"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">last_epoch</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. <code>tar_real</code> is that same input shifted by 1: At each location in <code>tar_input</code>, <code>tar_real</code> contains the  next token that should be predicted.</p>
<p>For example, <code>sentence</code> = "SOS A lion in the jungle is sleeping EOS"</p>
<p><code>tar_inp</code> =  "SOS A lion in the jungle is sleeping"</p>
<p><code>tar_real</code> = "A lion in the jungle is sleeping EOS"</p>
<p>The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.</p>
<p>During training this example uses teacher-forcing (like in the <a href="./text_generation.ipynb">text generation tutorial</a>). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.</p>
<p>As the transformer predicts each word, <em>self-attention</em> allows it to look at the previous words in the input sequence to better predict the next word.</p>
<p>To prevent the model from peaking at the expected output the model uses a look-ahead mask.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們會使用 teacher forcing 來訓練模型。</p>
<ul>
<li>teach forcing 比較穩定的 video link: </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="n">tar_inp</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">tar_real</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
  
  <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">)</span>
  
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">,</span> 
                                 <span class="kc">True</span><span class="p">,</span> 
                                 <span class="n">enc_padding_mask</span><span class="p">,</span> 
                                 <span class="n">combined_mask</span><span class="p">,</span> 
                                 <span class="n">dec_padding_mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

  <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>    
  <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
  
  <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="n">train_accuracy</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>英文是我們的來源語言，而我們目標則是將其轉成（簡體）中文。因此 <code>inp</code> 為 <code>en</code>，<code>tar</code> 為 <code>zh</code>。</p>
<ul>
<li>因為我們在訓練的時候輸入輸出句子都有加上 <bos>，在產生一個新句子的時候也要加上去</bos></li>
<li>因為不像是天龍八部生成器我們希望機器每次都能產生新的結果，NMT 是做 Conditional Generation，因此我們不是 sample 機器預測的中文字分佈而是取 <code>argmax</code></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">):</span>
  <span class="n">start_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">end_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
  
  <span class="c1"># inp sentence is english, hence adding the start and end token</span>
  <span class="n">inp_sentence</span> <span class="o">=</span> <span class="n">start_token</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">end_token</span>
  <span class="n">encoder_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  
  <span class="c1"># as the target is chinese, the first word to the transformer should be the</span>
  <span class="c1"># chinese start token.</span>
  <span class="n">decoder_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span>
        <span class="n">encoder_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
  
    <span class="c1"># predictions.shape == (batch_size, seq_len, vocab_size)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">encoder_input</span><span class="p">,</span> 
                                                 <span class="n">output</span><span class="p">,</span>
                                                 <span class="kc">False</span><span class="p">,</span>
                                                 <span class="n">enc_padding_mask</span><span class="p">,</span>
                                                 <span class="n">combined_mask</span><span class="p">,</span>
                                                 <span class="n">dec_padding_mask</span><span class="p">)</span>
    
    <span class="c1"># select the last word from the seq_len dimension</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, vocab_size)</span>

    <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="c1"># return the result if the predicted_id is equal to the end token</span>
    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predicted_id</span><span class="p">,</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
    
    <span class="c1"># concatentate the predicted_id to the output which is given to the decoder</span>
    <span class="c1"># as its input.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">predicted_id</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chinese-font-setup-for-matplotlib">Chinese font setup for matplotlib<a class="anchor-link" href="#Chinese-font-setup-for-matplotlib">&para;</a></h3><p>reference</p>
<ul>
<li><a href="https://www.jianshu.com/p/fc9a502ad243">https://www.jianshu.com/p/fc9a502ad243</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget -O /usr/share/fonts/truetype/liberation/simhei.ttf <span class="err">\</span>
    <span class="s2">"http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf"</span>
<span class="c1"># !test NotoSans-unhinted.zip || wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSans-unhinted.zip</span>
<span class="c1"># !test -d fonts || mkdir fonts</span>
<span class="c1"># !unzip -d fonts NotoSans-unhinted.zip</span>
<span class="c1"># !cp fonts/NotoSans* /usr/share/fonts/truetype/liberation/</span>
<span class="c1"># clear_output()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>--2019-05-29 02:06:40--  http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf
Resolving d.xiazaiziti.com (d.xiazaiziti.com)... 67.198.189.58
Connecting to d.xiazaiziti.com (d.xiazaiziti.com)|67.198.189.58|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10050868 (9.6M) [application/octet-stream]
Saving to: &lsquo;/usr/share/fonts/truetype/liberation/simhei.ttf&rsquo;

/usr/share/fonts/tr 100%[===================&gt;]   9.58M  2.04MB/s    in 4.9s    

2019-05-29 02:06:45 (1.96 MB/s) - &lsquo;/usr/share/fonts/truetype/liberation/simhei.ttf&rsquo; saved [10050868/10050868]

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="c1"># zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/NotoSans-Regular.ttf')</span>
<span class="n">zhfont</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">font_manager</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s1">'/usr/share/fonts/truetype/liberation/simhei.ttf'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"seaborn-whitegrid"</span><span class="p">)</span>
<span class="c1"># plt.style.use("fivethirtyeight")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="TODO_1">TODO<a class="anchor-link" href="#TODO">&para;</a></h3><ul>
<li>改成 2 * 4</li>
<li>加入 epoch 資訊</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPUAAAFOCAYAAABey1tnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @pysnooper.snoop()</span>
<span class="k">def</span> <span class="nf">plot_attention_weights</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
  
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  
  <span class="c1"># for pretty rendering intermidate result</span>
  <span class="k">if</span> <span class="n">max_len_tar</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:</span><span class="n">max_len_tar</span><span class="p">]</span>
  
  <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  
  <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">head</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># plot the attention weights</span>
    <span class="k">if</span> <span class="n">max_len_tar</span><span class="p">:</span>
      <span class="n">attn_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">][:</span><span class="n">max_len_tar</span><span class="p">,</span> <span class="p">:])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attn_map</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>  <span class="c1"># (inp_seq_len, tar_seq_len)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">attn_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attn_map</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
    <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"fontproperties"</span><span class="p">:</span> <span class="n">zhfont</span><span class="p">}</span>
    
    <span class="k">if</span> <span class="n">max_len_tar</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_len_tar</span><span class="p">:</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_len_tar</span><span class="p">))</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_len_tar</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)))</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">],</span> 
                       <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>    
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span>
        <span class="p">[</span><span class="s1">'&lt;start&gt;'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">'&lt;end&gt;'</span><span class="p">],</span> 
        <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Head </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">head</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
  
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

  
  <span class="k">if</span> <span class="ow">not</span> <span class="n">save_file_name</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_file_name</span><span class="p">)</span>
    
  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">''</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">result</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  
  <span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_zh</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">])</span>  
  <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Input: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Predicted translation: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
  
  <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
    <span class="n">plot_attention_weights</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">plot</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">predicted_sentence</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_layer_block</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"decoder_layer</span><span class="si">{num_layers}</span><span class="s2">_block2"</span>
<span class="n">plot_layer_block</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'decoder_layer4_block2'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">[</span><span class="mi">14</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
          <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Input: Are banks, markets, or regulators to blame?
Predicted translation: 缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆缆
</pre>
</div>
</div>
<div class="output_area">
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABMAAAAHMCAYAAAAzsNRKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inp_sentence = "America"</span>
<span class="c1"># result, attention_weights = evaluate(inp_sentence)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def translate(sentence):</span>
<span class="c1">#   result, attention_weights = evaluate(sentence)</span>
  
<span class="c1">#   predicted_sentence = tokenizer_zh.decode([i for i in result </span>
<span class="c1">#                                             if i &lt; tokenizer_zh.vocab_size])  </span>

<span class="c1">#   print('Input: {}'.format(sentence))</span>
<span class="c1">#   print('Predicted translation: {}'.format(predicted_sentence))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># predicted_sentence = tokenizer_zh.decode([i for i in result </span>
<span class="c1">#                                             if i &lt; tokenizer_zh.vocab_size])  </span>

<span class="c1"># print('Input: {}'.format(inp_sentence))</span>
<span class="c1"># print('Predicted translation: {}'.format(predicted_sentence))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tensorboard_1">Tensorboard<a class="anchor-link" href="#Tensorboard">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> tensorboard
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span><span class="nb">kill</span> <span class="m">6658</span>
<span class="o">%</span><span class="k">tensorboard</span> --logdir /content/gdrive/My\ Drive/nmt/logs
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>/bin/bash: line 0: kill: (6658) - No such process
</pre>
</div>
</div>
<div class="output_area">
<div class="output_html rendered_html output_subarea ">
<div id="root"></div>
<script>
      (function() {
        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};
        window.TENSORBOARD_ENV["IN_COLAB"] = true;
        document.querySelector("base").href = "https://localhost:6006";
        function fixUpTensorboard(root) {
          const tftb = root.querySelector("tf-tensorboard");
          // Disable the fragment manipulation behavior in Colab. Not
          // only is the behavior not useful (as the iframe's location
          // is not visible to the user), it causes TensorBoard's usage
          // of `window.replace` to navigate away from the page and to
          // the `localhost:<port>` URL specified by the base URI, which
          // in turn causes the frame to (likely) crash.
          tftb.removeAttribute("use-hash");
        }
        function executeAllScripts(root) {
          // When `script` elements are inserted into the DOM by
          // assigning to an element's `innerHTML`, the scripts are not
          // executed. Thus, we manually re-insert these scripts so that
          // TensorBoard can initialize itself.
          for (const script of root.querySelectorAll("script")) {
            const newScript = document.createElement("script");
            newScript.type = script.type;
            newScript.textContent = script.textContent;
            root.appendChild(newScript);
            script.remove();
          }
        }
        function setHeight(root, height) {
          // We set the height dynamically after the TensorBoard UI has
          // been initialized. This avoids an intermediate state in
          // which the container plus the UI become taller than the
          // final width and cause the Colab output frame to be
          // permanently resized, eventually leading to an empty
          // vertical gap below the TensorBoard UI. It's not clear
          // exactly what causes this problematic intermediate state,
          // but setting the height late seems to fix it.
          root.style.height = `${height}px`;
        }
        const root = document.getElementById("root");
        fetch(".")
          .then((x) => x.text())
          .then((html) => void (root.innerHTML = html))
          .then(() => fixUpTensorboard(root))
          .then(() => executeAllScripts(root))
          .then(() => setHeight(root, 800));
      })();
    </script>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">last_epoch</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">zh</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"restore for checkpoints, skip."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>
  <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
  
<span class="n">track_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"China, India, and others have enjoyed continuing economic growth."</span><span class="p">,</span>
    <span class="s2">"When that happens, a recession typically follows."</span><span class="p">,</span>
    <span class="s2">"In fact, central banks typically only have very fuzzy measures."</span><span class="p">,</span>
    <span class="s2">"Given the reaction in financial markets, they have succeeded."</span><span class="p">,</span>
    <span class="s2">"Moreover, discrimination in India often begins in the family."</span><span class="p">,</span>
    <span class="s2">"Europe&rsquo;s leaders must imbue their citizens with renewed hope."</span><span class="p">,</span>
    <span class="s2">"Are banks, markets, or regulators to blame?"</span>
<span class="p">]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">last_epoch</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">s_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">track_sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="s2">"s_</span><span class="si">{:02d}</span><span class="s2">_attn_epoch_</span><span class="si">{:02d}</span><span class="s2">.png"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"restore for checkpoints, skip."</span><span class="p">)</span>
  
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>50</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">last_epoch</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  
  <span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  
  <span class="c1"># inp -&gt; english, tar -&gt; chinese</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>
    
<span class="c1">#     if batch % 100 == 0:</span>
<span class="c1">#       print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(</span>
<span class="c1">#           epoch + 1, batch, train_loss.result(), train_accuracy.result()))</span>
      
  <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">'Saving checkpoint for epoch </span><span class="si">{}</span><span class="s1"> at </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">ckpt_save_path</span><span class="p">))</span>
    

  <span class="c1"># tensorboard</span>
  <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s2">"train_loss"</span><span class="p">,</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s2">"train_acc"</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Epoch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
                                                <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> 
                                                <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">'Time taken for 1 epoch: </span><span class="si">{}</span><span class="s1"> secs</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    
  <span class="c1"># intermidate translation</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_examples</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">en</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">en</span><span class="p">,</span> <span class="n">translation</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  
  <span class="c1"># attention vis by time</span>
  <span class="k">for</span> <span class="n">s_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">track_sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="s2">"s_</span><span class="si">{:02d}</span><span class="s2">_attn_epoch_</span><span class="si">{:02d}</span><span class="s2">.png"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s_idx</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">translation</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plot_layer_block</span><span class="p">,</span> <span class="n">max_len_tar</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">save_file_name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving checkpoint for epoch 1 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-1
Epoch 1 Loss 3.7955 Accuracy 0.1132
Time taken for 1 epoch: 572.7193248271942 secs

Saving checkpoint for epoch 2 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-2
Epoch 2 Loss 2.4882 Accuracy 0.2323
Time taken for 1 epoch: 349.3927080631256 secs

Saving checkpoint for epoch 3 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-3
Epoch 3 Loss 2.0187 Accuracy 0.2920
Time taken for 1 epoch: 353.2098364830017 secs

Saving checkpoint for epoch 4 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-4
Epoch 4 Loss 1.6881 Accuracy 0.3433
Time taken for 1 epoch: 348.1146204471588 secs

Saving checkpoint for epoch 5 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-5
Epoch 5 Loss 1.4675 Accuracy 0.3772
Time taken for 1 epoch: 347.92256355285645 secs

Saving checkpoint for epoch 6 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-6
Epoch 6 Loss 1.3243 Accuracy 0.3986
Time taken for 1 epoch: 348.398113489151 secs

Saving checkpoint for epoch 7 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-7
Epoch 7 Loss 1.2286 Accuracy 0.4127
Time taken for 1 epoch: 349.57627296447754 secs

Saving checkpoint for epoch 8 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-8
Epoch 8 Loss 1.1572 Accuracy 0.4236
Time taken for 1 epoch: 348.2038960456848 secs

Saving checkpoint for epoch 9 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-9
Epoch 9 Loss 1.1019 Accuracy 0.4319
Time taken for 1 epoch: 349.0648784637451 secs

Saving checkpoint for epoch 10 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-10
Epoch 10 Loss 1.0559 Accuracy 0.4389
Time taken for 1 epoch: 347.51360058784485 secs

Saving checkpoint for epoch 11 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-11
Epoch 11 Loss 1.0161 Accuracy 0.4450
Time taken for 1 epoch: 345.75678730010986 secs

Saving checkpoint for epoch 12 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-12
Epoch 12 Loss 0.9823 Accuracy 0.4504
Time taken for 1 epoch: 346.95540380477905 secs

Saving checkpoint for epoch 13 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-13
Epoch 13 Loss 0.9535 Accuracy 0.4548
Time taken for 1 epoch: 347.3623688220978 secs

Saving checkpoint for epoch 14 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-14
Epoch 14 Loss 0.9270 Accuracy 0.4588
Time taken for 1 epoch: 352.5413851737976 secs

Saving checkpoint for epoch 15 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-15
Epoch 15 Loss 0.9039 Accuracy 0.4627
Time taken for 1 epoch: 347.95324325561523 secs

Saving checkpoint for epoch 16 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-16
Epoch 16 Loss 0.8824 Accuracy 0.4659
Time taken for 1 epoch: 344.68012022972107 secs

Saving checkpoint for epoch 17 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-17
Epoch 17 Loss 0.8627 Accuracy 0.4691
Time taken for 1 epoch: 343.83316564559937 secs

Saving checkpoint for epoch 18 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-18
Epoch 18 Loss 0.8445 Accuracy 0.4720
Time taken for 1 epoch: 345.00450825691223 secs

Saving checkpoint for epoch 19 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-19
Epoch 19 Loss 0.8277 Accuracy 0.4748
Time taken for 1 epoch: 345.5647449493408 secs

Saving checkpoint for epoch 20 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-20
Epoch 20 Loss 0.8123 Accuracy 0.4774
Time taken for 1 epoch: 345.4926474094391 secs

Saving checkpoint for epoch 21 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-21
Epoch 21 Loss 0.7978 Accuracy 0.4797
Time taken for 1 epoch: 345.4738953113556 secs

Saving checkpoint for epoch 22 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-22
Epoch 22 Loss 0.7848 Accuracy 0.4817
Time taken for 1 epoch: 343.13048672676086 secs

Saving checkpoint for epoch 23 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-23
Epoch 23 Loss 0.7719 Accuracy 0.4840
Time taken for 1 epoch: 344.58143973350525 secs

Saving checkpoint for epoch 24 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-24
Epoch 24 Loss 0.7595 Accuracy 0.4859
Time taken for 1 epoch: 347.2893078327179 secs

Saving checkpoint for epoch 25 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-25
Epoch 25 Loss 0.7486 Accuracy 0.4879
Time taken for 1 epoch: 346.00260615348816 secs

Saving checkpoint for epoch 26 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-26
Epoch 26 Loss 0.7377 Accuracy 0.4897
Time taken for 1 epoch: 346.0875782966614 secs

Saving checkpoint for epoch 27 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-27
Epoch 27 Loss 0.7276 Accuracy 0.4915
Time taken for 1 epoch: 352.4587879180908 secs

Saving checkpoint for epoch 28 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-28
Epoch 28 Loss 0.7179 Accuracy 0.4929
Time taken for 1 epoch: 344.6506087779999 secs

Saving checkpoint for epoch 29 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-29
Epoch 29 Loss 0.7083 Accuracy 0.4947
Time taken for 1 epoch: 343.5449929237366 secs

Saving checkpoint for epoch 30 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-30
Epoch 30 Loss 0.6996 Accuracy 0.4961
Time taken for 1 epoch: 344.1521532535553 secs

Saving checkpoint for epoch 31 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-31
Epoch 31 Loss 0.6913 Accuracy 0.4977
Time taken for 1 epoch: 346.6304178237915 secs

Saving checkpoint for epoch 32 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-32
Epoch 32 Loss 0.6833 Accuracy 0.4988
Time taken for 1 epoch: 346.5781180858612 secs

Saving checkpoint for epoch 33 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-33
Epoch 33 Loss 0.6750 Accuracy 0.5004
Time taken for 1 epoch: 347.2600724697113 secs

Saving checkpoint for epoch 34 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-34
Epoch 34 Loss 0.6680 Accuracy 0.5014
Time taken for 1 epoch: 349.7422707080841 secs

Saving checkpoint for epoch 35 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-35
Epoch 35 Loss 0.6605 Accuracy 0.5028
Time taken for 1 epoch: 348.5436153411865 secs

Saving checkpoint for epoch 36 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-36
Epoch 36 Loss 0.6537 Accuracy 0.5041
Time taken for 1 epoch: 348.3004620075226 secs

Saving checkpoint for epoch 37 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-37
Epoch 37 Loss 0.6472 Accuracy 0.5050
Time taken for 1 epoch: 350.0017457008362 secs

Saving checkpoint for epoch 38 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-38
Epoch 38 Loss 0.6409 Accuracy 0.5062
Time taken for 1 epoch: 346.2030210494995 secs

Saving checkpoint for epoch 39 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-39
Epoch 39 Loss 0.6351 Accuracy 0.5073
Time taken for 1 epoch: 348.262184381485 secs

Saving checkpoint for epoch 40 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-40
Epoch 40 Loss 0.6287 Accuracy 0.5086
Time taken for 1 epoch: 348.1919550895691 secs

Saving checkpoint for epoch 41 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-41
Epoch 41 Loss 0.6228 Accuracy 0.5095
Time taken for 1 epoch: 347.0577027797699 secs

Saving checkpoint for epoch 42 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-42
Epoch 42 Loss 0.6175 Accuracy 0.5105
Time taken for 1 epoch: 358.33099126815796 secs

Saving checkpoint for epoch 43 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-43
Epoch 43 Loss 0.6113 Accuracy 0.5115
Time taken for 1 epoch: 350.2482817173004 secs

Saving checkpoint for epoch 44 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-44
Epoch 44 Loss 0.6062 Accuracy 0.5123
Time taken for 1 epoch: 348.85682702064514 secs

Saving checkpoint for epoch 45 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-45
Epoch 45 Loss 0.6013 Accuracy 0.5135
Time taken for 1 epoch: 348.5253598690033 secs

Saving checkpoint for epoch 46 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-46
Epoch 46 Loss 0.5961 Accuracy 0.5141
Time taken for 1 epoch: 349.2892951965332 secs

Saving checkpoint for epoch 47 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-47
Epoch 47 Loss 0.5916 Accuracy 0.5151
Time taken for 1 epoch: 347.695574760437 secs

Saving checkpoint for epoch 48 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-48
Epoch 48 Loss 0.5873 Accuracy 0.5159
Time taken for 1 epoch: 347.0519881248474 secs

Saving checkpoint for epoch 49 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-49
Epoch 49 Loss 0.5825 Accuracy 0.5166
Time taken for 1 epoch: 347.1879811286926 secs

Saving checkpoint for epoch 50 at /content/gdrive/My Drive/nmt/checkpoints/4layers_256d_8heads_1024dff_90train_perc/ckpt-50
Epoch 50 Loss 0.5785 Accuracy 0.5176
Time taken for 1 epoch: 347.0608217716217 secs

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TODO_2">TODO<a class="anchor-link" href="#TODO">&para;</a></h2><ul>
<li>看 decoder 的 self-attention</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluate">Evaluate<a class="anchor-link" href="#Evaluate">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following steps are used for evaluation:</p>
<ul>
<li>Encode the input sentence using the Portuguese tokenizer (<code>tokenizer_pt</code>). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.</li>
<li>The decoder input is the <code>start token == tokenizer_en.vocab_size</code>.</li>
<li>Calculate the padding masks and the look ahead masks.</li>
<li>The <code>decoder</code> then outputs the predictions by looking at the <code>encoder output</code> and its own output (self-attention).</li>
<li>Select the last word and calculate the argmax of that.</li>
<li>Concatentate the predicted word to the decoder input as pass it to the decoder.</li>
<li>In this approach, the decoder predicts the next word based on the previous words it predicted.</li>
</ul>
<p>Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def evaluate(inp_sentence):</span>
<span class="c1">#   start_token = [tokenizer_zh.vocab_size]</span>
<span class="c1">#   end_token = [tokenizer_zh.vocab_size + 1]</span>
  
<span class="c1">#   # inp sentence is portuguese, hence adding the start and end token</span>
<span class="c1">#   inp_sentence = start_token + tokenizer_zh.encode(inp_sentence) + end_token</span>
<span class="c1">#   encoder_input = tf.expand_dims(inp_sentence, 0)</span>
  
<span class="c1">#   # as the target is english, the first word to the transformer should be the</span>
<span class="c1">#   # english start token.</span>
<span class="c1">#   decoder_input = [tokenizer_en.vocab_size]</span>
<span class="c1">#   output = tf.expand_dims(decoder_input, 0)</span>
    
<span class="c1">#   for i in range(MAX_LENGTH):</span>
<span class="c1">#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(</span>
<span class="c1">#         encoder_input, output)</span>
  
<span class="c1">#     # predictions.shape == (batch_size, seq_len, vocab_size)</span>
<span class="c1">#     predictions, attention_weights = transformer(encoder_input, </span>
<span class="c1">#                                                  output,</span>
<span class="c1">#                                                  False,</span>
<span class="c1">#                                                  enc_padding_mask,</span>
<span class="c1">#                                                  combined_mask,</span>
<span class="c1">#                                                  dec_padding_mask)</span>
    
<span class="c1">#     # select the last word from the seq_len dimension</span>
<span class="c1">#     predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)</span>

<span class="c1">#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)</span>
    
<span class="c1">#     # return the result if the predicted_id is equal to the end token</span>
<span class="c1">#     if tf.equal(predicted_id, tokenizer_en.vocab_size+1):</span>
<span class="c1">#       return tf.squeeze(output, axis=0), attention_weights</span>
    
<span class="c1">#     # concatentate the predicted_id to the output which is given to the decoder</span>
<span class="c1">#     # as its input.</span>
<span class="c1">#     output = tf.concat([output, predicted_id], axis=-1)</span>

<span class="c1">#   return tf.squeeze(output, axis=0), attention_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def plot_attention_weights(attention, sentence, result, layer):</span>
<span class="c1">#   fig = plt.figure(figsize=(8, 16))</span>
  
<span class="c1">#   sentence = tokenizer_en.encode(sentence)</span>
  
<span class="c1">#   attention = tf.squeeze(attention[layer], axis=0)</span>
  
<span class="c1">#   for head in range(attention.shape[0]):</span>
<span class="c1">#     ax = fig.add_subplot(4, 2, head+1)</span>
    
<span class="c1">#     # plot the attention weights</span>
<span class="c1">#     ax.matshow(attention[head][:-1, :], cmap='viridis')</span>

<span class="c1">#     fontdict = {'fontsize': 10, "fontproperties": zhfont}</span>
    
<span class="c1">#     ax.set_xticks(range(len(sentence)+2))</span>
<span class="c1">#     ax.set_yticks(range(len(result)))</span>
    
<span class="c1">#     ax.set_ylim(len(result)-1.5, -0.5)</span>
        
<span class="c1">#     ax.set_xticklabels(</span>
<span class="c1">#         ['&lt;start&gt;']+[tokenizer_en.decode([i]) for i in sentence]+['&lt;end&gt;'], </span>
<span class="c1">#         fontdict=fontdict, rotation=90)</span>
    
<span class="c1">#     ax.set_yticklabels([tokenizer_zh.decode([i]) for i in result </span>
<span class="c1">#                         if i &lt; tokenizer_zh.vocab_size], </span>
<span class="c1">#                        fontdict=fontdict)</span>
    
<span class="c1">#     ax.set_xlabel('Head {}'.format(head+1))</span>
  
<span class="c1">#   plt.tight_layout()</span>
<span class="c1">#   plt.show()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can pass different layers and attention blocks of the decoder to the <code>plot</code> parameter.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&para;</a></h2><p>In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.</p>
<p>Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create <a href="https://arxiv.org/abs/1810.04805">BERT</a> and train state of the art models. Futhermore, you can implement beam search to get better predictions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>transformer-XL animation</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ian goodfellow interview</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">幾個要求</span><span class="err">：</span>
<span class="o">-</span> <span class="n">用大螢幕看</span>
<span class="o">-</span> <span class="n">分享給更多人知道</span>
<span class="o">-</span> <span class="n">給我你的</span> <span class="n">fb</span>
</pre></div>
</div>
</div>
</div>
</div>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                        <a href="https://leemeng.tw/tag/nlp.html" rel="tag">NLP</a>
                        <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">Tensorflow</a>
                    </span>

                </p>













































































                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary " style="color: #FFFFFF">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '淺談神經機器翻譯：如何用 Transformer 以及 Tensorflow 2 實現英中翻譯';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
                </div>

                <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>                </ul>
            </div>
        </div>

        <div class="row footer-bottom">
            <div class="col-twelve">
                <div class="copyright">
                    <span>Powered by <a href="http://getpelican.com/" target="_blank">Pelican</a></span>
                    <span>© Copyright Hola 2017</span>
                    <span>Design by <a href="https://www.styleshout.com/" target="_blank">styleshout</a></span>
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>
        </div> <!-- end footer-bottom -->
    </footer> <!-- end footer -->


    <div id="preloader">
        <div id="loader"></div>
    </div>

        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script>


</body>
</html>