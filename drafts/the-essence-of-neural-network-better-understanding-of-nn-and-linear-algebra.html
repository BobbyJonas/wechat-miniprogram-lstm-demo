<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 神經網路的本質：直觀理解神經網路 & 線性代數</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="s" />
        <meta name="keywords" content="Manim, Python" />
        <meta name="tags" content="Manim" />
        <meta name="tags" content="Python" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/TwoLayersReLUInBetweenSolveHardTwoCurves.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html" />
        <meta property="og:title" content="神經網路的本質：直觀理解神經網路 & 線性代數" />
        <meta property="og:description" content="s" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <!--for responsive embed youtube video-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/embed_youtube.css">

    <!--for prettify dark-mode result-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/darkmode.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="神經網路的本質：直觀理解神經網路 &amp; 線性代數">神經網路的本質：直觀理解神經網路 &amp; 線性代數</a><ul><li><a class="toc-href" href="#一些能夠幫助你的背景知識" title="一些能夠幫助你的背景知識">一些能夠幫助你的背景知識</a></li><li><a class="toc-href" href="#深度學習框架操作容易，但你真的了解神經網路嗎？" title="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？</a></li><li><a class="toc-href" href="#返璞歸真：神經網路怎麼解決二元分類任務？" title="返璞歸真：神經網路怎麼解決二元分類任務？">返璞歸真：神經網路怎麼解決二元分類任務？</a></li><li><a class="toc-href" href="#第一印象：直觀感受簡單-NN-如何解決二分類任務" title="第一印象：直觀感受簡單 NN 如何解決二分類任務">第一印象：直觀感受簡單 NN 如何解決二分類任務</a></li><li><a class="toc-href" href="#結論" title="結論">結論</a></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/TwoLayersReLUInBetweenSolveHardTwoCurves.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="https://leemeng.tw/tag/manim.html" rel="tag">Manim</a>
                            <a href="https://leemeng.tw/tag/python.html" rel="tag">Python</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html" title="">
                            神經網路的本質：直觀理解神經網路 & 線性代數
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-09-21 (Sat)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>



    <!--custom images with icon shown on left nav-->

</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這是篇幫助你直觀理解神經網路的科普文。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定深度學習之旅的基礎。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>（小提醒：因動畫皆為黑色背景，強烈推薦用左下按鈕以 Dark Mode 閱讀本文）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是個眾人對人工智慧（<strong>A</strong>rtificial <strong>I</strong>ntelligence, AI）趨之若鶩的時代。此領域近年的快速發展很大一部份可歸功於<a href="https://leemeng.tw/deep-learning-resources.html">深度學習（Deep Learning）</a>以及<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神經網路（<strong>N</strong>eural <strong>N</strong>etwork, 後簡稱為 NN）</a>。現行的深度學習框架也日漸成熟，讓任何人都可以使用 <a href="https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/beginner.ipynb">TensorFlow</a> 或 <a href="https://pytorch.org/">PyTorch</a> 輕鬆建立神經網路，解決各式各樣的問題。</p>
<p>舉個例子，你在 30 秒內就可以訓練出一個具有 98% 正確率的數字辨識 NN：</p>
<div class="highlight"><pre><span></span><span class="c1"># 此例使用 TensorFlow，但各大深度學習框架的實現邏輯基本上類似</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="c1"># 載入深度學習 Hello World: MNIST 數字 dataset</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># 建立一個約有 10 萬個參數的「小型」神經網路</span>
<span class="c1"># 在現在模型參數動輒上千萬、上億的年代，此神經網路不算大</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># 選擇損失函數、optimizer</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="c1"># 訓練模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 訓練後的 NN 在測試集上可得到近 98% 正確辨識率</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># 實際測試結果</span>
<span class="c1"># loss: 0.0750 - accuracy: 0.9763</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>是的，扣除註解不到 15 行就可以把讀取數據、訓練 <code>model</code> 以及推論全部搞定了。這邊秀出程式碼只是要讓你感受一下透過框架（framework）建立一個神經網路有多麽地「簡單」。而基本上這也是現在絕大多數線上課程以及教學文章會/能教你的東西。對數字辨識有興趣的讀者可以自行嘗試<a href="https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/beginner.ipynb"> TensorFlow 的 Colab 筆記本</a>。</p>
<p>我等等要秀給你看的神經網路以參數量來說比上面這個 <code>model</code> 還簡單 10,000 倍，但保證很有看頭。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="一些能夠幫助你的背景知識">一些能夠幫助你的背景知識<a class="anchor-link" href="#一些能夠幫助你的背景知識">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我會透過動畫來幫你把神經網路與<a href="https://www.youtube.com/watch?v=fNk_zzaMoSs">線性代數（Linear Algebra）</a>的概念結合，但具備些背景知識能讓你有更深的體會：</p>
<ol>
<li>能夠讀懂文章開頭建立 NN 的 <a href="https://www.python.org/">Python</a> 程式碼</li>
<li>了解線上課程都會教的<a href="https://www.youtube.com/watch?v=Dr-WRlEFefw">超基本 NN 概念</a><ul>
<li>何謂<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（Fully Connected Layer）</a></li>
<li>何謂參數以及如何估計全連接層的參數量</li>
<li>常見的 activation functions 如 <a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">ReLU</a></li>
</ul>
</li>
<li>基本的線性代數概念如矩陣相乘、向量空間</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/00010.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        一個簡單 NN 嘗試解決二元分類的過程（線性轉換）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外，我會把目光放在<strong>已經訓練好</strong>的 NN，因此不會特別說明<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">如何訓練一個簡單神經網路</a>。不過別擔心，就算你完全沒背景知識應該也能繼續閱讀。等等如果覺得節奏太快，可以回來參考<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">自然語言處理與深度學習入門指南</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？<a class="anchor-link" href="#深度學習框架操作容易，但你真的了解神經網路嗎？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>讓我們再次回到文章開頭透過 <a href="https://www.tensorflow.org/guide/keras">Keras</a> 建立的神經網路 <code>model</code>：</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
<p>前人種樹，後人乘涼。</p>
<p>就跟你在任何教學文章裡頭都會看到的，現在要使用深度學習框架建立<strong>基本的</strong> NN 非常容易，只要當作疊疊樂一層層 layer 疊上去就好了。下圖則將 <code>model</code> 用視覺上更容易理解的方式呈現：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/manim/mnist-simple.jpg"/>
</center>
<center>
                        輸入是 28*28 = 784 維的圖片像素，輸出則是 10 個數字類別的簡單 2-layers NN
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們也可以一鍵查看整個 NN 的參數量：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               100480    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>儘管擁有 10 萬個可訓練參數，此 <code>model</code> 是個在深度學習領域裡只能被歸類在 Hello World 等級的可憐 NN。畢竟這世界很瘋狂，<a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">我們以前討論過的 BERT</a> 以及 <a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">GPT-2</a> 都是現在 NLP 界的知名語言模型，而它們可都是擁有<strong>上億</strong>參數的強大 NN。那可是此 <code>model</code> 的 100 倍大。</p>
<p>但先別管 BERT 或 GPT-2 了，就算是這個 Hello World 等級的 NN，你真的覺得你對它的運作機制有足夠的理解嗎？</p>
<p>講白點，儘管現在會使用 TensorFlow 或是 PyTorch 來建立神經網路的人多如牛毛，事實上不少人（包含剛入門的我）對最基本的神經網路都沒有足夠<strong>直觀</strong>的理解。而這主要是因為：</p>
<ul>
<li>強大的深度學習框架把底層細節邏輯都包了起來</li>
<li>不少人到現在還沒搞懂跟 NN 相關的線性代數概念</li>
<li>能視覺化神經網路內部機制的手法並不那麼普遍</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/manim/three-key-components.jpg"/>
</center>
<center>
                        構成本文的關鍵三要素：Manim、二元分類與簡單 NN
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了讓更多人能夠<strong>直觀</strong>地體會神經網路的運作機制，我將透過：</p>
<ol>
<li>繪圖工具：<a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3Blue1Brown</a> 的強大 Python 動畫引擎 <a href="https://github.com/3b1b/manim">Manim</a></li>
<li>學習任務：比 <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> 還簡單許多的<strong> 2 維</strong><a href="https://en.wikipedia.org/wiki/Binary_classification">二元分類</a>問題</li>
<li>模型架構：1 到 2 層、只有不到 10 個參數的超簡單神經網路</li>
</ol>
<p>來闡述<strong>基本的</strong>神經網路以及相關線性代數概念。前言很長，但如果你想要了解神經網路的本質，或是喜歡看動畫來了解數學概念，那我會建議繼續往下閱讀：）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="返璞歸真：神經網路怎麼解決二元分類任務？">返璞歸真：神經網路怎麼解決二元分類任務？<a class="anchor-link" href="#返璞歸真：神經網路怎麼解決二元分類任務？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://en.wikipedia.org/wiki/Binary_classification">二元分類（Binary Classification）</a>是<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html">機器學習（Machine Learning）</a>領域裡一個十分基本的任務，其目標是把一個集合（set）中的所有數據點（data point）依照某種分類規則劃分成<strong>兩</strong>個族群（groups）或類別（classes）。比方我們之間看過的<a href="https://demo.leemeng.tw/">貓狗圖像辨識</a>。</p>
<p>如前所述，本文會將數據點的最大維度限制為 <strong>2 </strong>。如果你讀過我之前的任何一篇文章，可能會覺得這任務是在「羞辱」我們的智商。畢竟我們已經用 NN 達成以下成就：</p>
<ul>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">執行假新聞偵測（BERT）</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">生成新金庸小說（GPT-2）</a></li>
<li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html">把英文翻成中文（Transformer）</a></li>
<li><a href="https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html">生成新海誠動畫（CartoonGAN）</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這些「進階」任務跟本文的 2 維二元分類相比複雜許多，也十分有趣且具有挑戰性。如果你有興趣深入了解，可以點擊對應連結來查看我為這些任務撰寫的相關文章。</p>
<p>回到二元分類。正因其十分單純，使得我們能夠一窺 NN 的本質。你馬上會發現，光是觀察 NN 如何解決這個簡單任務，就能讓你對 NN 有更深刻且直觀的理解。</p>
<p>比方說以下是一個假想的二元分類資料集（dataset）：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/ShowLinearSeparableDataPoints.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        包含兩類別（曲線）的 2 維資料集
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在此資料集裡，兩曲線分別來自不同類別，各自包含 100 個數據點 $x$。每個數據點 $x$ 可以很自然地用 2 維的 $\left (x_{coord}, y_{coord}  \right )$ 座標來表示。另外你可以從右下角得知黃點的標籤 $y = 0$，藍點為 $1$。</p>
<p>我也將 x 軸與 y 軸上的 2 個基底向量（basis vector）分別用藍色與紅色表示。</p>
<p>那麼要如何分類這個資料集呢？複習一下我們在<a href="https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html"> AI For Everyone 的 10 個重要 AI 概念</a>裡就已看過的內容：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        目前多數的機器學習以及 AI 應用本質上都是讓電腦學會一個映射函數（Mapping Function），幫我們將輸入的數據 x 轉換到理想的輸出 y。
                        <br/>
<span style="float:right;margin-right: 1.5rem">─ Andrew Ng</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>套用相同想法，所謂的二元分類問題即是「在給定所有藍點與黃點 $x = \left (x_{coord}, y_{coord}  \right )$ 的情況下，你能不能找出一個函數 $f$，將這些 2 維數據 $x$ 完美地<strong>轉換</strong>到它們的 1 維標籤 $y$ 呢？」</p>
<p>換句話說，我們想要找出一個 $x$ 的函數 $f(x)$ 使得以下式子成立：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) 
&amp; = f(\begin{bmatrix} x_{coord} \\ y_{coord} \end{bmatrix}) \\
&amp; = f(\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}) \\
&amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們有無數種 model $f(x)$ 的方法，但在線性代數的世界裡，我們可以用矩陣運算的形式定義 $f(x)$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) &amp; = W x + b \\
  &amp; = \begin{bmatrix} w_{1} &amp; w_{2}  \end{bmatrix} x + b \\ 
  &amp; = \begin{bmatrix} w_{1} &amp; w_{2}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
  &amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在上面的式子中：</p>
<ul>
<li>$x$ 是一個 2 維的 column vector</li>
<li>$W$ 是一個 1 x 2 的權重矩陣（weight matrix）</li>
<li>$b$ 為偏差（bias），是一個純量（scalar）</li>
</ul>
<p>如果我們先暫時忽略 $b$，事實上 $f(x)$ 對輸入 $x$ 做的<strong>轉換</strong>跟大多數 NN 都會使用到的<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（<strong>F</strong>ully <strong>C</strong>onnected Layer，後稱 FC）</a>是完全一致的。反過來說，使用一層 FC 的 NN 基本上就是在做矩陣運算（以及 activation function，後述）。</p>
<p>因此 $f(x)$ 與 NN 之間有美好的對應關係：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/SymbolicOneByTwoMatrixAndNN.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        1 x 2 矩陣運算與 1-Layer NN 的動態同步
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>沒錯，我們剛剛建立了這世上最簡單的 1-Layer 神經網路！而之所以說是 1-Layer，是因為圖上 NN 的第一層為輸入數據（input data），第二層開始才是我們實際<strong>新定義</strong>的 FC。矩陣跟 FC 的對應關係家喻戶曉，但我相信這應該是你第一次實際看到兩者之間的<strong>動態同步</strong>。</p>
<p>從線性代數的角度來看，我們是透過權重矩陣 $W$ 對 2 維的 $x$ 進行線性轉換後得到 1 維的 $y$；而以神經網路的角度檢視，我們是將以 2 維向量代表（represent）的 $x$ 透過與權重 $W$ 進行加權總和後得到新的 representation $y$。這是你常聽到有人說深度學習就是在做<a href="https://arxiv.org/abs/1206.5538">表徵學習（Representation Learning）</a>的原因。</p>
<p>另外值得注意的是矩陣 $W$ 裡頭每個參數 $w_{ij}$ 的命名方式。$w_{ij}$ 代表是從 NN 前一層第 $i$ 個神經元（neuron）連到下一層第 $j$ 個神經元的邊（edge）。這個 notation 是為了讓你能更直觀地了解 NN 與矩陣之間的對應關係，跟傳統線性代數命名矩陣元素時有些差異，還請留意。</p>
<p>噢對了，就跟文章開頭看到的程式碼一樣，要使用 TensorFlow 定義如上的神經網路也十分容易：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> 
                                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 2)                 2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>扣除 bias，$W$ 的確只有 2 個參數。為了讓你加深印象，讓我們實際將一組數字代入 $W$ 與 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
W = \begin{bmatrix} 1 &amp; 0  \end{bmatrix} \\ 
x = \begin{bmatrix} 3 \\ 7 \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>則運作結果如下：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/NumberOneByTwoMatrixAndNN.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">從這邊之後繼續寫</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而如果你了解<a href="https://youtu.be/fNk_zzaMoSs">線性代數的本質</a>，就會知道這個 $f(x)$ 實際上是透過一個簡單的線性轉換（linear transformation）以及位移（offset）來將 2 維的 $x$ 轉換成 1 維的 $y$。</p>
<p>舉個例子，假設 $W = [1, -1]$，$b = 3$，則此 1-Layer NN 會這樣轉換 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/ApplyMatrixAsSingleLinearTransformation.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="{filename}images/s"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>事實上，這也是世界上最簡單的神經網路之一</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="第一印象：直觀感受簡單-NN-如何解決二分類任務">第一印象：直觀感受簡單 NN 如何解決二分類任務<a class="anchor-link" href="#第一印象：直觀感受簡單-NN-如何解決二分類任務">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">秀出</span> <span class="n">data</span> <span class="n">圖</span><span class="err">，</span><span class="n">問讀者要怎麼解</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你的時間寶貴，因此我事先將本文的精華都濃縮在底下這個 1 分鐘的短片了。此影片展示了一個簡單神經網路解決二分類任務的<strong>完整過程</strong>。我等等會更仔細地說明影片內容，但現在請你馬上點擊播放鍵觀看吧！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video controls="" muted="" playsinline="" poster="https://leemeng.tw/drafts/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="https://leemeng.tw/images/manim/TwoLayersReLUInBetweenSolveHardTwoCurves.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>順帶一提，這是一個只有 9 個參數的神經網路，其規模跟現在媒體整天在報導的 A.I. 相比可說是滄海一粟。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        但這是我看過最美麗、直觀的神經網路運作。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我不知道看影片前的你對神經網路或是線性代數的理解程度，但我相信很多人都能在這短短的一分鐘內（重新）獲得些啟發。如果你願意，我強烈建議你至少再看一次影片並在需要時暫停咀嚼。裡頭有很多十分基本但重要的 NN 概念值得掌握。</p>
<p>你有再看一遍嗎？讓我幫你把影片裡隱含的重要概念一一列出：</p>
<ul>
<li>神經網路是將輸入（維度）做一連串簡單轉換，最後得到理想輸出（維度）的函式</li>
<li>最基本且常見的神經網路是「層」為單位，每一層的矩陣相乘事實上都在做線性轉換</li>
<li>線性轉換基本上就是對輸入空間的數據做旋轉、縮放、延伸等轉換</li>
<li>層跟層之間常會透過非線性轉換函式來提升神經網路整體的轉換能力</li>
<li>神經網路也常被視為是在做 representation learning，因為每一層的轉換都將該層輸入的數據轉換成更適合達到任務目標的形式</li>
<li>透過對輸入數據做一連串適當的</li>
<li>針對某些任務，特定的神經網路架構有其能力極限，我們透過學習從該架構中找出一組參數，讓神經網路做適合的轉換以完成任務</li>
</ul>
<p>很多東西你可能都知道了，人類是視覺動物，</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="結論">結論<a class="anchor-link" href="#結論">&para;</a></h2><ul>
<li>下一代</li>
<li>我們在 dl resource 說過神經網路是一連串的幾何轉換</li>
</ul>
</div>
</div>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="https://leemeng.tw/tag/manim.html" rel="tag">Manim</a>
                        <a href="https://leemeng.tw/tag/python.html" rel="tag">Python</a>
                    </span>

                </p>























































































                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary ">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '神經網路的本質：直觀理解神經網路 & 線性代數';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<!-- footer
================================================== -->
<footer>
    <div class="row footer-bottom">
        <div class="col-twelve">
            <div class="go-top">
            <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
            </div>
        </div>
    </div> <!-- end footer-bottom -->
</footer> <!-- end footer -->


        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script><!--https://darkmodejs.learn.uno/-->
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.4.0/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.2s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>


</body>
</html>