<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="TODO" />
        <meta name="keywords" content="TensorFlow, TensorFlow.js, 自然語言處理" />
        <meta name="tags" content="TensorFlow" />
        <meta name="tags" content="TensorFlow.js" />
        <meta name="tags" content="自然語言處理" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/text-generation-cover.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/drafts/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html" />
        <meta property="og:title" content="讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部" />
        <meta property="og:description" content="TODO" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部">讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部</a><ul><li><a class="toc-href" href="#Demo：生成全新的天龍八部橋段" title="Demo：生成全新的天龍八部橋段">Demo：生成全新的天龍八部橋段</a></li><li><a class="toc-href" href="#模型是怎麼被訓練的" title="模型是怎麼被訓練的">模型是怎麼被訓練的</a></li><li><a class="toc-href" href="#TensorFlow-2.0-開發" title="TensorFlow 2.0 開發">TensorFlow 2.0 開發</a></li><li><a class="toc-href" href="#深度學習專案步驟" title="深度學習專案步驟">深度學習專案步驟</a><ul><li><a class="toc-href" href="#1.-定義問題及要解決的任務" title="1. 定義問題及要解決的任務">1. 定義問題及要解決的任務</a></li><li><a class="toc-href" href="#2.-準備原始數據、資料清理" title="2. 準備原始數據、資料清理">2. 準備原始數據、資料清理</a></li><li><a class="toc-href" href="#3.-建立能丟入模型的資料集" title="3. 建立能丟入模型的資料集">3. 建立能丟入模型的資料集</a></li><li><a class="toc-href" href="#4.-定義能解決問題的函式集" title="4. 定義能解決問題的函式集">4. 定義能解決問題的函式集</a></li><li><a class="toc-href" href="#5.-定義評量函式好壞的指標" title="5. 定義評量函式好壞的指標">5. 定義評量函式好壞的指標</a></li><li><a class="toc-href" href="#6.-訓練並選擇出最好的函式" title="6. 訓練並選擇出最好的函式">6. 訓練並選擇出最好的函式</a></li><li><a class="toc-href" href="#7.-將函式-/-模型拿來做預測" title="7. 將函式 / 模型拿來做預測">7. 將函式 / 模型拿來做預測</a></li></ul></li><li><a class="toc-href" href="#如何使用-TensorFlow.js-跑模型並生成文章_1" title="如何使用 TensorFlow.js 跑模型並生成文章">如何使用 TensorFlow.js 跑模型並生成文章</a></li><li><a class="toc-href" href="#結語" title="結語">結語</a></li></ul></li></ul></div>
      </div>
    </div>


    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/text-generation-cover.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">TensorFlow</a>
                            <a href="https://leemeng.tw/tag/tensorflowjs.html" rel="tag">TensorFlow.js</a>
                            <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/drafts/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html" title="">
                            讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-03-25 (Mon)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTovNav()"></i>
        </a>
    </div>


</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<link href="https://leemeng.tw/tfjs-apps/lstm-text-generation/index.css" rel="stylesheet"/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote style="margin-bottom: 1rem">
<p>
        木婉清轉頭向他，背脊向著南海鱷神，低聲道：「你是世上第一個見到我容貌的男子！」緩緩拉開了面幕。段譽登時全身一震，眼前所見，如新月清暉，如花樹堆雪，一張臉秀麗絕俗。
        <br/>
<span style="float:right;margin-right: 1.5rem">第四回：崖高人遠</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<p><a href="https://bit.ly/2TUycBQ">《天龍八部》</a>一直是我最喜歡的<a href="https://zh.wikipedia.org/wiki/%E9%87%91%E5%BA%B8%E4%BD%9C%E5%93%81">金庸著作</a>之一，最近重新翻閱，有很多新的感受。</p>
<p>閱讀到一半我突發奇想，決定用<a href="https://leemeng.tw/deep-learning-resources.html">深度學習</a>來訓練一個能夠生成《天龍八部》的<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF">循環神經網路</a>。訓練結果還不完美，但我認為已經很有娛樂性質，且有時已經能夠產生令人驚嘆或是捧腹大笑的文章了。</p>
<p>因此我決定使用 <a href="https://www.tensorflow.org/js">Tensorflow.js</a> 將利用 <a href="https://www.tensorflow.org/alpha">TensorFlow 2.0</a> 訓練出來的模型弄上線，讓你也能實際看看這個 AI 上嗑了什麼藥。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/dali-old-castle.jpg"/>
</center>
<center>
                        大理古城一隅，段譽出身之地
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 demo 之後，針對那些熟悉 <a href="https://www.python.org/">Python</a> 或是 <a href="https://www.javascript.com/">JavaScript</a> 的你，我將簡單地說明如何使用 TensorFlow 以及 Tensorflow.js 來實作這樣的 AI 應用。</p>
<p>希望你能從中得到些啟發，並在閱讀本文後實際動手做點自己的東西。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Demo：生成全新的天龍八部橋段">Demo：生成全新的天龍八部橋段<a class="anchor-link" href="#Demo：生成全新的天龍八部橋段">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>本篇使用一個十分簡單的<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E">長短期記憶 RNN</a> 來生成文章。在多次「閱讀」天龍八部之後，這個模型可以在給定一段新文本的情況下，逐字產生類似風格的文章。</p>
<p>比方說給定書中的一個橋段：</p>
<div class="highlight"><pre><span></span>烏老大偏生要考一考慕容復，說道：「慕容公子，你瞧這不是大大的
</pre></div>
<p>模型順著上面的話，接著產生：</p>
<div class="highlight"><pre><span></span>不算？」馬夫人道：「不錯，咱們非要尋死不可。」

段譽大喜，說道：「小姑娘，你待我這麼好，鬼鬼祟祟，一切又不聽你的話，你管甚麼老兄弟不相干，我去幫過彥之。」

王夫人哼了一聲，說道：「這裏是甚麼話？」段譽道：「不行！你別過來。用真蠻子，我便將這件事了，一大惡人擠在地下，立時便會斃命，那便如何是好？」
</pre></div>
<p>模型產生的文章內容本身很ㄎ一ㄤ，惹人發笑，但用詞本身很天龍八部。（至少我自己寫不出這樣的內容）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/antony-xia-522590-unsplash.jpg"/>
</center>
<center>
                        姑蘇慕容家所在的蘇州
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>現在馬上就讓我們產生一些新的橋段吧！首先將已經訓練好的模型載入你的瀏覽器。依據你的網路速度，載入時間會有所差異。</p>
<p>成功載入後只要不重新整理此頁面，你將可以不斷地使用同個模型產生新的文章。</p>
<section style="margin-bottom: 3rem">
<button id="load-model" style="display:inline-block">載入模型</button>
<div id="app-status" style="display:inline-block"></div>
</section>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外你會發現有 2 個可供你調整的參數：</p>
<section style="margin-bottom: 3rem">
<div>
<span class="input-title">生成長度（字單位）</span>
<input id="generate-length" value="150"/>
</div>
<div>
<span class="input-title">生成溫度（隨機度）</span>
<input id="temperature" value="0.6"/>
</div>
</section><p>第一次可以直接使用預設值。現在點擊<strong>生成文章</strong>來產生全新的天龍八部橋段：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<section style="margin-bottom: 3rem">
<div>
<button disabled="true" id="generate-text">生成文章</button>
<button disabled="true" id="initialize-seed">重置輸入</button>
</div>
</section>
<section style="margin-bottom: 3rem">
<div>
<span class="input-title">起始句子：</span>
<span id="text-generation-status" style="display: none"></span>
<textarea id="seed-text" rows="1" style="min-height: 6em" value="">蕭峯吃了一驚，心想：「哥哥大喜之餘，說話有些忘形了，眼下亂成</textarea>
</div>
</section>
<section style="margin-bottom: 3rem">
<div>
<span class="input-title">生成結果：</span>
<textarea id="generated-text" readonly="true" rows="10" value=""></textarea>
</div>
</section>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如何？希望模型產生的結果有令你會心一笑。它當初可快把我逗死了。</p>
<p>現在你可以嘗試幾件事情：</p>
<ul>
<li>點<strong>生成文章</strong>來讓模型依據同輸入產生新橋段</li>
<li>點<strong>重置輸入</strong>來隨機取得一個新的起始句子</li>
<li>增加模型生成的<strong>文章長度</strong></li>
<li>調整<strong>生成溫度</strong>來改變文章的變化性</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/chris-rhoads-254898-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>生成溫度是一個實數值，而當溫度越高，模型產生出來的結果越隨機、越不可預測（也就越ㄎㄧㄤ）；而溫度越低，產生的結果就會越像天龍八部原文。優點是真實，但同時字詞的重複性也會提升。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        機器並沒有情感，只有人類可以賦予事物意義。我們無法讓機器自動找出所謂的最佳生成溫度，因為人的感覺十分主觀：找出你自己覺得最適合的溫度來生成文章。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果你沒有打算深入探討技術細節，那只需要記得在這篇文章裡頭的模型是一個以「字」為單位的語言模型（Character-based Language Model）即可：給定一連串已經出現過的字詞，模型會想辦法去預測出下一個可能出現的字。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/raychan-1229841-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>值得注意的是，我們並不單純是拿出現機率最高的字出來當結果，這樣太無趣了。</p>
<p>每次機器做預測前都會拿著一個包含大量中文字的機率分布 p，在決定要吐出哪個字時，會對該機率分佈 p 做抽樣，從中隨機選出一個字。因此就跟你在上面 demo 看到的一樣，就算輸入句子相同，每次模型仍然可能生成完全不同的文章。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/max-felner-448887-unsplash.jpg"/>
</center>
<center>
                        抽樣的過程類似擲骰子，儘管有些結果較易出現，你還是有機會骰到豹子
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>因為隨機抽樣的關係，每次模型產生的結果基本上都是獨一無二的。</p>
<p>如果你在生成文章的過程中得到什麼有趣的虛擬橋段，都歡迎留言或是透過 SNS 與我分享！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="模型是怎麼被訓練的">模型是怎麼被訓練的<a class="anchor-link" href="#模型是怎麼被訓練的">&para;</a></h2><p>在看完 demo 以後，你可能會好奇這個模型是怎麼被訓練出來的。</p>
<p>實際的開發流程大致可以分為兩個部分：</p>
<ul>
<li><a href="https://www.tensorflow.org/alpha/tutorials/sequences/text_generation">用 TensorFlow 2.0 訓練一個 LSTM 模型</a></li>
<li><a href="https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation">使用 TensorFlow.js 部屬該模型</a></li>
</ul>
<p>這些在 TensorFlow 以及 TensorFlow.js 官網都有十分詳細的說明。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/tf-demo.png"/>
</center>
<center>
                        這篇文章參考了不少 TensorFlow 官網（左）及 TensorFlow.js 線上 demo（右）的程式碼
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果你也想開發一個類似的應用，閱讀官方教學中你所熟悉的語言版本（Python / JavaScript）是最直接的作法：</p>
<ul>
<li><a href="https://www.tensorflow.org/alpha/tutorials/sequences/text_generation">TensorFlow 2.0 Alpha - Text generation with an RNN</a></li>
<li><a href="https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation">TensorFlow.js Example: Train LSTM to Generate Text</a></li>
</ul>
<p>因為官方已經有提供能在 <a href="https://colab.research.google.com/">Google Colab</a> 上使用 GPU <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb">訓練 LSTM 的教學筆記本</a>，本文便不再另行提供。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/simon-abrams-286276-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外，符合以下條件可以讓你更輕鬆地閱讀接下來的內容：</p>
<ul>
<li>熟悉 <a href="https://www.python.org/">Python</a></li>
<li>碰過 <a href="https://keras.io/">Keras</a> 或是 <a href="https://www.tensorflow.org/">TensorFlow</a></li>
<li>具備<a href="https://leemeng.tw/deep-learning-resources.html#courses">機器學習 &amp; 深度學習基礎</a></li>
<li>了解何謂<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF">循環神經網路</a>以及<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E">長短期記憶</a></li>
</ul>
<p>如果你是喜歡先把基礎打好的人，可以先查閱我上面附的這些資源連結。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TensorFlow-2.0-開發">TensorFlow 2.0 開發<a class="anchor-link" href="#TensorFlow-2.0-開發">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>平常有在接觸深度學習的讀者或許都已經知道，最近 TensorFlow 隆重推出 <a href="https://www.tensorflow.org/alpha">2.0 Alpha 預覽版</a>，希望透過全新的 API 讓更多人可以輕鬆地開發機器學習以及深度學習應用。</p>
<p>當初撰寫本文的其中一個目的，也是想趁著這次大改版來讓自己熟悉 TensorFlow 2.0 的開發方式。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="resp-container">
<iframe allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" class="resp-iframe" frameborder="0" src="https://www.youtube.com/embed/TTQQiJ-mHYA"></iframe>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>對我來說，TensorFlow 2.0 有幾個重點：</p>
<ul>
<li>將 <a href="https://www.tensorflow.org/alpha/guide/keras/overview">tf.keras</a> 視為其官方高級 API</li>
<li>方便除錯的 <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb">Eager Execution</a> 成為預設值</li>
<li>負責讀取、處理大量數據的 <a href="https://www.tensorflow.org/alpha/guide/data_performance">tf.data</a> API</li>
<li>自動幫你建構計算圖的 <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function">tf.function</a></li>
</ul>
<p>在這篇文章裡頭會看到前 3 者。下節列出的程式碼皆在 <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb">Google Colab</a> 上用最新版本的 TensorFlow 2.0 Nightly 執行。</p>
<div class="highlight"><pre><span></span>pip install tf-nightly-gpu-2.0-preview
</pre></div>
<p>如果有 GPU 則強烈建議安裝 GPU 版本的 TF Nightly，訓練速度跟 CPU 版本可以差到 10 倍以上。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="深度學習專案步驟">深度學習專案步驟<a class="anchor-link" href="#深度學習專案步驟">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>好戲終於登場。</p>
<p>如同多數的深度學習專案，要訓練一個以 LSTM 為基礎的語言模型，你大致需要走過以下幾個步驟：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/deep-learning-pj-steps-menglee.jpg"/>
</center>
<center>
                        開發一個 DL 專案時我常用的流程架構
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這個流程是一個大方向，依據不同情境你可能需要自行調整流程來符合自己的需求。</p>
<p>這篇文章會用 TensorFlow 2.0 簡單地帶你走過所有步驟。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-定義問題及要解決的任務">1. 定義問題及要解決的任務<a class="anchor-link" href="#1.-定義問題及要解決的任務">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很明顯地，在訓練模型前首先得確認我們的問題（Problem）以及想要交給機器解決的任務（Task）是什麼。</p>
<p>前面已經提過，我們的目標就是要找出一個天龍八部的語言模型（Language Model），讓該模型在被餵進一段文字以後，能吐出類似天龍八部的文章。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><div class="resp-container">
<iframe allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" class="resp-iframe" frameborder="0" src="https://www.youtube.com/embed/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9"></iframe>
</div></p>
<center>
    十分推薦李宏毅教授講解序列生成的影片
    <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這實際上是一個<a href="https://youtu.be/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9">序列生成（Sequence Generation）</a>問題，而機器所要解決的任務也變得明確：給定一段文字單位的序列，它要能吐出下一個合理的文字單位。</p>
<p>這邊說的文字單位（Token）可以是</p>
<ul>
<li>字（Character，如劍、寺、雲）</li>
<li>詞（Word，如吐蕃、師弟、阿修羅）</li>
</ul>
<p>本文則使用「字」作為一個文字單位。假設有一個天龍八部的句子：</p>
<div class="highlight"><pre><span></span>『六脈神劍經』乃本寺鎮寺之寶，大理段氏武學的至高法要。
</pre></div>
<p>這時候裡頭的每個字包含標點符號都是一個文字單位，整個句子就構成一個文字序列。我們可以擷取一部份的句子：</p>
<div class="highlight"><pre><span></span>『六脈神劍經』乃本寺鎮寺之寶，大理段氏武
</pre></div>
<p>接著在訓練模型時要求它讀入這段文字，並預測出原文裡頭下一個出現的字：<code>學</code>。</p>
<p>一旦訓練完成，就能得到你開頭看到的那個以字為單位的語言模型了。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-準備原始數據、資料清理">2. 準備原始數據、資料清理<a class="anchor-link" href="#2.-準備原始數據、資料清理">&para;</a></h3><p>巧婦難為無米之炊，沒有數據一切免談。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/caroline-attwood-243834-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我在網路上自行蒐集了天龍八部原文，做些簡單的數據清理後發現整本小說總共約含 120 萬個中文字，實在是一部曠世巨作。儘管因為版權問題不宜提供下載連結，Google 是你的好朋友。</p>
<p>現在假設我們把原文全部存在一個 Python 字串 <code>text</code> 裡頭，則部分內容可能如下：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 隨意取出第 9505 到 9702 的中文字</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">9505</span><span class="p">:</span><span class="mi">9702</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>咱們見敵方人多，不得師父號令，沒敢隨便動手。」左子穆道：「嗯，來了多少人？」干光豪道：「大約七八十人。」左子穆嘿嘿冷笑，道：「七八十人，便想誅滅無量劍了？只怕也沒這麼容易。」

龔光傑道：「他們用箭射過來一封信，封皮上寫得好生無禮。」說著將信呈上。

左子穆見信封上寫著：「字諭左子穆」五個大字，便不接信，說道：「你拆來瞧瞧。」龔光傑道：「是！」拆開信封，抽出信箋。

那少女在段譽耳邊低聲道：
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們也可以看看整本小說裡頭包含多少中文字：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"天龍八部小說共有 </span><span class="si">{n}</span><span class="s2"> 中文字"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"包含了 </span><span class="si">{w}</span><span class="s2"> 個獨一無二的字"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>天龍八部小說共有 1235431 中文字
包含了 4330 個獨一無二的字
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>相較於英文只有 26 個字母，中文字自然是比較多的。</p>
<p>如同我們在<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">寫給所有人的自然語言處理與深度學習入門指南</a>看過的，要將文本數據丟入神經網路，我們得先做些前處理，將這些中文字對應到一個個的數字（或稱索引）才行。</p>
<p>我們可以使用 <code>tf.keras</code> 裡頭的 <code>Tokenizer</code> 幫我們把整篇小說建立字典，並將同樣的中文字對應到同樣的索引數字：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># 初始化一個以字為單位的 Tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span>\
    <span class="o">.</span><span class="n">preprocessing</span>\
    <span class="o">.</span><span class="n">text</span>\
    <span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span>
        <span class="n">num_words</span><span class="o">=</span><span class="n">num_words</span><span class="p">,</span>
        <span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">filters</span><span class="o">=</span><span class="s1">''</span>
<span class="p">)</span>

<span class="c1"># 讓 tokenizer 讀過天龍八部全文，</span>
<span class="c1"># 為每個出現的字建立字典並將中文字轉</span>
<span class="c1"># 成對應的數字索引</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">text_as_int</span> <span class="o">=</span> <span class="n">tokenizer</span>\
    <span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 隨機選取一個片段文本方便之後做說明</span>
<span class="n">s_idx</span> <span class="o">=</span> <span class="mi">21004</span>
<span class="n">e_idx</span> <span class="o">=</span> <span class="mi">21020</span>
<span class="n">partial_indices</span> <span class="o">=</span> \
    <span class="n">text_as_int</span><span class="p">[</span><span class="n">s_idx</span><span class="p">:</span><span class="n">e_idx</span><span class="p">]</span>
<span class="n">partial_texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> \
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">partial_indices</span>
<span class="p">]</span>

<span class="c1"># 渲染結果，可忽略</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"原本的中文字序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"轉換後的索引序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_indices</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>原本的中文字序列：

['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']

--------------------

轉換後的索引序列：

[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很明顯地，現在整部天龍八部都已經被轉成一個巨大的數字序列，每一個數字代表著一個獨立的中文字。</p>
<p>我們可以換個方向再看一次：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>人類看的中文字   機器看的輸入索引  
------------------------------
司                557
空                371
玄                215
雙                214
掌                135
飛                418
舞               1209
，                  1
逼                837
得                 25
牠               1751
無                 49
法                147
近                537
前                111
。                  2
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-建立能丟入模型的資料集">3. 建立能丟入模型的資料集<a class="anchor-link" href="#3.-建立能丟入模型的資料集">&para;</a></h3><p>做完基本的數據前處理以後，我們需要將 <code>text_as_int</code> 這個巨大的數字序列轉換成神經網路容易消化的格式與大小。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text_as_int</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>[1639, 148, 3, 3, 280, 5, 192, 819, 374, 800]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">text_as_int</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_as_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"text_as_int 是一個 </span><span class="si">{_type}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"小說的中文字數： </span><span class="si">{n}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"前 5 個索引："</span><span class="p">,</span> <span class="n">text_as_int</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>text_as_int 是一個 &lt;class 'list'&gt;

小說的中文字數： 1235431

前 5 個索引： [1639, 148, 3, 3, 280]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        在建立資料集時，你要先能想像最終交給模型的數據長什麼樣子。這樣能幫助你對數據做適當的轉換。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>依照當前機器學習任務的性質，你會需要把不同格式的數據餵給模型。</p>
<p>在本文的序列生成任務裡頭，理想的模型要能依據前文來判斷出下一個中文字。因此我們要丟給模型的是一串代表某些中文字的數字序列：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"實際丟給模型的數字序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_indices</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"方便我們理解的文本序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_texts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>實際丟給模型的數字序列：
[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]

方便我們理解的文本序列：
['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而模型要給我們的理想輸出應該是向左位移一個字的結果：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"實際丟給模型的數字序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"方便我們理解的文本序列："</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">partial_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>實際丟給模型的數字序列：
[371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]

方便我們理解的文本序列：
['空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為什麼是這樣的配對？</p>
<p>想一想，一個模型如果可以給我們這樣的輸出，代表它：</p>
<ul>
<li>看到第一個輸入字 <code>司</code> 時可以正確輸出 <code>空</code></li>
<li>在之前看過 <code>司</code>，且新輸入字為 <code>空</code> 的情況下，可以輸出 <code>玄</code> </li>
<li>在之前看過 <code>司空</code>，且新輸入字為 <code>玄</code> 的情況下，可以輸出 <code>雙</code></li>
<li>在之前看過 <code>司空玄雙掌飛</code>，且新輸入字為 <code>舞</code> 的情況下，可以輸出 <code>，</code></li>
</ul>
<p>當一個語言模型可以做到這樣的事情，就代表它已經掌握了<strong>訓練文本</strong>（此文中為天龍八部）裡頭用字的統計結構，因此我們可以用它來產生新的天龍八部文章。</p>
<p>你現在應該也可以了解，這個語言模型是專為天龍八部的文本所誕生的。畢竟日常生活中，給你 <code>舞</code> 這個字，你接 <code>，</code> 的機率有多少呢？</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/niketh-vellanki-202943-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了讓你加深印象，讓我把序列擺直，再次列出模型的輸入以及輸出關係：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>時間點 輸入字  輸入索引   輸出字  輸出索引  
-------------------------------------
   1    司     557      空      371
   2    空     371      玄      215
   3    玄     215      雙      214
   4    雙     214      掌      135
   5    掌     135      飛      418
   6    飛     418      舞      1209
   7    舞     1209     ，      1
   8    ，     1        逼      837
   9    逼     837      得      25
  10    得     25       牠      1751
  11    牠     1751     無      49
  12    無     49       法      147
  13    法     147      近      537
  14    近     537      前      111
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>每一列（row）是一個時間點，而</p>
<ul>
<li><strong>輸入索引</strong>代表模型在當下時間吃進去的輸入</li>
<li><strong>輸出索引</strong>則代表我們要模型輸出的結果</li>
</ul>
<p>輸入字・輸出字則只是方便我們理解對照，實際上模型只吃數字。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/nlp-kaggle-intro/pop-zebra-754186-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>現在我們了解一筆輸入・輸出該有的數據格式了。兩者皆是一個固定長度的數字序列，而後者是前者往左位移一個數字的結果。</p>
<p>但這只是一筆數據（以下說的一筆數據，都隱含了輸入序列以及對應的輸出序列的 2 個數字序列）。</p>
<p>在有 GPU 的情況下，我們常常會一次丟一批（batch）數據，讓 GPU 可以平行運算，加快訓練速度。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/gpu.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>現在假設我們想要一個資料集，而此資料集可以一次給我們 128 筆長度為 10 的輸入・輸出序列，則我們可以用 <code>tf.data</code> 這樣做：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 方便說明，實際上我們會用更大的值來</span>
<span class="c1"># 讓模型從更長的序列預測下個中文字</span>
<span class="n">SEQ_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 數字序列長度</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># 幾筆成對輸入/輸出</span>

<span class="c1"># text_as_int 是一個 python list</span>
<span class="c1"># 我們利用 from_tensor_slices 將其</span>
<span class="c1"># 轉變成 TensorFlow 最愛的 Tensor &lt;3</span>
<span class="n">characters</span> <span class="o">=</span> <span class="n">tf</span>\
    <span class="o">.</span><span class="n">data</span>\
    <span class="o">.</span><span class="n">Dataset</span>\
    <span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="n">text_as_int</span><span class="p">)</span>

<span class="c1"># 將被以數字序列表示的天龍八部文本</span>
<span class="c1"># 拆成多個長度為 SEQ_LENGTH (10) 的序列</span>
<span class="c1"># 並將最後長度不滿 SEQ_LENGTH 的序列捨去</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">characters</span>\
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">SEQ_LENGTH</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
           <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 天龍八部全文所包含的成對輸入/輸出的數量</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> \
    <span class="nb">len</span><span class="p">(</span><span class="n">text_as_int</span><span class="p">)</span> <span class="o">//</span> <span class="n">SEQ_LENGTH</span>

<span class="c1"># 將一個片段的數字序列取頭去尾作為輸入序列</span>
<span class="c1"># 並把向左位移一個字的數據序列作為輸出序列</span>
<span class="c1"># （下面有 visualization 解釋這在做什麼）</span>
<span class="k">def</span> <span class="nf">build_seq_pairs</span><span class="p">(</span><span class="n">chunk</span><span class="p">):</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target_text</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">target_text</span>

<span class="c1"># 將每個從文本擷取出來的序列分別都</span>
<span class="c1"># 拆成兩個數字序列作為輸入／輸出序列</span>
<span class="c1"># 再將得到的所有數據隨機打亂順序</span>
<span class="c1"># 最後再一次拿出 BATCH_SIZE（128）筆數據</span>
<span class="c1"># 作為模型一次訓練步驟的所使用的資料</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">sequences</span>\
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">build_seq_pairs</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
           <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這段建構 <code>tf.data.Dataset</code> 的程式碼雖然不短，但有超過一半是我寫給你的註解。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>事實上用 <code>tf.data</code> 架構一個資料集並不難，且學會以後你每次都可用類似的方式呼叫 <a href="https://www.tensorflow.org/guide/datasets">TensorFlow Data API</a> 來處理<strong>任何</strong>文本數據，而不需要每次遇到新文本都從頭開始寫類似的功能（<code>batch</code>、<code>shuffle</code> etc）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/avi-richards-183715-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>上面已經有非常多的註解幫助你理解發生了什麼事，但如果你想自己動手，可以參考<a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb">官方用 TensorFlow 2.0 訓練 LSTM 的 Colab 筆記本</a>。</p>
<p>雖然我不是酷拉皮卡，但如果要把上面 <code>build_seq_pairs</code> 的處理具現化的話，大概就像是下面這樣（假設序列長度為 6）：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span>擷取的片段序列       輸入/輸出序列
-------------------------------
                 -&gt; 烏老大拱手還
                 |
烏老大拱手還禮 -----
                 |
                 -&gt; 老大拱手還禮


                 -&gt; 星宿派人數遠
                 |
星宿派人數遠較 -----
                 |
                 -&gt; 宿派人數遠較


                 -&gt; 過不多時，賈
                 |
過不多時，賈老 -----
                 |
                 -&gt; 不多時，賈老
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你會發現針對序列長度 <code>SEQ_LENGTH</code> 為 6 的情況，我會刻意將天龍八部文本切成長度為 <code>SEQ_LENGTH + 1</code>：7 的句子，再從這些句子建立出輸入及輸出序列。</p>
<p>到此為止，我們已經用 <code>tf.data</code> 建立出一個可以拿來訓練語言模型的資料集了。</p>
<p>TensorFlow 2.0 預設就是 <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb">Eager Execution</a>，因此你不再需要使用老朋友 <code>tf.Session()</code> 或是 <code>tf.placeholder</code> 就能非常直覺地存取數據：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># print 是用來幫你理解 tf.data.Dataset</span>
<span class="c1"># 的內容，實際上存取資料集非常簡單</span>
<span class="c1"># 現在先關注下面的 print 結果</span>
<span class="k">for</span> <span class="n">b_inp</span><span class="p">,</span> <span class="n">b_tar</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">index_word</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"起始句子的 batch："</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">b_inp</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"目標句子的 batch："</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">b_tar</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">"第一個起始句子的索引序列："</span><span class="p">)</span>
    <span class="n">first_i</span> <span class="o">=</span> <span class="n">b_inp</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">first_i</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"第一個目標句子的索引序列："</span><span class="p">)</span>
    <span class="n">first_t</span> <span class="o">=</span> <span class="n">b_tar</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">first_t</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">"第一個起始句子的文本序列："</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">first_i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"第一個目標句子的文本序列："</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">first_t</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>起始句子的 batch：
tf.Tensor(
[[  19   31   84 ...  111    2    3]
 [1454  120    2 ...   10   12    7]
 [ 313  516  136 ...   41   31   17]
 ...
 [   9    5  396 ...    3   24   18]
 [1063  508  547 ...    3    3  153]
 [  23   16  107 ...  641    1   14]], shape=(128, 10), dtype=int32) 

目標句子的 batch：
tf.Tensor(
[[  31   84 1626 ...    2    3    3]
 [ 120    2  114 ...   12    7   94]
 [ 516  136  937 ...   31   17    1]
 ...
 [   5  396   83 ...   24   18  301]
 [ 508  547  146 ...    3  153   62]
 [  16  107  260 ...    1   14   53]], shape=(128, 10), dtype=int32) 

-------------------- 

第一個起始句子的索引序列：
[  19   31   84 1626   18  238  125  111    2    3] 

第一個目標句子的索引序列：
[  31   84 1626   18  238  125  111    2    3    3] 

-------------------- 

第一個起始句子的文本序列：
['來', '到', '兩', '扇', '大', '石', '門', '前', '。', '\n'] 

第一個目標句子的文本序列：
['到', '兩', '扇', '大', '石', '門', '前', '。', '\n', '\n']
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了讓你理解資料集回傳的內容，上面用了不少 <code>print</code>。但事實上資料集 <code>ds</code> 負責的就是每次吐出 128 筆數據的 Tensor，而每筆數據裡頭包含了長度為 10 的輸入・輸出數字序列（因此 batch Tensor 的維度為 <code>(128, 10)</code>）。</p>
<p>去除 <code>print</code>，你要從資料集 <code>ds</code> 取得一個 batch 非常地簡單：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">b_inp</span><span class="p">,</span> <span class="n">b_tar</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># 蒙多想去哪就去哪</span>
    <span class="c1"># 想怎麼存取 b_iup, b_tar 都可以</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"b_inp 是個 Tensor：</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">b_inp</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">b_inp.numpy() 則是 ndarray:</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">b_inp</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>b_inp 是個 Tensor：

tf.Tensor(
[[  19    1   21 ... 1518   18  822]
 [ 334  182  184 ...   82  370    1]
 [   8   21   31 ...   47  390  538]
 ...
 [ 751  427    6 ...  589   26   58]
 [  62   10   12 ...    9   55   40]
 [1191    1   17 ...   11    2   16]], shape=(128, 10), dtype=int32)

b_inp.numpy() 則是 ndarray:

[[  19    1   21 ... 1518   18  822]
 [ 334  182  184 ...   82  370    1]
 [   8   21   31 ...   47  390  538]
 ...
 [ 751  427    6 ...  589   26   58]
 [  62   10   12 ...    9   55   40]
 [1191    1   17 ...   11    2   16]]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-定義能解決問題的函式集">4. 定義能解決問題的函式集<a class="anchor-link" href="#4.-定義能解決問題的函式集">&para;</a></h3><p>呼！我們花了不少時間在建構資料集，是時候捲起袖子將這些資料丟入模型了！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/How_to_Roll_Up_Sleeves_01.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>回想資料集內容，你現在應該已經很清楚我們想要模型解決的問題是什麼了：丟入一個數字序列，模型要能產生包含下個時間點的數字序列，最好是跟當初的<strong>輸出</strong>序列一模一樣！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如同我們在 <a href="https://demo.leemeng.tw/">AI 如何找出你的喵</a>裡頭說過的：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        任何類型的神經網路本質上都是一個映射函數。它們會在內部進行一連串特定的數據轉換步驟，想辦法將給定的輸入數據轉換成指定的輸出形式。 
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們現在要做的就是定義一個神經網路架構，讓這個神經網路（或稱函式）幫我們把輸入的數字序列轉換成對應的輸出序列。</p>
<p>我們期待這個模型具有「記憶」，能考慮以前看過的所有歷史資訊，進而產生最有可能的下個中文字。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/nlp-kaggle-intro/rnn-animate.gif"/>
</center>
<center>
                        循環神經網路非常適合處理具有順序關係的數據
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而在<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">自然語言處理與深度學習入門指南</a>我們就已經談過，循環神經網路中的 LSTM 模型非常適合拿來做這件事情。</p>
<p>因此雖然理論上你可以用任意架構的神經網路（如基本的前饋神經網路）來解決這個問題，使用 LSTM（或 GRU，甚至是 1D CNN）是一個相對安全的起手式。</p>
<p>在 TensorFlow 裡頭，使用 <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras">Keras API</a> 建立一個神經網路就像在疊疊樂，是一件很輕鬆的事情：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 超參數</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">RNN_UNITS</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="c1"># 使用 keras 建立一個非常簡單的 LSTM 模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># 詞嵌入層</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">num_words</span><span class="p">,</span> 
        <span class="n">output_dim</span><span class="o">=</span><span class="n">EMBEDDING_DIM</span><span class="p">,</span>
        <span class="n">batch_input_shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="p">))</span>

<span class="c1"># LSTM 層</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="n">RNN_UNITS</span><span class="p">,</span> 
    <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">recurrent_initializer</span><span class="o">=</span><span class="s1">'glorot_uniform'</span>
<span class="p">))</span>

<span class="c1"># 全連接層</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">num_words</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/model_summary.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這邊我們建立了一個由<a href="https://www.tensorflow.org/alpha/tutorials/sequences/word_embeddings">詞嵌入層</a>、LSTM 層以及全連接層組成的簡單 LSTM 模型。此模型一次吃 128 筆長度任意的數字序列，在內部做些轉換，再吐出 128 筆同樣長度，4330 維的 Tensor。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/yifeng-lu-1230629-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果你還記得，4330 是天龍八部裡頭所有出現過的中文字的數目。</p>
<p><div id="supervised" style="display: inline">因此</div>事實上我們已經把本來看似沒有正解的生成問題轉變成一個<a href="https://zh.wikipedia.org/wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92">監督式</a>，且有 4330 分類的<a href="https://en.wikipedia.org/wiki/Statistical_classification">分類問題</a>了。模型輸出的每個維度值都跟某個中文字的出現機率成正比。</p>
<p>值得一提的是，儘管這個神經網路（或稱映射函數）看起來非常有希望能解決我們的序列生成問題，我們並不僅僅是建立了 1 個映射函數而已。事實上，我們用 <code>tf.keras</code> 定義了一個有接近 1,300 萬參數的函式<strong>集合</strong>（Function set）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/antoine-dautry-428776-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這跟你看到一個資料集裡頭的特徵 <code>x</code> 跟目標值 <code>y</code> 成直線，然後想用 <code>a * x + b = y</code> 形式去 fit <code>y</code> 的道理是一樣的。</p>
<p>你相信 <code>a * x + b = y</code> 形式的映射函數能幫你把輸入 <code>x</code> 有效地對應到目標 <code>y</code>，你只是不知道最佳的參數組合 <code>(a, b)</code> 是多少罷了。</p>
<p>同理，很多研究結果顯示 LSTM 模型能很好地處理序列數據，我們只是還不知道最適合生成天龍八部文章的參數組合是什麼而已。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/nlp-kaggle-intro/backpropagation-example.gif"/>
</center>
<center>
                        深度學習中我們常使用梯度下降與反向傳播來從函數集合中找出最好的函數（某個特定參數組合的神經網路架構）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>參數 <code>a</code> 以及 <code>b</code> 有無限多種組合，而每一組 <code>a</code> 與 <code>b</code> 的組合都對應到一個實際的<strong>函數</strong>。每個函數都能幫你把 <code>x</code> 乘上 <code>a</code> 倍再加上 <code>b</code> 去 fit 目標值 <code>y</code>，只是每個函數的表現不一而已。而把所有可能的函數放在一起，就是所謂的函數集合。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/jeremy-thomas-99326-unsplash.jpg"/>
</center>
<center>
                        本文的 LSTM 模型架構因為參數組合無窮無盡，本身就像是一個巨大的函數空間。而我們得從裡頭找出能解決問題的特定函數（參數組合）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>針對 <code>a * x + b = y</code> 這個簡單例子，我們可以直接用線性代數從整個函式集合裡頭瞬間找出最佳的函數 <code>f</code>（即最佳的 <code>(a, b)</code>）。</p>
<p>不過在深度學習領域裡頭，我們需要透過<a href="https://zh.wikipedia.org/zh-tw/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">梯度下降（Gradient Descent）</a>以及<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">反向傳播算法（Backpropagation）</a>來幫我們在浩瀚無垠的函式集合（如本文中的 LSTM 網路架構）裡頭找出一個好的神經網路（某個 1,300 萬個參數的組合）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/deep-learning-framework.png"/>
</center>
<center>
                        深度學習框架
                        （<a href="https://agi.io/2018/02/09/survey-machine-learning-frameworks/" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>幸好我們後面會看到，像是 <a href="https://www.tensorflow.org/">TensorFlow</a>、<a href="https://pytorch.org/">Pytorch</a> 等深度學習框架幫我們把這件事情變得簡單多了。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.-定義評量函式好壞的指標">5. 定義評量函式好壞的指標<a class="anchor-link" href="#5.-定義評量函式好壞的指標">&para;</a></h3><p>有了<a href="#建立能丟入模型的資料集">資料集</a>以及 <a href="#定義能解決問題的函式集">LSTM 模型架構</a>以後，我們得定義一個<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%B1%BA%E5%AE%9A%E5%A6%82%E4%BD%95%E8%A1%A1%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E7%8F%BE">損失函數（Loss Function）</a>。</p>
<p>在監督式學習裡頭，一個損失函數評估某個模型產生出來的預測結果 <code>y_pred</code> 跟正確解答 <code>y</code> 之間的差距。一個好的函式／模型，要能最小化損失函數。</p>
<p>有了損失函數以後，我們能讓模型計算目前預測結果與正解的差異（Loss），據此不斷地改善參數以降低這個差異。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/robot_thinking.jpg"/>
</center>
<center>
                        機器學習模型或 AI 不會幫我們定義損失函數，因為只有我們能決定什麼是對的，什麼是錯的（至少在 2019 年是這樣）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>依照不同情境、不同機器學習任務你會需要定義不同的損失函數。</p>
<p>如同<a href="#supervised">前述</a>，其實我們要 LSTM 模型做的是一個分類問題（Classification Problem）：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        給定之前看過的文字序列以及當下時間點的新輸入字，預測下一個出現的字。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>因此本文的問題可以被視為一個有 4330 個分類（字）的問題。而要定義分類問題的損失相對簡單，使用 <a href="https://keras.io/zh/losses/#sparse_categorical_crossentropy">sparse_categorical_crossentropy</a> 是個不錯的選擇：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 超參數，決定模型一次要更新的步伐有多大</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># 定義模型預測結果跟正確解答之間的差異</span>
<span class="c1"># 因為全連接層沒有使用 activation function</span>
<span class="c1"># from_logits= True </span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span>\
    <span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 編譯模型，使用 Adam Optimizer 來最小化</span>
<span class="c1"># 剛剛定義的損失函數</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">),</span> 
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>model.compile</code> 讓我們告訴模型在訓練的時候該使用什麼<a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers">優化器（optimizers）</a>來最小化剛剛定義的<a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses">損失函數</a>。</p>
<p>完成這個步驟以後，就只剩實際訓練並使用模型了！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="6.-訓練並選擇出最好的函式">6. 訓練並選擇出最好的函式<a class="anchor-link" href="#6.-訓練並選擇出最好的函式">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在完成前 5 個步驟以後，訓練一個 Keras 模型本身是一件非常簡單的事情，只需要呼叫 <code>model.fit</code>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 決定看幾篇天龍八部文本</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">ds</span><span class="p">,</span> <span class="c1"># 前面使用 tf.data 建構的資料集</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/fit-logging.jpg"/>
</center>
<center>
                        Keras 模型在訓練時就會不斷吐出結果供你參考
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>但很多時候你需要跑很多次 <code>fit</code>。</p>
<p>一般來說，你事先並不知道要訓練多少個 epochs 模型才會收斂，當然也不知道怎麼樣的超參數會表現最好。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/diz-play-31367-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>大多時候，你會想要不斷地驗證腦中的點子、調整超參數、訓練新模型，並再次依照實驗結果嘗試新點子。</p>
<p>這時候 TensorFlow 的視覺化工具 <a href="https://www.tensorflow.org/tensorboard">TensorBoard</a> 就是你最好的朋友之一：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/tensorboard.jpg"/>
</center>
<center>
                        利用 TensorBoard 記錄下實驗結果，方便記錄自己做了什麼實驗，什麼 work 什麼不 work
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>TensorFlow 2.0 新增了 <a href="https://github.com/tensorflow/tensorboard/blob/master/docs/r2/tensorboard_in_notebooks.ipynb">JupyterNotebook 的 Extension</a>，讓你可以直接在筆記本或是 <a href="https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/r2/tensorboard_in_notebooks.ipynb">Google Colab</a> 上邊訓練模型邊查看結果。</p>
<p>跟以往使用 TensorBoard 一樣，你需要為 Keras 模型增加一個 <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TensorBoard">TensorBoard Callback</a>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span>\
        <span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="s2">"logs"</span><span class="p">),</span>
    <span class="c1"># 你可以加入其他 callbacks 如</span>
    <span class="c1"># ModelCheckpoint,</span>
    <span class="c1"># EarlyStopping</span>
<span class="p">]</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">ds</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> 
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>接著在訓練開始之後（之前也行）載入 Extension 並執行 TensorBoard 即可：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> tensorboard.notebook
<span class="o">%</span><span class="k">tensorboard</span> --logdir logs
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/tensorboard-demo2.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>除了確保模型有一直努力在降低損失函數以外，我們也可以觀察模型在訓練過程中生成的文章內容。比方說給定一個句子：</p>
<div class="highlight"><pre><span></span>喬峯指著深谷，
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>模型在完全沒有訓練的情況下生成的結果為：</p>
<div class="highlight"><pre><span></span>喬峯指著深谷，鑠淆孤癸抑私磚簧麥笠簸殯膽稼匿聲罪殖省膻臆啟殖
》斥酒燥弄咪薔鬃衝矚理蝗驗吞柢舌滴漂撿毛等櫈磁槃鞭爛辣諱輝母犢楊拜攜戛婉額虐延久鋒幟懸質迸飭南軌忸瑩娘檔麵獎逍菌包怖續敗倨凍赭彈暖顴衽劑街榻裝貨啕畿驛吳
</pre></div>
<p>這模型並沒有中邪。只不過模型中的 1,300 萬個參數完全隨機，你可不能期待有什麼奇蹟發生。</p>
<p>而在模型看了 20 遍天龍八部以後產生的結果：</p>
<div class="highlight"><pre><span></span>喬峯指著深谷，說道：「我不知道，不是你的好人，你就是你的好。」木婉清道：「他&hellip;&hellip;你&hellip;&hellip;我&hellip;&hellip;我&hellip;&hellip;師父是誰？」

段正淳道：「王姑娘，你還是不是？」段譽道：「你說過的話，他&hellip;&hellip;我&hellip;&hellip;你&hellip;&hellip;你&hellip;&hellip;」

那女郎道：「嗯
</pre></div>
<p>結果還是不理想，「你我他」後面只會加一大堆點點點。但其實也已經有不少值得注意的地方：</p>
<ul>
<li>模型已經知道怎麼產生正確的人名</li>
<li>知道 <code>道</code> 後面要接冒號以及上括號</li>
<li>知道前有上括號時後面應該要有下括號</li>
<li>知道要適時加入換行</li>
</ul>
<p>這其實已經是不小的成就了！</p>
<p>而在看過 100 遍天龍八部以後產生的結果：</p>
<div class="highlight"><pre><span></span>喬峯指著深谷，往前走去。

段譽見到這等慘狀，心下大驚，當即伸手去撫摸她的頭髮，心想：「我想叫你滾出去！」一面說，一面擤了些鼻涕拋下。

那大漢掙扎著要站起身來，只見一條大漢身披獸皮，眼前青光閃閃，雙手亂舞
</pre></div>
<p><code>擤了些鼻涕拋下</code> 很不段譽，但我還是笑了。</p>
<p>文章本身順暢很多，而且內容也豐富不少。另外用字也挺天龍八部的。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/lstm-text-generation/theodor-lundqvist-438530-unsplash.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你應該也已經注意到句子之間沒有太大的故事關聯性。而這邊帶出一個很重要的概念：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這個語言模型學會的只是天龍八部裡頭用字的統計關係，而不是金庸的世界觀。因此永遠不要期待模型每次都能產生什麼有意義的結果。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>儘管還不完美，到此為止我們手上已經有訓練過的模型了。讓我們拿它來產生新的文本了吧！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="7.-將函式-/-模型拿來做預測">7. 將函式 / 模型拿來做預測<a class="anchor-link" href="#7.-將函式-/-模型拿來做預測">&para;</a></h3><p>大部分你在深度學習專案裡頭訓練出來的模型可以直接拿來做預測。</p>
<p>不過因為循環神經網路傳遞狀態的方式，一旦建好模型，<code>BATCH_SIZE</code> 就不能做變動了。但在實際生成文章時，我們需要讓 <code>BATCH_SIZE</code> 等於 1。</p>
<p>因此在這邊我們會重新建立一個一模一樣的 LSTM 模型架構，將其 <code>BATCH_SIZE</code> 設為 1 後讀取之前訓練時儲存的參數權重：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 跟訓練時一樣的超參數，</span>
<span class="c1"># 只差在 BATCH_SIZE 為 1</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">RNN_UNITS</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># 專門用來做生成的模型</span>
<span class="n">infer_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># 詞嵌入層</span>
<span class="n">infer_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">num_words</span><span class="p">,</span> 
        <span class="n">output_dim</span><span class="o">=</span><span class="n">EMBEDDING_DIM</span><span class="p">,</span>
        <span class="n">batch_input_shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="p">))</span>

<span class="c1"># LSTM 層</span>
<span class="n">infer_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="n">RNN_UNITS</span><span class="p">,</span> 
    <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span>
<span class="p">))</span>

<span class="c1"># 全連接層</span>
<span class="n">infer_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">num_words</span><span class="p">))</span>

<span class="c1"># 讀入之前訓練時儲存下來的權重</span>
<span class="n">infer_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">)</span>
<span class="n">infer_model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>除了<a href="https://www.tensorflow.org/alpha/tutorials/sequences/text_generation#restore_the_latest_checkpoint">讀取權重</a>，這段程式碼對你來說應該已經十分眼熟。有了 <code>infer_model</code> 以後，接著我們要做的就是：</p>
<ul>
<li>將起始文本丟入模型</li>
<li>抽樣得到新的中文字</li>
<li>將新得到的字再丟入模型</li>
<li>重複上述步驟</li>
</ul>
<p>而實際預測的流程大概就長這個樣子：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/sampling.png"/>
</center>
<center>
                        重複抽樣取得新的中文字
                        （<a href="https://www.tensorflow.org/alpha/tutorials/sequences/text_generation#the_prediction_loop" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如同我們在<a href="Demo：生成全新的天龍八部橋段">開頭的 demo</a> 所看到的，依照你設定的<strong>生成長度</strong>，我們需要重複上述步驟數次。</p>
<p>而要執行一次的抽樣也並沒有非常困難：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 代表「喬」的索引</span>
<span class="n">seed_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">234</span><span class="p">]</span> 

<span class="c1"># 增加 batch 維度丟入模型取得預測結果後</span>
<span class="c1"># 再度降維，拿掉 batch 維度</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
    <span class="n">seed_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">infer_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># 利用生成溫度影響抽樣結果</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">/</span> <span class="n">temperature</span>

<span class="c1"># 從 4330 個分類值中做抽樣</span>
<span class="c1"># 取得這個時間點模型生成的中文字</span>
<span class="n">sampled_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>抽樣的程式碼為了方便解說有稍作刪減，如果你要實際動手跑看看，請參考官方的 <a href="https://www.tensorflow.org/alpha/tutorials/sequences/text_generation">Text generation with an RNN</a>。</p>
<p>這邊我想要你看到的重點是如何利用<strong>生成溫度</strong> <code>temperature</code> 的概念來影響最後的抽樣結果。</p>
<p>如同 demo 時說明的：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        生成溫度是一個實數值，而當溫度越高，模型產生出來的結果越隨機、越不可預測
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>模型的輸出為一個 4330 維度的 Tensor，而其中的每一維都對應到一個中文字。維度值越大即代表該字被選到的機會越大。</p>
<p>而當我們把整個分佈 <code>predictions</code>除以一個固定值 <code>temperature</code> 時，越大的值被縮減的程度越大，進而讓各維度之間的絕對差異變小，使得原來容易被選到的字被抽到的機會變小，少出現的字被選到的機會稍微提升。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/lstm-text-generation/temperature_dist.jpg"/>
</center>
<center>
                        溫度越高，分佈會變得越平滑，罕見字被選到的機會上升，生成結果越隨機
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這就是為何我們會想手動調整生成溫度的原因。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="如何使用-TensorFlow.js-跑模型並生成文章_1">如何使用 TensorFlow.js 跑模型並生成文章<a class="anchor-link" href="#如何使用-TensorFlow.js-跑模型並生成文章">&para;</a></h2><p>到此為止，你應該已經了解如何使用 Python 以及 TensorFlow 2.0 來架構出一個能產生天龍八部文本的 LSTM 模型了。</p>
<p>一般來說，只要你把剛剛生成文本的 Keras 模型儲存下來，接著就可以在任何機器或雲端平台（如 GCP、AWS）上進行生成：</p>
<div class="highlight"><pre><span></span><span class="n">infer_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"model.h5"</span><span class="p">)</span>
</pre></div>
<p>最近適逢 <a href="https://js.tensorflow.org/">TensorFlow.js</a> 推出 <a href="https://github.com/tensorflow/tfjs/releases/tag/v1.0.0">1.0.0 版本</a>，我決定嘗試使用 <a href="https://github.com/tensorflow/tfjs-converter">tfjs-converter</a> 將 Keras 模型轉換成 TensorFlow.js 能夠運行的格式：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span>tensorflowjs_converter <span class="se">\</span>
    --input_format<span class="o">=</span>keras <span class="se">\</span>
    model.h5 <span class="se">\</span>
    tfjs_model_folder
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>轉換完成後會得到 tfjs 的模型，接著只要把它放到伺服器或是 Github 上就能在任何靜態網頁上載入模型：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="nx">model</span> <span class="o">=</span> <span class="nx">tf</span><span class="p">.</span><span class="nx">loadLayersModel</span><span class="p">(</span><span class="s2">"your_tfjs_model_url"</span><span class="p">);</span>
<span class="kr">const</span> <span class="nx">output</span> <span class="o">=</span> <span class="nx">model</span><span class="p">.</span><span class="nx">predict</span><span class="p">(</span><span class="nx">input</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們在<a href="https://leemeng.tw/deep-learning-resources.html#tensorflow.js">由淺入深的深度學習資源整理</a>就曾介紹過 <a href="https://js.tensorflow.org/">TensorFlow.js</a>，他們有很多有趣的 <a href="https://www.tensorflow.org/js/demos/">Demos</a>，有興趣想要在瀏覽器上實作 AI 應用的你可以去了解一下。</p>
<p>使用 TensorFlow.js 好處在於：</p>
<ul>
<li>隱私有保障。使用者上傳、輸入的內容不會傳到伺服器上</li>
<li>開發者不需租借伺服器或是建置 API 端點，無部署成本</li>
</ul>
<p>當你能向本文一樣把模型讀入瀏覽器以後，只要使用 <a href="https://js.tensorflow.org/api/latest/">TensorFlow.js API</a> 將我們剛剛在前面介紹的 Python 邏輯實現即可。</p>
<p>熟悉 JavaScript 的你甚至還可以<a href="https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation">直接在瀏覽器訓練類似本文的 LSTM 模型並生成文章</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="結語">結語<a class="anchor-link" href="#結語">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>人跟機器的差別：我們理解世界觀，機器理解統計關係
我看十遍也寫不出類似的文章，機器看十遍無法理解天龍八部裡頭眾生的愛恨情仇。</p>
<ul>
<li>要如何在深度學習專案有所進展<ul>
<li>idea -&gt; 實驗：Keras</li>
<li>實驗　-&gt; 結果：GPU</li>
<li>結果  -&gt; 點子：TensorBoard</li>
</ul>
</li>
<li>深度學習專案 7 步驟<ul>
<li>定義問題與機器學習任務</li>
<li>搜集並清理資料</li>
<li>建立能放入模型的資料集</li>
<li>定義能解決問題的函式集</li>
<li>定義何謂好何謂壞的 metric</li>
<li>訓練並選出最好的模型</li>
<li>使用模型做預測</li>
</ul>
</li>
<li>了解 TensorFlow 2.0 新功能<ul>
<li>tf.data</li>
<li>tf.keras</li>
<li>tensorboard in notebook</li>
</ul>
</li>
<li>了解如何整合 TensorFlow 以及 TensorFlow.js</li>
</ul>
<p>向金庸致敬</p>
<p>我要繼續閱讀天龍八部了</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<script src="https://leemeng.tw/tfjs-apps/lstm-text-generation/dist/lstm-text-generation.03657dc5.js"></script>
</div>
</div>
</div>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">TensorFlow</a>
                        <a href="https://leemeng.tw/tag/tensorflowjs.html" rel="tag">TensorFlow.js</a>
                        <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                    </span>

                </p>











































































                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary " style="color: #FFFFFF">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
                </div>

                <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>                </ul>
            </div>
        </div>

        <div class="row footer-bottom">
            <div class="col-twelve">
                <div class="copyright">
                    <span>Powered by <a href="http://getpelican.com/" target="_blank">Pelican</a></span>
                    <span>© Copyright Hola 2017</span>
                    <span>Design by <a href="https://www.styleshout.com/" target="_blank">styleshout</a></span>
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>
        </div> <!-- end footer-bottom -->
    </footer> <!-- end footer -->


    <div id="preloader">
        <div id="loader"></div>
    </div>

        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTovNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}
</script>


</body>
</html>