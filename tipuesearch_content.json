{"pages":[{"title":"資料科學文摘 Vol.6 人類壽命大進展、GAN、數據工廠以及產品分析","text":"文摘來到第 6 篇，不知道這是你看的第幾篇呢？ 這週我們一樣保持閱讀的「營養均衡」，從全球平均壽命變化的資料視覺化、深度學習最夯的「對抗生成網路」話題、產品分析框架到理解何謂「數據工廠」，我希望能讓閱讀本文摘的你，廣泛地了解各領域跟「資料」相關的議題，並進一步找出自己的興趣，加以深度探索。 本週閱讀清單： Twice as long – life expectancy around the world Interview with Deep Learning Researcher and The GANfather: Dr. Ian Goodfellow Data Factories Engagement Drives Stickiness Drives Retention Drives Growth 讓我們開始閱讀吧！ Twice as long – life expectancy around the world 現在全球健康以及公衛還是存在很多不平等，但別忘了我們已經取得巨大進展。 如同我們上週在 如何用 30 秒了解台灣發展與全球趨勢：用 GapMinder 培養正確世界觀 一文中聊到，好的資料視覺化可以幫助我們快速地了解世界。這週牛津大學的經濟學家 Max Roser 用 3 張橫跨 2 世紀的世界地圖，來告訴我們全球平均壽命（Life Expectancy）的變化： 我們可以看到這 200 年來，生活在世界上的人們經歷了 3 個階段： 在 1800 年以前所有人的平均壽命 < 40 歲，大部分兒童早夭 在 1950 年，部分地區健康大幅改善，歐美及日本的平均壽命為 60 歲，為非洲整體平均的 2 倍，鴻溝顯而易見 在 2015 年，幾乎全球所有地區都能活到 60 歲以上，鴻溝逐漸縮小 我們都希望自己親人及朋友活得長久。就是因為這樣，你更應該感激這 200 年人類取得的進步。 近 2 世紀人類在健康狀況改善的卓越成就，套句 Max Roser 的說法就是： 在人類歷史上，這是我們第一次改善了整個人群的健康狀況。在人類健康狀況停滯千年後，封印終於解除。 ─ Max Roser 值得一提的是，在 1950 年，台灣的平均壽命為 55.5 歲，經過了 65 年，來到了 80 歲。平均每 3 年，台灣人的平均壽命增加 1 歲，成長速度不可小覷。 你也可以用 Our World in Data 提供的圖表來看看全球變化： Interview with Deep Learning Researcher and The GANfather: Dr. Ian Goodfellow 伊恩．古德費洛（Ian Goodfellow）是 Google Brain 的研究科學家 ，最知名的成就是在 2014 年推出 生成對抗網路（Generative Adversarial Network, 簡稱 GAN） 。GAN 最基本的概念是讓兩個神經網路互相對抗，讓模型可以依靠較少的人類介入以及訓練資料，自己學會高度複雜的工作。自從那之後，GAN 領域的研究一日千里，現在 arXiv 上該論文有超過 5,000 次引用 。 GAN 有很多用途，像是自動產生高畫質圖片 （圖片來源： NIPS 2016 Tutorial ） GAN 有非常多「用途」，像是自動產生圖片、創作音樂、寫詩或是製造假新聞。但在這篇文摘裡頭，讓我們先專注於這篇訪問伊恩的內容。 在這篇訪談裡頭，伊恩給想開始研究 ML 的人一些建議： 徹底學好基礎。像是寫程式、除錯、並學習機率及線性代數。很多時候在研究 ML 的時候，幫助你最多的是扎實的基礎，而不是非常前衛的想法（這是他從 Google Brain 創立者 吳恩達 得到的建議） 沒有什麼運算資源時，要選對研究主題。（沒有像是 Google 那樣等級的運算資源的話，就不要想去實現全世界最準的 ImageNet 分類器） 一開始找個人家已經做過的題目來磨練你的 ML 能力。 最後一點需要額外解釋一下。 如果你在練習 ML 的時候，選擇跟隨前人「已經成功」的東西來實作的話，這樣就算自己實作出來的模型表現不好，你也知道只是你的實作、基本功出了問題，而不是這個點子錯了。接著只要回去複習基本概念、加強實作功力即可。 但如果你的 ML 的實作能力沒到一個水平，然後又馬上想要嘗試一個天馬行空的點子／演算法，最後實作出來失敗，你很難知道，到底是點子本身有瑕痴，還是因為你實作能力差而出問題。 另外如果你現在就想開始了解 GAN 的話，可以試試 GAN lab ，在網頁上玩玩生成對抗網路。 GAN lab 讓你可以利用網頁瀏覽器直接探索 GAN 並了解其運作原理 （ 圖片來源 ） Data Factories 身處數據時代，我們應該更關心自己的資料被怎麼利用。 這篇文章想說的是，其實 Facebook、Google 以及其他廣告業者都是所謂的「數據工廠」，而如果政府要立法規範這些工廠，最有效的方法就是請它們允許使用者看到工廠裡頭的情況。 我認為「數據工廠」是對 Google 及 Facebook 這種利用數據來創造價值的公司的一個貼切比喻。因為他們除了使用者的行為數據，也從廣告代理商以及第三方數據收集業者取得大量資料。透過將這些原始資料「加工」並產生衍生價值，據此創造巨大收益。 然而這些「數據工廠」跟一般傳統的「工廠」有一個非常大的差異：誰都無法窺探該「工廠」的內部情況。 記者可以去 Nike 製造足球的工廠裡頭拍拍照，讓世人知道這些工廠內部的運作情況，但在這年代，你無法去 Facebook 裡頭拍拍照，了解他們是怎麼利用各式各樣的演算法，來「活用」所有跟你相關的資料（你按過讚的內容、瀏覽過的網頁，甚至是你為了雙重認證而輸入的電話號碼）。 因此立法者以及那些關心自己數據可能被濫用的使用者要了解的是，要規範 Facebook 這種公司，不能只要求 Facebook 公布他們從使用者手上拿到的原始資料（Raw Data），而是應該公布那些他們利用演算法以及結合多種數據來源所產生出的 user profile，讓使用者自行判斷要不要繼續讓該公司使用自己的 profile。 雖然多數人其實只在乎 Facebook 能不能秀給他們更多的動物影片以及朋友動態，不太在意自己的數據被怎麼拿來獲利。 Engagement Drives Stickiness Drives Retention Drives Growth 在以提供 App 作為服務的公司裡頭，資料科學家大都會需要進行產品分析（Product Analysis）進而改善自家產品。 這篇文章介紹了 App 產業以及我常在使用的一個分析框架，讓你可以感受一下，實際上 DS 在做產品分析的時候，要看些什麼東西。 有做過產品分析的你，應該能很快地理解這個流程圖： 這張圖最重要的核心概念是： 當使用者發現你產品的價值以後，他們會主動回來。 當使用者發現你的產品的價值後，就會進一步參與使用（Engage），而好的參與程度（Engagement Level）會增加他們對此產品的黏著度（Stickiness），進一步讓他們願意回來繼續使用你的產品（Retentaion）。而有了越來越多的忠實用戶，就能進一步帶給你的產品成長（Growth），不斷持續地這個好的循環。 在我們理解每個階段代表的意義以後，我們還需要一些指標（indicators）來實際幫助我們了解產品在每個階段的表現。 像是 Engagement 底下的 TS/DAU 即分別代表「使用時間（ T ime S pent）」以及「每天活躍使用者人數（ D aily A ctive U sers）」。這兩個都很常被拿來衡量使用者參與一個產品的程度。有了好的參與程度，一個使用者就更有可能在安裝 7 天後還回來繼續使用（Retention 階段的 D7）。 這邊沒有篇幅一個個介紹圖中的指標，但要注意的是，在看指標的時候，要去想它是早期指標（Early Indicators）還是延遲指標（Lagging Indicators）。 比方說你的最終目標是提升每月活躍使用者人數（ M onthly A ctive U sers，最右邊 Growth 階段的 MAU）這個延遲指標（延遲在於要過了 1 個月你才知道結果），那你除了看 MAU 以外，還需要去看 TS/DAU 等早期指標。因為 MAU 需要一個月的時間才能計算出來，有時候產品表現差，你從每天使用的人數下降就可以略知一二，可以馬上做調整而不需等到一個月後 MAU 數字難看才大傷腦筋。 及早發現，及早治療。 產品分析領域在網路上的資源不多，有機會再跟你分享我的心得。 結語 呼！這就是本週文摘的內容啦！希望你閱讀後有感覺自己腦中多了點東西，變得聰明了一點。 社會人口、機器學習、產品分析以及數據隱私的議題，你會發現這些文章儘管領域大相徑庭，他們都與「數據」脫離不了關係。 在這個時代，任何人的日常生活中都充斥著大量數據。我們需要重新思考、檢視並理解身邊的數據，甚至活用它們來創造更好的世界。 這也是我寫這系列文章的原因，希望讓更多人（包含我自己）能更輕鬆地用數據理解這個世界。歡迎你點擊下面的訂閱按鈕，未來跟著我一起繼續探索這個世界：）","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-6.html"},{"title":"如何用 30 秒了解台灣發展與全球趨勢：用 GapMinder 培養正確世界觀","text":"再稍微花個 5 秒鐘咀嚼一下你所看到的。 現在問你自己，你看到了什麼？ 在這個資訊爆炸的時代，大腦為了保護你的心智不被大量數據淹沒，可能已經很習慣性地忽視眼前數據其背後所隱含的意義。 但讓我提醒你一下，就在剛剛的 30 秒內，全世界過去 200 年至今的經濟（所得收入）與社會（平均壽命）發展狀況活生生地重現在你眼前！ 這可不是小時候歷史老師會 / 能秀給你看的東西（至少我的老師沒有）我不知道你感受如何，但在我 第一次見識到此圖 的時候，內心可說是激動得不得了！ 瑞典全球公衛教授 漢斯・羅斯林 2006 年在 TED 利用上面的泡泡圖向觀眾們解說世界的經濟與社會發展 將專注拉回台灣。看著台灣的發展軌跡，你甚至還可以發現一些值得注意的現象： 1939 至 1945 年，人民所得以及平均壽命走倒車（二戰） 1945 至 1953 年國民平均壽命的大幅提升，所得回歸正常 1960 年代以後，經濟與社會的持續穩定發展 2000 年後成長趨緩 除了本身的發展軌跡以外，還可以發現到了 21 世紀，台灣在右上角，名列前茅。 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 泡泡圖除了可以讓我們觀察世界趨勢，也能同時了解台灣的發展軌跡以及與其他國家的相對位置 剛剛在看圖的時候，你應該還有很多其他發現且迫不及待地想要了解更多。 事實上，如果在看了剛剛的動畫以後，你突然渴望想要知道更多是很正常的。 重要的是不要停止問問題，好奇心有其存在的理由。 ─ 愛因斯坦 如果你現在想要更加地了解台灣或是其他不同的國家在各種社會 / 經濟 / 健康指標的發展（如所得收入、兒童死亡率、二氧化碳排放量等），我鼓勵你先上去改改 X 或 Y 軸、點選不同國家，查看結果以後再繼續往下讀。 畢竟文章跑不掉，你的好奇心則可能在幾秒鐘後消逝：） 你回來了嗎？ 你現在應該已經暸解，透過值得信賴的數據來源（比方說 聯合國 ）以及良好的呈現方式（文章開頭的泡泡圖），能讓你在很短時間內「正確」地掌握全世界趨勢以及台灣的發展狀況。 儘管媒體總是報憂不報喜，你會發現全世界大致上變得越來越好。 你也會發現以「人均收入」以及「平均壽命」的角度來看，台灣的表現在全世界也是數一數二，這點值得我們欣慰及驕傲。 《真確》是 2018 年由漢斯・羅斯林（Hans Rosling）所撰。（圖片來源： 迷誠品 ） 在《真確》這本書裡，羅斯林教授闡述如何利用數據以及正確心態來理解世界，是一本深具啟發性的著作。而「泡泡圖」則是他在傳達知識時，經常使用到的工具。 首先你需要知道，文中的泡泡圖（Bubble Chart）的開發以及各個國家的數據整理，並非由我獨自完成，而是由漢斯・羅斯林教授與他所創辦的 GapMinder 基金會 從多個國際組織（如聯合國、國際衛生組織、世界銀行等） 蒐集、整理 而來。（給他們點掌聲！） 事實上，你可以直接使用 官方的泡泡圖 ，或是像本文一樣，依照 這邊的教學 來將泡泡圖內嵌在你自己的網站裡頭。 我知道你在想什麼。 「既然官方都已經有泡泡圖了，為何你要在這裡再弄一個出來呢？」 非常好的問題，但讓我先賣個關子。 我會在 為何需要本文的泡泡圖？ 章節裡頭仔細說明。（提示：跟台灣有關係） 在這邊想先讓你知道的是，文章接下來會說明泡泡圖裡頭有什麼台灣數據可供你探索，以及提供一些探索台灣以及世界的例子，讓你在了解世界的同時熟悉泡泡圖的使用方式。 等你熟悉泡泡圖以後，可以利用它來更深入瞭解台灣與以及任何你有興趣的國家，並培養正確的世界觀。最重要的是，在有了正確思維以後，你能怎樣讓世界以及台灣變得更加美好。 前言很長，不過接下來才是重頭戲。準備好了就跟上我們的探索之旅吧！ 本文章節 為何需要本文的泡泡圖？ 有什麼台灣數據可供探索？ 用泡泡圖探索世界 媽媽不生寶寶了：生育率大幅下降 怎麼創造乾淨未來：煤炭消耗與環境污染 民主大躍進：我很自由，不過不想參與政治 看到數據背後的故事 你能怎樣讓世界更好？ 為何需要本文的泡泡圖？ 你可能在想，何必要大費周章地弄出自己的泡泡圖。畢竟，只要使用 GapMinder 基金會（以下簡稱 GapMinder） 官方釋出的泡泡圖 就可以透過數據來探索「全世界」與「台灣」了啊？ 這句話只對了前半段。 第一個沒有那麼嚴重但是有點令人困擾的問題是，目前官方的泡泡圖只有英文，沒有繁體中文。 雖然台灣人的英文能力普遍不差，但是要所有人在看到每個國家的英文名字後馬上反應出來，可不是一件簡單的事情。 你還有多少把握可以認出東帝汶或柬埔寨的英文名字？（提示：下圖有其中一個） 更不用說聯合國以及各個國際組織定義的各式各樣社會 / 經濟 / 公衛指標的英文了。（還記得結核病、旱災或是償債出口比怎麼唸嗎？） 我們看泡泡圖的主要目的是為了瞭解世界，而不是學習翻譯各種英文專業術語。 就算這樣講，英翻中或許問問 Google 還是勉強可以解決。但官方泡泡圖存在的第二個問題，則非常致命。 GapMinder 釋出的 泡泡圖 截圖。在右邊的清單搜尋「 Taiwan 」不會有結果 在我撰寫此文的這個時間點（2018 年 10 月），在 GapMinder 上的泡泡圖 裡頭，你並無法找到「 Taiwan 」的存在。 沒錯，你可以現在 去搜尋看看 。然後你會發現有 2,300 多萬人口的台灣並不存在 GapMinder 的泡泡圖之中。 依據 GapMinder 的說法 ，泡泡圖預設只顯示 聯合國會員國 。因此理所當然地，台灣不會被顯示在上面。 儘管沒有在聯合國裡頭，在台灣努力生活的人們確確實實地存在著（ 圖 為寧夏夜市） 2018 年 7 月，GapMinder 表示 他們正在想辦法讓非聯合國會員國（如台灣、香港）也能被加到泡泡圖裡頭 ，但自從那之後已經過了數個月。 我真的不怪他們，畢竟他們是非營利機構，人手有限且已經為世界做出很多貢獻了。 聯合國總部，紐約（圖片來源： Foreign Policy ） 只是，我無法忍受在閱讀完《真確》並想要開始認真地探索這個世界的時候，發現裡頭竟然沒有熟悉的台灣。 後來的故事你大概猜得到了。我開始研究 GapMinder 製作泡泡圖的程式碼 以及 數據儲存格式 。我寫些程式、閱讀聯合國以及各個國際組織的相關文獻以後，把台灣「駭」進泡泡圖的國家列表裡頭，並將裡頭所有國家以及（幾乎）所有指標翻譯成中文。你在文章開頭看到的泡泡圖就這樣誕生了。（感謝 Google 大神以及咖啡因！） 現在，在了解本文泡泡圖的典故之後，讓我們看一下目前的泡泡圖裡頭有哪些台灣數據可供你探索。 有什麼台灣數據可供探索？ GapMinder 將所有搜集來的資料整理在這個 Github Repo 裡頭，也是本文泡泡圖的數據來源。 理想上，每個指標（如二氧化碳排放量、國民平均壽命、人均收入）都會（或者說都要）包含每個國家及地區每年的資料才能方便我們做比較。但你可以想像，這不太可能實現。 實際上，依照不同國家的數據開放狀況、國際組織蒐集數據的方法差異，都有可能造成指標裡頭沒有某些國家的資料。 以台灣為例，透過分析 GapMinder 數據來源，我們可以知道，截至目前為止，泡泡圖裡頭總共有 500 多個指標，其中約有 40 % （ 200 個 ）指標含有台灣數據。 （跟本文的泡泡圖以及數據來源一樣，此圖的資訊也會定期更新） 以大分類來看的話，「健康」及「工作」分類有較多的資料可供我們檢視台灣的狀況並同時與其他國家做比較；相較之下，「社會」及「人口」涵蓋的台灣指標較少，公共建設分類則只有 1 個（交通死亡人數）。 健康分類中，屬男女的「癌症」相關數據最為完整：大腸癌、胃癌、肝癌、乳癌、攝護腺癌 .. 應有盡有。 工作分類則有各個年齡層的失業 / 就業率及「勞動參與率」等指標，你可以自行稍後在泡泡圖上查看。 不過實際上，你也不需記住哪些分類有多少台灣數據。 利用泡泡圖的選單，我們可以馬上知道每個分類底下有多少指標、有哪些指標有台灣數據、最早的年份為何 如果你剛剛有玩泡泡圖的話，可能會好奇在每個分類後面的數字代表什麼。圖中健康分類後面的 （63/166） 代表在泡泡圖中，健康分類底下總共有 166 個指標，而其中的 63 個有台灣數據。這跟我們上一張長條圖吻合。 現在看到上圖第三欄的「肺癌病例數」：指標名稱後面的 （1990 ~ 則代表在該指標中，台灣數據最早可以追溯到西元 1990 年。有了這些額外資訊，可以讓你更方便地探索台灣與世界的關係。 值得一提的是，以上的結果僅代表 GapMinder 目前有的數據。他們持續努力地在添加新的數據，而我也預計在未來導入更多的台灣數據。但現在，先讓我們從已有的指標裡頭選幾個來探索看看吧！ 用泡泡圖探索世界 在《真確》裡頭，漢斯・羅斯林教授已經向我們展示了很多很棒的泡泡圖範例。而在這個章節裡頭，我會列出一些自己利用泡泡圖探索世界以及了解台灣的例子。 媽媽不生寶寶了：生育率大幅下降 很多我們以為是常態的事物，事實上在幾十年前完全不存在。 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 以婦女人均嬰兒數為例，在 1950 年前，跟亞洲大多數國家相同，台灣每個婦女平均有 6 個嬰兒。現代大多數的年輕人應該無法想像這件事情。 但我們可以看到從 1960 年代開始，婦女人均嬰兒數以不可思議的速度溜滑梯下降，直到近年每位婦女平均只有一名嬰兒。 解釋歷史從來不簡單，但我們可以想像在當時，醫療技術以及節育概念還不高，間接造成較高的兒童死亡率。兒童的死亡率高，也就代表平均一位婦女需要生產更多嬰兒來延續後代。要證實這點，我們可以把 X 軸的「人均所得」換成「兒童死亡率」： 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 不只台灣，我們可以發現全世界有一樣的趨勢：兒童死亡率下降，而同時媽媽們也不需再生那麼多寶寶。這現象很大部分是因為醫療進步、女性教育的普及以及家庭觀念的改變。 另外從代表不同洲的顏色可以看到，在 2018 年，所有婦女人均嬰兒數 > 6 的國家都位在非洲。 當然，婦女人均嬰兒數減少，同時也代表 高齡化社會的來臨 。讓我們將 X 軸換成「60 歲以上人口佔總人口比例」以後，看看日本的發展： 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 日本的高齡化人口比例增加 ，也代表青壯年的負擔加重。不只日本，在未來要怎樣建立一個良好的長照制度，在台灣也是一個日漸重要的議題。 怎麼創造乾淨未來：煤炭消耗與環境污染 台灣能源供給高度仰賴進口，其進口量長期維持在 97 到 98 ％，而 煤炭又為台灣第二大主要進口能源 。 因為碳密度高，燃燒煤炭又會產生比其他化石燃料（石油、天然氣）來得更多的二氧化碳，造成更嚴重的氣候暖化以及環境破壞。讓我們看看從以前到現在，一個台灣人平均消耗的煤炭以及產生的二氧化碳的變化趨勢： 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 我們可以看到從 1990 年起，台灣煤炭的人均消耗量（用來發電）快速增加，而同時人均二氧化碳的排放量也逐年增高。儘管近年趨向穩定，我們可以看到作為對照組的美國在 2010 年以後的人均煤炭消耗量已經低於我們。 環境考量以及再生能源的成本下降，讓歐美各國的政府以及能源業者決定投向再生能源懷抱，但台灣似乎還想要 建立燃煤電廠 。 在 2014 年時，只有哈薩克跟澳大利亞的煤炭消耗量超越我們。而作為世界第一煤炭出口國，澳洲自己也因為大量開挖煤炭而導致大堡礁的生態浩劫。 怎麼減少煤炭消耗並維持人民生活水準（如提高再生能源利用率），是台灣的重要議題之一。 民主大躍進：我很自由，不過不想參與政治 對於現在的台灣人來說，「民主」是如吃飯喝水般的基本存在。 但台灣的「民主」一直都存在嗎？要回答這個問題，我們可以看看台灣的 民主指數（Democracy Index） 發展： 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 雖然上頭的數據只到 2011 年，但我想要你看的是，1990 年（也是我出生的那年）之後，比起所得提升速度，我們的民主指數的成長速度讓人驚訝，可以說是三級跳！ 基本上近年台灣的分數變動不大。而在最新的 2017 年全球民主指數 裡頭，台灣則獲得了 7.73 分，全球排名第 33 名。（第一名為挪威，美國 21，日本 23，中國則為 139 名） 民主指數滿分為 10 分，由 5 個評量標準做平均： 選舉過程及多元程度（獲 9.58 分） 政府功能（獲 8.21 分） 政治參與（獲 6.11 分） 政治文化（僅 5.63 分） 公民自由度（ 9.12 分） 可以看到雖然我們的公民自由度很高，但政治參與以及政治文化不足。 我個人認為跟長期無意義的藍綠對抗文化以及年輕一代普遍對政壇上的政治人物冷感有關。 要讓民主成功，我們必須參與，而非只是冷眼旁觀。沒有投票的人沒有權利抱怨。 ─ 路易．路蒙，美國小說家 儘管我們的民主程度已經值得讚賞，在公民參與部分還有很多地方可以改善。 看到數據背後的故事 在上一章節，我們看了一些利用泡泡圖探索台灣以及世界的例子。 相信你也有這種錯覺：搭配著大量數據，泡泡圖彷彿讓你站在上帝的視角綜觀全球。 但我們不能就這樣停止，自我膨脹地以為彷彿透過泡泡圖裡頭的數據，就已經暸解世間萬物。 正如《真確》裡頭漢斯・羅斯林教授跟我們說的： 我要你看到統計數據背後的個別故事，也要你看到個別故事背後的統計數據。不靠數據無法了解世界，但光靠數據也無法了解世界。 舉例來說，在我查看台灣婦女在工業（Industry Sector）的勞動比例時，發現一個有趣的現象： 您的瀏覽器不支援影片標籤，請留言通知我：Ｓ 工業一般給人的印象就是包含了很多需要體力的工作，因此看到右邊台灣婦女在工業的勞動比例逐年下降（與之相對，服務業勞動比例上升）完全符合我的期待。但是，看看那個 阿爾及利亞 ！ 光看那條節節上升的曲線無法幫助我們實際了解阿爾及利亞，如同我們無法光靠數據了解世界。 說來慚愧，在觀察到這現象前，儘管小時候從歷史老師的口中聽過它，我完全沒有研究過這個國家。 阿爾及利亞, 瓦赫蘭 （ 圖片來源 ） 透過一些閱讀，我現在了解阿爾及利亞（Algeria）是一個位於非洲北部的國家，1962 年從法國殖民統治下獲得獨立。因為 婦女解放 以及女權運動崛起地相對較其他伊斯蘭國家早，該國的女性在各個階級都很活躍。在勞動市場可以看到她們開大卡車、當加油站工人並穿寬大的工作服； 女性議員在議會佔的比例 在阿拉伯世界也是獨占鰲頭，最近甚至 還舉辦比基尼示威 ，來呼籲保守的社會給予女性更多自由。 在這邊不是要推薦你去阿爾及利亞觀光或是 Google 搜尋比基尼照片。 我想強調的是，讓你的好奇心跨越冷冰冰的數字。在透過數據有個宏觀的概念以後，針對你有興趣的問題去實際查查資料，問問人並了解背後的故事。 在你開始這麼做以後，會發現世界變得更遼闊，更多采多姿。 你能怎樣讓世界更好？ 看了那麼多的泡泡圖以及數據，實際上我們可以怎樣讓台灣以及世界變得更好呢？ 我相信每個人都有自己的想法，但這邊讓我給出一些拙見。 如果你是老師或是從事教育業，開始思考要怎麼利用數據來教導學生或是下一代正確的世界觀吧！不要再教他們背誦歷史年表或是生硬數字，而是利用容易理解的資料視覺化工具（如本文的泡泡圖）將過去、現在的世界展示給他們看，刺激他們的好奇心，讓他們自主發問、蒐集資料並想像未來。 如果你是從事經濟 / 社會 / 公衛 / 能源 / 政治 / 國際關係等專業領域的話，重新思考在這個世紀，我們應該要密切關注的人類發展指標吧！ 舉國民平均所得這個指標來說，我們在文章開頭看到近 200 年來全世界每個國家在國民平均所得皆有改善，但在 21 世紀只看這個就夠了嗎？ 21 世紀，我們面臨的新問題是貧富差距。 從「平均所得」這單一數字來看一個國家的經濟發展非常危險，因為這會讓我們忽視一個國家所得的分配。 在 21 世紀，我們應該更關注如上圖的指標：「最富有的 10 % 人所擁有的收入份額」，來確保我們不只解決貧困，還會記得要對付社會不平等問題。 就算你認為自己不屬於上面兩種人，別擔心！我幫你列了一個自由勾選的行動清單： [ ] 查看漢斯・羅斯林 在 TED 上的演講 [ ] 閱讀《真確》一書 [ ] 使用泡泡圖找出一個有趣發現（然後留言跟大家分享！） [ ] 分享本文以讓更多人開始探索台灣與世界 [ ] 查看 GapMinder 官網，尤其是 Dollar Street [ ] 找出泡泡圖的翻譯錯誤、協助翻譯指標以幫助其他查看的人（有意願可以跟我聯絡！） [ ] 提供新的數據集建議（理想上是有各國且包含台灣的數據） [ ] 加強數據科學力，學習利用數據說故事（尤其適合資料科學家） 最後，最重要的，希望你能對世界多點好奇，盡情探索。你可以用任何方式探索世界，但如果你打算回來玩玩泡泡圖，隨時歡迎！我會持續更新數據來源並將我（和你）的新發現更新到文章裡頭。 希望你享受我們這趟探索之旅，並期待再次與你探索世界。 致謝 感謝 漢斯・羅斯林 教授，我要謝謝他帶我用更宏觀、積極的態度來理解這個世界並帶給我無數啟發。這篇文章以及文內的泡泡圖是我向他的致敬。 （漢斯・羅斯林教授已於 2017 年 2 月 7 日在瑞典烏普薩拉逝世） 小提醒 我會持續為泡泡圖增加新的數據，你可以將本頁網址加入書籤，方便你隨時回來查看最新泡泡圖：） 你也可選擇用下面的按鈕訂閱我部落格的文章，在最新文章出來的時候收到消息。 另外如果你是在 leemeng.tw/gapminder 閱讀的朋友，還請移駕到 leemeng.tw/gapminder.html 留言，並查看我與其他網友的討論，謝謝！","tags":"Miscellaneous","url":"https://leemeng.tw/gapminder.html"},{"title":"資料科學文摘 Vol.5 數據科學家面臨的挑戰、儀表板設計以及未來的被駭人生","text":"真正的數據科學家面臨的 8 個挑戰是什麼？ 何時一個資料科學家可以說他 / 她真正地「完成」了工作？ 10 個儀表板設計的原則是什麼？ 何謂「被駭」人生？ 為了了解這些跟資料科學息息相關的問題以及可能的解答，這週我們一樣會透過閱讀幾篇文章，來分別了解幾位優秀的資料科學家、UI/UX 設計師甚至是歷史學家是怎麼想的。如同以往的 文摘 ，針對每篇英文文章我會附上摘要並穿插自己的心得，供時間寶貴的你做參考。 事不宜遲，讓我們直接開始吧：） 本週閱讀清單 When Your Job Is Done as a Data Scientist 8 Real Challenges Data Scientists Face Data visualisation, from 1987 to today 10 rules for better dashboard design Hackable humans and digital dictators 本週想跟你分享 5 篇文章。如同以往的 文摘 ，你可以點擊任一連結，從有興趣的摘要看起。有時間的話，我則鼓勵你點擊下面各文章的標題 / 圖片來查看英文原文。 When Your Job Is Done as a Data Scientist 在一個企業裡頭，資料科學家（ D ata S cientist, DS ）常常會被各個部門（Product, Marketing, Sales Team etc）要求做各種不同的分析。如果你把每個分析視為一個專案（Project）的話，2 個你常常會需要問自己的問題是： 什麼時候可以說這個專案完成了？ 要做到什麼程度可以說我這個工作做完了？ 在這篇文章裡頭，資料科學家 Conor Dewey 說明了一個簡單的判斷原則： 如果利害關係人無法利用你的成果做出決策，則你的工作就不算完成。 如果專案的利害關係人（Stakeholders）沒有辦法利用你的分析成果做出（好的）決策，則你的工作就還沒結束。反之，當你確定自己的工作結果能夠影響企業決策後，就不需要再去鑽研一些太複雜但沒有 actionable impact 的事情上面。 如同我們在 之前的文摘 中看到的，比起建立複雜的深度學習模型，學會做一個好的簡報，並跟非技術專業的利害關係人溝通結果，進而 影響企業決策 才是對一個 DS 來說更為重要的事情。 為了產生最大的影響力，不管在做什麼分析或者專案的時候，都得要好好控管自己的時間以及專案的優先順序（Priority）。 雖然該作者在文中並沒有著墨於如何管理時間，你可以利用美國總統 艾森豪 的 時間管理準則 來決定專案的優先順序： 你會發現，這其實就是我們從小到大在說的「輕重緩急」。 將專案依照重要性（Importance）以及緊急程度（Urgency）分為四個象限以後，你就能很清楚地知道該把自己大部分的工作時間花在那些最重要，且緊急的專案上面（上圖的左上角），藉此最大化自己的影響力。 重要的事情通常不太緊急；緊急的事情大多不太重要 - 艾森豪 8 Real Challenges Data Scientists Face 富比士 的這篇文章說明數據科學家在實際工作時會面臨到的 8 個挑戰。以下是我針對這些挑戰，整理出來 5 點 DS 應該時時刻刻放在心上的準則： 你得至少專精一個部門的領域專業。此部門可以是銷售、行銷、廣告或是產品部門，擇你所愛 能向非技術人才、利害關係人簡單明瞭地說明洞見以及可執行的決策，並把技術細節留到 Q&A 不要盲目地想從資料中找出什麼。先利用領域專業或者是直覺來弄出一個假設，然後利用數據驗證結果 明白一個分析的「可信度」只跟你用來做出該分析的原數據「品質」一樣高 不斷地磨練自己處理數據的技能。這通常體現在使用 Python、 R 以及 SQL 的能力 關於第 2 點，此篇文章則是這樣說明的： A data scientist that cannot articulate what their model does and why it's of value to business stakeholders is going to have a difficult path to success. 有固定在追蹤本部落格的你，想必已經非常了解清晰溝通的重要性。你也可閱讀之前的 資料科學文摘 Vol.4 來了解更多相關內容。至於第 4 點，我們則在兩篇文章中有針對資料工程以及數據品質做些著墨： 資料科學家為何需要了解資料工程 資料科學家 L 的奇幻旅程 Vol.1 新人不得不問的 2 個問題 Garbage in, garbage out。 了解企業內的資料處理流程，可以讓你合理地評估利用這些數據產生出來的分析，到底有多少價值以及可信度。 Data visualisation, from 1987 to today 在經濟學人負責資料視覺化的 Graham Douglas 分享他從 1987 年工作到現在，所使用的工具以及製圖歷程。遠在 2, 30年前，在「資料科學」這詞根本還不存在的年代，資料視覺化更像是一門藝術，而不是資料科學： Before computers, creating charts was a lot more like art than data science. 對已經習慣使用 Matplotlib 、 ggplot2 以及 Tableau 等資料視覺化工具的 DS 來說，可能很難想像製作一張折線圖，還需要自己拿尺出來畫等間距格線的時代。 雖然我們現在已經可以利用各種程式語言來輕鬆製圖，讀這篇文章能讓我們重新思考並感謝現代資料視覺化工具帶給我們的方便。我們也看到持續學習新技術以及工具的重要。 對資料視覺化或是 R 語言中的 ggplot2 有興趣的話，可以參考 淺談資料視覺化以及 ggplot2 實踐 。 10 rules for better dashboard design UX/UI 設計師的 Taras Bakusevych 提供了一些很不錯的儀表板（Dashboard）設計建議。 3 點我覺得可以特別提出來： 簡潔，想辦法把精華弄在一頁 不要太依賴互動性，要讓使用者不需什麼操作就能得到重要資訊 選擇對的視覺呈現方式來陳述你想表達的數據關係 針對第 1 點，文章是這樣說的： Don't tell the full story, instead summarize, surface only key info. 大部分儀表板的用意是要讓使用者在「幾秒鐘」之內掌握所有他需要知道的重要資訊。 為了達到這個目的，你應該仔細思考，到底該在儀表板上的有限空間裡頭（一個視窗畫面內）顯示什麼圖表。 不要因為大部分的儀表板可以無限捲動，你就一直往下加新的圖表。什麼圖表都放進去的話，很容易造成資訊過多（Information Overload）而導致使用者抓不到重點。 針對 「選擇對的視覺呈現方式來陳述你想表達的數據關係」 這點，文中則給出一個數據關係跟圖表類型的對照表： 對於一個老練的 DS，這些判斷基準應該都已經很自然地存在你腦海之中的吧！不過我覺得這很適合當做一個 reference 或者 cheatsheet 來使用，提醒自己。 Hackable humans and digital dictators 這篇文章記錄了 人類大歷史 的作者，以色列歷史學家 Yuval Noah Harari 最近在接受新書訪談： 21 世紀的 21 堂課 的內容。 你會說，為何在資料科學文摘裡頭包含了這篇文章？ 在這個一切以數據為本，「數據主義」超越「人文主義」的時代，身為一個 DS，我覺得除了注重數據分析的手法以外，作為一個有血有肉的「人」，也需要去了解數據、機器學習以及 AI 會對未來的我們以及下一代造成什麼樣的影響。這篇訪談中 Harari 用易懂的方式，以歷史學家的角度說明這件事情，值得一讀。以下是我閱讀後整理的摘要。 21 世紀人類面臨的 3 個挑戰： 核子戰爭 氣候變遷 科技破壞（Technological Disruption） 這些挑戰最難的點在於，它們並不能只靠單一一個國家解決，而是要跨國合作。 而前 2 個挑戰幾乎所有人都理解，因此或許不會發生，但最後一項挑戰（科技破壞）的影響卻不太明顯。 未來的人工智慧（ A rtifical I ntelligence, AI ）肯定會自動化掉更多人的「現有」工作。這些 AI 系統也將透過更多的 IoT 裝置來蒐集更多我們的資料（像是搜尋紀錄、身體資訊、情緒變化等），分析這些數據以後來幫我們自動做決策。 這些系統甚至最後可能會告訴我們（現在已經有些系統號稱）： 「透過大數據分析，我比你自己還懂你自己」 這就是所謂的「被駭人生」：這些利用機器學習或是人工智慧的系統能 hack 我們，透過大數據分析，在我們實際行動之前，就已經精準地預測，或者說是大幅度地直接影響我們內心、腦中的決策。你只要想像你現在在做大多數決策的時候，是比較常「聆聽自己內心的聲音」還是去「查看網站、服務、App 給你的個人推薦」就可以稍微了解這點了。 We're becoming Hackable human. 注意的是我們可不是在討論科幻小說，這邊的 AI 不會有情緒感情，只是有著龐大數據、運算能力以及複雜演算法的系統。 如果我們是這些 AI 系統的主人，AI 是為我們每個人自己的利益來服務的話很好。但看看那些大量蒐集你的數據的科技公司：一個比較可能出現的未來是，少數菁英掌握了 AI 力量，而 AI 會為了他們的利益而服務。在這樣的情況下，大多數的人類都會成為不重要的存在，等著被機器取代（如果我們什麼都不做的話）。 The most important fact anybody who is alive today needs to know about the 21 century is that we are becoming hackable animals ... If you can hack something, you can replace it. 這不是在危言聳聽，而是在討論現在的科技發展趨勢之下，可能產生的一個未來。重點是我們在了解現況以後，打算怎麼改變未來。 在找出解決方案之前，你得先了解有什麼問題。 現在還在閱讀 21 世紀的 21 堂課，希望之後能再跟你分享一些我的讀後心得。 結語 在這篇文摘裡頭，我們透過幾篇文章來了解以下幾個議題： 數據科學家的一些工作準則 最大化你的工作影響力並為專案分優先順序 幾個儀表板設計的原則 數據主義時代下的「被駭」人生 因為本文篇幅有限，我只能跟你分享閱讀這些文章以後，自己覺得最精華的一小部分。 閱讀這些文章讓我受益匪淺，因此我分享了自己的摘要，希望能幫助到沒有時間閱讀全部文章的你。儘管如此，我仍建議你從有興趣的議題開始閱讀原文或者相關文章以進一步學習。 同時非常歡迎閱讀後跟我分享你的想法，或是提供一些你覺得有幫助的相關文獻，我會很感激。 Remember we are what we read. Read those books or articles that will make you a better person ：）","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-5-challenges-that-data-scientists-facing-dashboard-design-and-hackable-humans.html"},{"title":"給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上）","text":"今天我們來聊聊如何將 Docker 應用在資料科學領域裡頭吧！ 全文共分上下 2 篇。在這篇裡頭，我們將透過一些簡單的比喻來直觀地理解何謂 Docker，並讓你能在閱讀本文後馬上利用 Docker 來加速你的開發效率；在下篇的內容當中，我則會分享一個資料科學家（ D ata S cientist：DS）為了解決一些數據問題而時常碰到的 3 種 Docker 使用方式。 不管是哪一篇，我們都不會深入探討 Docker 本身是以什麼技術被實現的。反之，我們將會以 DS 的角度，專注在「應用」層面：如何把 Docker 實際應用在資料科學以及資料工程領域裡頭。 這系列文章適合 2 種讀者： 對 Docker 完全沒有概念，但想讓自己的 Workflow 更有效率的資料科學家 熟悉 Docker，但好奇其在資料科學領域如何被應用的工程師 讓我們開始吧！ 雲端運算 & Docker 在解釋何謂 Docker 之前，讓我把你已經非常熟悉的雲端運算（Cloud Computing）老朋友叫出來。 Amazon Web Service（AWS） 、 Google 雲端平台（GCP） 以及 Microsoft Azure 大概是大家最耳熟能詳的幾家雲端計算 / 服務平台了。隨著時代的演進，這些平台提供越來越多樣的機器學習 API，讓開發人員不需做複雜的開發，透過一個 HTTP 要求就能直接使用各種酷炫的服務，比方說： Amazon Lex 讓你使用 Amazon Alexa 的深度學習技術建立聊天機器人 Google Cloud Vision API 讓你快速建立一個圖像辨識服務 Azure Content Moderate API 讓你自動審核網路上的圖片以及文字 儘管如此，很多時候只使用這些現成的 API 並不能滿足我們這些 DS 以及企業的野心。 比起使用現成 API，如何運用雲端運算來 scale 各種數據處理工作是一個 DS / DE 更常問的問題 除了直接用各家雲端平台提供的 API 以外，一個 DS 可能更常需要利用雲端上的計算資源來完成以下的工作： 部署一些新的分析工具來嘗試提升自己及分析團隊的效率 開發、訓練、部署並規模化（scale）自己的機器學習模型 對大量數據做批次處理，將結果儲存後顯示在儀表板上 事實上，這就是本系列文章最想要跟你分享的 3 件 DS 可以活用 Docker 來最大化產出的案例。 當我們透過這篇文章（上篇）熟悉了 Docker 的基本概念以及操作以後，就能在下篇裡頭深入地探討它們。因此在這篇先讓我們專注在學習 Docker 的基礎知識吧！ 雖然我們現在不會細談，但如果你再看一次上面的 3 個工作的話，會發現裡頭可不只包含資料科學（Data Science）。除了建置儀表板以及設計 ML 演算法以外，這裡頭還包含了不少軟體工程、資料工程甚至 DevOps 成分。當然資料工程師（ D ata E ngineer）很樂意幫助你，但如果你想要快速地自己兜出一些方法呢？你該用什麼工具？ 你可能覺得一個 DS 要在各種 deadlines 內完成以上所有的事情是不可能的。不過後面我們會慢慢發現，活用 Docker 能讓這些工作變得簡單許多。 接著就讓我們以 DS 的角度了解 Docker 到底是什麼技術。我相信閱讀接下來的文章，對你之後開發效率的提升是一個非常好的投資。 Docker：可愛的大鯨魚 首先看看以下這張 Docker 示意圖： 有什麼感覺嗎？注意到上圖包含了 3 個要素： 海洋 鯨魚 貨櫃 現在讓我們發揮點想像力。 如果你把雲端運算的平台想像成一個充滿運算資源的 大海 的話，Docker 就是如圖中在裡頭悠遊的大 鯨魚 。這隻 鯨魚 將上述所有 DS 想要做的數據處理工作、執行的 App，一個個封裝成彼此獨立的 貨櫃 ，並載著它們在這大海上運行。 Docker 提供的抽象化讓我們能輕鬆地運行任何想使用的資料科學工具、軟體而不需花費過多時間在建置底層環境。 我知道你可能還是沒什麼感覺，讓我們看下去。 鯨魚背上的貨櫃：Docker 容器 實際上這一個個假想的貨櫃就代表著 Docker 術語裡頭的容器（Container）。 「容器」顧名思義，是一個「容納」了某些東西的「器具」。 一般而言，一個容器裡通常會包含了一個完整的 App。這邊的 App 不是手機上的 App，而是指廣義的應用程式（ App lication）。 DS 常用的 App 可以是： 一個包含 TensorFlow 函式庫的 Jupyter Notebook 伺服器 一個 ML 產品，如透過已訓練的模型來判斷圖片裡頭有沒有貓咪的 Flask App 一個 SQL 查詢以及資料視覺化的工具，如 Superset 一個簡單的 Python Script，針對輸入的大量數據做處理 要從頭建構這些 App 的環境不是不可能，但除了基本的 pip install 以外你還需要花不少工夫；更令人困擾的是，很多時候你在 Mac、Windows 上安裝環境的步驟，到了雲端上的 Linux 機器上就完全行不通了。 如果這時候有人先幫我們把一個在哪邊都能跑的 App 環境建好，我們不是就能馬上開始使用各種分析工具，進行各種有趣的分析，而不用煩惱底層如不同 OS 的差異了嗎？ Docker 的容器就是這樣的一個概念，幫你事先將一個 App 所需要的所有環境，包含作業系統都「容納」在一起。 Docker 將一個 App 會使用到的程式語言函式庫（JAVA、Python、R）、資料庫、甚至作業系統（OS）都包在一個自給自足的容器（CONTAINER）裡頭。想使用某個 App 的 DS 不用從頭建置環境，只需利用 Docker 啟動該容器即可開始工作 容器裡頭不只包含 App 自己本身的程式碼，也涵蓋了所有能讓這個 App 順利執行的必要環境： App 需要的各種 Python 函式庫，如特定版本的 TensorFlow、Pandas 及 Jupyter Notebook MySQL、MongoDB 等 App 會用到的資料庫 App 會用到的各種 metadata、資料集 各種 OS 限定的驅動程式（drivers）、依賴函式庫 （把所有你想得到的東西填進來） 包羅萬象。 因此只要我們能利用 Docker 把一個 App 需要執行的環境全部包在一個容器裡頭，我們就能在任何有 Docker 的地方啟動並運行該容器。不再需要每次重新建置環境，也不用考慮不同機器上的安裝問題。 而這正是 Docker 最強大的地方： Docker - Build, Ship, and Run Any App, Anywhere 因為連 OS 都被包起來了，實際上每個容器（container）的執行環境都是自給自足的（self-contained）。 你可以把它想像成非常輕量的 虛擬機器 ，其執行結果不會因為啟動該容器的「計算環境」不同而受到影響，在任何地方（Anywhere）都能順利被執行，且執行的結果都是一樣的。 以我們前面的比喻來說的話，每個貨櫃（容器 / App）都是我們想要 Docker 幫我們運送（執行）的東西，而不管 Docker 這隻鯨魚（或大船）現在在哪個海洋（計算環境）裡頭，它都能使命必達。 Docker 就像艘大船，幫我們在任何海洋（計算環境）上運送我們的貨櫃（容器） 有一點值得澄清的是，就算 Docker 幫我們抽象化建置一個 App 環境的工作，在執行一個容器的時候，我們還是需要實際的計算資源來跑這些容器。 因此前面所謂的「計算環境」指的是一個擁有計算資源（CPU、GPU、記憶體 etc）且我們實際運行 Docker 的地方。這計算環境可以是任何一家雲端服務平台上的機器，如 AWS 的某台 EC2 機器 、 GCP 上一個包含數千台機器的群集（Cluster），或是你現在用來看本文的筆電。只要 Docker 能在該計算環境下運行，它就能幫我們在該環境「之上」執行任何容器。 簡單來說： Docker 幫我們抽象化在任何 OS 上建置環境的工作。只要給 Docker 一個容器，它就能在任何地方啟動該容器以供你使用。 現在你對 Docker 以及容器概念有個高層次的理解了，讓我們來看看這些 Docker 容器實際上是怎麼來的吧！ 貨櫃（Docker 容器）從哪來 在了解 Docker 這隻大鯨魚能幫我們運行任意的容器 / App 以後，你腦中浮現的第一個問題應該是： 這些容器（貨櫃）最初是怎麼被產生的？ 非常好的一個問題。 事實上，要產生一個新的 Docker 容器，Docker 需要一份「環境安裝步驟書」來讓它幫我們自動地建置容器內的環境，比方說使用什麼 OS，用什麼版本的 TensorFlow 等等。這份步驟書在 Docker 的世界裡被稱作 Dockerfile 。 舉個例子，以下是 Tensorflow 官方釋出的一個 Dockerfile （截錄重要部分）： FROM ubuntu:16.04 ... RUN pip --no-cache-dir install \\ ipykernel \\ jupyter \\ numpy \\ pandas \\ sklearn \\ && \\ python -m ipykernel.kernelspec ... # Install TensorFlow CPU version from central repo RUN pip --no-cache-dir install \\ http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl ... CMD [\"/run_jupyter.sh\", \"--allow-root\"] 除了 RUN 、 CMD 等 Docker 專屬的關鍵字以後，你會發現這份 Dockfile 裡頭的指令其實跟你平常在本地開發時也會使用的指令如 pip install 沒有相差太多。差別在於透過第一行的 FROM ubuntu:16.04 指令，我們要求 Docker 在這個容器裡頭建置一個 Ubuntu OS 後，在其之上安裝這些函式庫。 追求規模性：Docker 映像檔的誕生 聽完以上的解釋，你可能會覺得在我們每次要啟動一個新的容器的時候，Docker 就得拿出 Dockerfile，一步步建置該容器的環境。 這樣的實作也不是不行，但很沒有效率。為什麼？ 其中一個考量是可擴展性（Scalability）。 有時你會想要用同一份 Dockerfile 在短時間內迅速地產生好幾個一模一樣的容器(s)： 用多個相同的機器學習模型，同時對大量的新數據做批次預測 使用多個相同的 Python Script 來處理大量數據 這時候與其在每次要啟動新的容器時才拿出 Dockerfile 建置環境，Docker 可以事先用這個 Dockerfile 把建置環境所需的步驟先做好一遍，然後把該環境「拍張照」，存成一個 Docker 映像檔（image）後等待之後的使用。 等你決定要開始使用容器的時候，因為我們已經有一個環境的快照（Snapshot），Docker 就能利用該映像檔，快速地啟動 1 個（或 100 個）相同的容器給你。 Docker 三元素 ： Dockerfile、Docker 映像檔以及 Docker 容器 到了這邊，我們已經了解 Docker 最基本也是最重要的概念： Docker 利用 Dockerfile 預先建置好一個 Docker 映像檔。在使用者想要使用容器的時候，以該映像檔為基礎，運行一個對應的 Docker 容器 坐而言不如起而行。 在掌握了這些概念以後，我相信你也迫不及待地想要開始使用 Docker 了，接下來就讓我們實際操作 Docker 來體會一下它的威力。 Docker 映像檔：法式千層酥 不管是 Windows 或是 Mac 用戶，你都可以很輕鬆地在 官方網站 下載 Docker 並安裝。 下載完以後啟動 Docker，大鯨魚就會在你的筆電上開始閒晃，等待你的指示。一般而言，我們會在 terminal 使用各種 docker 指令來跟大鯨魚溝通。 當 Docker 就緒以後，依照我們前面的所學，你會需要一個 Dockerfile 或是 Docker 映像檔來產生一個 Docker 容器。就像 Github 是一個被大家拿來分享程式碼的地方， Dockerhub 則被用來分享 Dockerfile 以及 Docker 映像檔。 假設我們現在要開始一個新的 TensorFlow 專案，並且想透過 Jupyter Notebook 進行開發，最省力的方式就是從 Dockerhub 下載一個 TensorFlow 官方 幫我們弄好的 Docker 映像檔。 讓我們打開一個 terminal 並輸入 docker pull 指令： docker pull tensorflow/tensorflow 第 1 個 tensorflow 代表 Tensorflow 的官方 Dockerhub repository，就跟 Github repository 的概念相同；第 2 個則是容器名稱。 你會看到當 Docker 在下載映像檔的時候，同時也在建置環境，而其環境會分成一層一層（Layer）的： $ docker pull tensorflow/tensorflow Using default tag: latest latest: Pulling from tensorflow/tensorflow 3b37166ec614: Already exists ba077e1ddb3a: Already exists 34c83d2bc656: Already exists 84b69b6e4743: Already exists 0f72e97e1f61: Already exists 6086c6484ab2: Pull complete 25817b9e5842: Pull complete 5252e5633f1c: Pull complete 8de57ae4ad7d: Pull complete 4b7717108c3b: Pull complete b65e9e47e80a: Pull complete 006d31e013ea: Pull complete 700521cc53f3: Pull complete Digest: sha256:f45d87bd473bf999241afe444748a2d3a9be24f8d736a808277b4f3e32159566 Status: Downloaded newer image for tensorflow/tensorflow:latest 我們不會細談 Docker 實作細節，但你可以想像 Docker 映像檔是一個法式千層酥（Mille Feuille）。 這時候的 Docker 是一名蛋糕師傅，利用 Dockerfile 作為食譜，逐行執行裡頭的指令以建立一層層的環境。每做出一層新的環境，就把它加在目前所有環境的上面，最後成為一個 Docker 映像檔。 這樣做有 2 個好處： 當你對 Dockerfile 做變動的時候，Docker 可以只針對被改變的那一層環境做修改，而不用重建每一層，減少建置環境所需要的時間 有利用到一樣環境的不同映像檔可以分享部分結果（如上面的 Already exists ） 一個 Docker 映像檔就像是蛋糕師傅利用 Dockerfile 食譜做出來的法式千層酥 （誠摯地希望你不是晚上看本文，餓了） 依照你的網路速度，下載映像檔所需的時間可能有所不同。 在下載完成以後，輸入 docker images 指令可以顯示所有目前本地端擁有的 Docker 映像檔： $ docker images tensorflow/tensorflow REPOSITORY TAG IMAGE ID CREATED SIZE tensorflow/tensorflow latest 76fb62c3cb89 2 weeks ago 1 .23GB 這邊因為我的環境裡已經有一大堆的映像檔，我在 docker images 後面加入額外的篩選器來告訴 Docker 只顯示 tensorflow repository 裡頭的 tensorflow 容器。 有了映像檔以後，最令人期待的時刻終於來臨了！ 我們現在要呼叫 Docker 幫我們從這個映像檔產生並執行（run）一個新的 Docker 容器： docker run -it -p 1234 :8888 tensorflow/tensorflow 短短一行指令，包含了 3 個你不可不知的重要概念： 利用 docker run 來告訴 Docker 我們要利用 tensorflow/tensorflow 映像檔來運行一個容器。實際上 Docker 容器就是在 Docker 映像檔的環境之上再加 1 層可執行的環境供你使用（貫徹千層酥的理念） 利用 -it 參數來告訴 Docker 我們同時要建立一個互動式的 TTY 連線，讓容器內的結果直接顯示在我們的 terminal 裡頭，彷彿我們在本地環境下執行該 App 一樣。我們之後還可以直接在 terminal 使用 Ctrl + C 或 Command + C 來終止容器 利用 -p 1234:8888 告訴 Docker 我們將會透過本地端的 1234 port 來連到容器裡頭的 8888 port 你可以透過 docker run --help 來查看所有 docker run 可以使用的參數。 另外，一個 DS 應該都知道， 8888 是 Jupyter Notebook 預設的 port。因此我們的企圖就跟司馬昭之心一樣，打算透過本地端的 1234 port 連到在容器裡頭跑的 Jupyter Notebook。 現在打開你的瀏覽器並輸入 localhost:1234 ，應該就能連到容器內部的 Jupyter Notebook 伺服器： 容器內的 Juypter Notebook 畫面，所有環境包含 TensorFlow 都已經幫你設置好 （輸入你在啟動容器的 terminal 裡看到的 token 就能通過認證） 對你沒看錯，你已經用 Docker 建置了一個完整的資料科學環境，裡頭有 TensorFlow 以及 Jupyter Notebook。 而你只需要 2 個指令： docker pull tensorflow/tensorflow docker run -it -p 1234 :8888 tensorflow/tensorflow 建置環境什麼的交給 Docker 吧，你已經能馬上開始實作機器學習模型了。 有些 DS 可能會覺得他的 Anaconda 或者是 pip 功能爐火純青，不需要用到 Docker 也能自己在本地建出這樣的環境。其實沒錯，如果你只是開發個人專案，說真的不學 Docker 也沒關係（喂！） 但就如我們在下篇會看到的，當你在開發企業等級的數據處理工作、機器學習模型的時候，你可不能永遠躲在你的本地環境裡頭。當你習慣於在不透過 Docker 的情況下在本機建置環境，等到要在各種雲端平台上的機器重現你的結果的時候，你就會發現不妙了。 利用 Docker 分享你的成果 為了加強你使用 Docker 的動機，讓我再給個例子。 有持續關注我文章的讀者會發現，我在 資料科學文摘 Vol.3 Pandas、Docker 以及數據時代的反思 裡頭有提到，Docker 除了讓我們免除建置環境的痛苦以外，也能讓我們與他人簡單地分享開發結果。 Cat Recognizer 是我用 TensorFlow 以及 Flask 實作的一個非常 naive 的貓咪辨識 App。 如同我們前面所說的，我事先將所有此 App 需要的環境用一個 Dockerfile 定義、全部包在一個 Docker 映像檔後分享在 Docker Hub 上面。 任何想要使用此 App 的人，只需要利用 Docker 輸入兩行指令： docker pull leemeng/cat docker run -it -p 2468 :5000 leemeng/cat 接著他們就能在瀏覽器輸入 localhost:2468 來看到我的 App： Docker 讓你與其他人分享成果，不須額外做一大堆環境設定 當然這個 ML App 在預測能力以及 UI 上都不完美，但這邊重點是你能利用 Docker 與他人快速地分享成果。如果你有想到其他利用 Docker 封裝好的 ML App 例子（或者是你接下來打算做一個自己的），非常歡迎留言讓我知道它們的存在：） 總結 呼！看完本文以後，相信你現在應該對 Docker 有個非常清楚的認識了： Docker 是一個能幫我們在各種不同 OS 上建置開發環境的工具 Docker 三元素包含 Dockerfile、Docker 映像檔（Image）以及 Docker 容器（Container） Docker 利用 Dockerfile 預先建置好一個 Docker 映像檔。在使用者想要使用容器的時候，以該映像檔為基礎，運行一個對應的 Docker 容器 Docker Hub 上有各式各樣可以直接供使用的映像檔 你只需要 docker pull 及 docker run 就能開始一個分析專案 給自己鼓鼓掌！ 現在這張 Docker 的示意圖在你眼裡應該變得平易近人許多 正因為我們是資料科學家，利用 Docker能幫我們抽象化很多不必要的環境建置工作，加速我們的開發效率。 在本系列文章的下篇出爐之前，我鼓勵你先 下載 Docker ，並開始在 Docker Hub 或者 Google 搜尋一些你感興趣的映像檔，甚至自己寫一個 Dockerfile 將你目前的專案打包起來跟別人分享。 雖然我們這篇因為篇幅關係沒有細講，但只要有一個 Dockerfile，你就能使用 docker build 來輕鬆建立一個自給自足的 Docker 映像檔。一個 Dockerfile 也不難寫，像是上面貓咪的 App 的 Dockerfile 也不過就如此幾行： FROM python:3.6.3 MAINTAINER Meng Lee \"b98705001@gmail.com\" COPY ./requirements.txt /app/requirements.txt WORKDIR /app RUN pip install -r requirements.txt COPY . /app ENTRYPOINT [ \"python3\" ] CMD [\"app.py\"] 在本篇裡頭我們都是在自己的機器上使用 Docker。在下篇，我們將利用本篇學到的 Docker 知識，將其運用在浩瀚無垠的雲端平台之上，去最大化我們的影響力。 在那之前你可以先熟悉熟悉 Docker，下次遇到你的 DS 同事時，可以問問他/她： 嘿！你的 Docker Image 呢？","tags":"Miscellaneous","url":"https://leemeng.tw/3-ways-you-can-leverage-the-power-of-docker-in-data-science-part-1-learn-the-basic.html"},{"title":"資料科學文摘 Vol.4 數據科學 MMORPG 上線！你，選好自己的角色了嗎？","text":"如同以往，這篇文摘會介紹幾篇最近作者閱讀的文章以及其摘要。 不過這次在條列式列出文章以前，我想先跟你分享身為一個資料科學家（ D ata S cientist，DS），我在閱讀這些文章後得到的一些想法。 與其說是想法，應該說是「針對資料科學家這個職業，自己感受到的一些發展趨勢以及對這個職業接下來數年的職涯預測」。對於那些只有 3 分鐘可以閱讀此文的你，這些想法可以歸納成以下幾點： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 如同 大型多人線上角色扮演遊戲（MMORPG）一般 ，在後數據時代，萬能、什麼數據工作都會的「資料科學家」這個幻想已在式微。取而代之的是各個對相關領域專精的「數據」職業角色們：商業分析師、資料工程師、機器學習工程師、AI 研究者等（圖為 線上遊戲：暗黑破壞神 3 的角色一覽） 接下來我將會列出本週的閱讀清單，並在簡單說明各篇摘要的同時，一一描述它們是如何跟上述幾點概念互相呼應。最重要的，我們將探討在這個什麼職業都跟數據扯上關係的年代，你要如何在「全球數據科學 MMORPG」裡頭，找出自己的定位以及角色。 這篇文章適合對資料科學領域有興趣，或是未來想從事數據相關工作的你。放心，以文章長度來說，保證比上一篇文章：「 一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載 」要來得平易近人許多。 讓我們開始本週的閱讀之旅吧！ 本週閱讀清單 One Data Science Job Doesn't Fit All The Death of the Data Scientist How to be a bad data scientist! Beyond Interactive: Notebook Innovation at Netflix What Data Scientists Really Do, According to 35 Data Scientists 如同以往的 文摘 ，你可以從任意一篇開始看我寫的摘要。不過建議先把所有標題掃過一遍，感受一下我們接下來要談的話題。 另外如果真的很趕時間，可以直接 跳到文章最後 看我給你的建議。 One Data Science Job Doesn't Fit All 在這篇文章中，Airbnb 解釋他們如何在經過多年發展資料科學以後，將資料科學家分為三個路線（Tracks）： 分析路線（Analytics） 演算法路線（Algorithms） 推論路線（Inference） 會這樣做的其中一個很大原因是因為「資料科學」包含的領域太廣，不像這樣細分的話，第一，DS 們不知道自己該注重在什麼方面的知識；第二，公司內部跟某個 DS 合作的團隊也不知道他的專精以及該怎麼期待他的能力。 Airbnb 經過多年經驗，將資料科學家細分為三個路線，主要就是為了讓每個 DS 能專注在對的地方 其實想想很自然。就像是現在我們很習慣將工程師粗淺地分為前端（Frontend）和後端（Backend），未來的資料科學家也有很大機會依照個人的專精以及企業需求來細分路線。要不現在你想知道一家公司對 DS 的定義，還得親自去問裡頭的資料科學家到底在做什麼，且十家公司的 DS 可能會給你 9 種答案。 理想上一個資料科學家是通才（Generalist），三個路線的專業都大致了解。儘管如此，學海無涯。一個建議是至少找出哪個路線你有興趣，去專精它，並尋找渴望你專業的企業。 這呼應到我們最前面提到的第 4 項趨勢（也是最重要的一項）： 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 將這些路線想像成角色扮演遊戲（RPG）中的角色就對了！順帶一提，作者自己想專注在演算法路線，輔修分析路線，你呢？ 另外這篇沒提到跟資料科學密切相關的資料工程（Data Engineering），個人臆測是因為 Airbnb 的資料平台本身建得夠齊全，有很專業的資料工程師在幫 DS 完成這些事情。 The Death of the Data Scientist 「資料科學家的滅亡」。 非常聳動的標題，而且你可以從封面圖片看出作者想要表達 DS 會像恐龍一樣滅絕。 不過基本上我是認同的。 不是說 DS 不再重要，而是再過幾年，就像當年的「大數據」風潮，各企業或許不會再像現在一窩蜂地招聘大量的「資料科學家」，而是各行各業的每個人都能很自然地將資料科學應用在自己的工作裡頭。 如同我們在 揭開資料科學的神秘面紗 一文提到的一樣，在數據驅動的時代之下，培養「資料科學力」將不再只是資料科學家的專利；就算你不是資料科學家，也應該加入這個行業。 這呼應到我們最前面提到的第 3 個趨勢： 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 How to be a bad data scientist! 這篇說明了一般人在學習資料科學時會有的一些錯誤思維，我們應該隨時警惕自己並改善學習態度。 我自己歸納一下新手 DS 常會遇到的迷思或困境： 缺乏持續學習的動力：剛開始你可能因為資料科學很夯，薪水很高決定成為一個 DS。但資料科學領域的最大特色是變動很快。缺乏熱情或是單純跟隨潮流的人，如果沒有持續學習的動力可能會中途開始懷疑人生 誤以為了解全世界：上了幾門線上課程或是參加過 Kaggle 競賽，利用乾淨的資料在 Jupyter Notebook 上建立一個 XGboost 模型就誤以為掌握了所有的資料科學。事實上，業界的 DS 需要做更多事情，如清理資料、建立可靠的資料管道以及與其他部門溝通協調等等。雖然本文沒辦法教你怎麼做良好溝通，想多了解資料工程的話可以參考 資料科學家為何需要了解資料工程 為了學而學，沒有思考如何應用所學：這點甚至稍微資深的 DS 都會遺忘。你最少要嘗試將平常閱讀的文章、學到的分析手法應用在解決工作上的問題。甚至更好的是，改善自己或者周遭人們的問題 這篇並不直接跟本篇主題相關，不過值得 DS 們參考。 Beyond Interactive: Notebook Innovation at Netflix 平常有在關注 Jupyter Notebook 的 DS 們想必都注意到 Netflix 這篇文章了吧。 Netflix 的資料平台（Data Platform）團隊發現，儘管企業內部有各式各樣使用該平台的使用者（如 DS、資料工程師以及分析人員等），並且表面上看來都在使用不同的程式語言，不同的工具，但事實上這些平常在處理數據的人的工作流程（Workflow）大多都可以分為這幾個步驟： 存取資料 資料處理 資料視覺化 排程以及產品化（Productization） Netflix：不同數據專業的人使用很不一樣的工具以及程式語言，但其實宏觀來看，處理數據的工作流程都很類似 在明白這點以後，Netflix 的資料平台團隊展示了他們如何利用 Notebook 的「介面跟計算分離」這個特性，開發出能讓所有分析人員使用的統一介面。 在 Netflix 裡頭，任何一個 DS / DE 都可以利用一個簡單的 Notebook 介面做到： 存取 Netflix 裡頭所有的數據：內部有專門的團隊維護一個可以存取所有資料的 Python 函式庫 參數化 Notebook：一個 Notebook 可以變成一個模板（Template），讓使用者可以每次利用不同參數重新執行類似的數據處理 排程（Scheduling）。當使用者決定為目前 Notebook 規劃排程後，該平台會將當下使用者的 Notebook 存到 AWS S3 變成排程工作的參數設定，並在實際排程時建立輸出用的 Notebook，將所有 Logs 以及輸出都放在該輸出用的 Notebook 裡面，方便之後查看以及除錯。這最小化了一個 DS 建立 ETL 工作的時間以及人力成本。 這篇因為篇幅關係不會進一步解釋，但就算你平常沒在用 Notebook，應該也能感受到 Netflix 的數據平台團隊為了支援每天能在 100 PB 的數據量上跑的 15 萬個處理工作（Job）所做出的努力吧！ 抽象化（Abstraction）是對付複雜性（Complexity）最好的解藥。 這個例子我們看到，Netflix 為了提高他們內部資料科學家的效率以及規模性（Scalability），做了一個這樣的數據平台，將所有基本的資料工程，甚至是對一個正常的 DS 來說需要花不少時間熟悉的數據處理流程都自動化 / 抽象化了。 在全球數據量仍然爆炸性成長的今天，這樣的抽象化只會越來越普遍地出現在各個企業裡頭，而這對一個資料科學家來說當然是好事。再過一陣子，一個一般的 DS 或許也就不用再花所謂的 80 % 時間來做數據清理、建構資料管道等瑣事上，而是能有更多的時間在建構預測模型、進行複雜分析等更高層次的工作。 現在我們已經有各種開源的自動化工具，幫我們快速地將機器學習產品化（如 Amazon 的 SageMaker ）、自動化清理數據的工具（如 Google 的 CLOUD DATAPREP ）等等。一方面 DS 要慶幸這些事情可以被自動化，一方面則要努力學習新知，不能停滯不前。 這呼應到我們前面提到的第 1 點趨勢： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 儘管我們並不都在有這些平台的企業工作，了解自己企業的現有狀況，盡可能將能夠自動化的「數據處理瑣事」抽象化，能讓一個 DS 提高自己的效率以及工作價值。 What Data Scientists Really Do, According to 35 Data Scientists 如果只有閱讀一篇原文的時間的話，我推薦你這篇哈佛商業評論的文章。 這篇透過訪談多位資料科學家的工作經驗，讓我們能好好地思考「資料科學家」這個職業的未來走向。 首先，根據這些資料科學家所說，（事實上我也這麼認為）一個 DS 並不像有些人想像的，整天在研究 AI 演算法。 實際上，這些 DS 在做的是： 資料搜集、資料清理 統計推論（Statistical Inference） 建立儀表板（Dashboard）或是績效報告 實作機器學習以及資料處理管道（Date Pipeline） 跟決策者辯論，影響企業決策 跟專案的利害關係人說明分析結果 從這篇文章，我們也再次觀察到同樣的趨勢：現在的資料科學家的工作範圍以及被期待的技能樹過於廣泛，未來將會再進一步細分。其專業領域的細分的方式則可能依企業不同而異，像是前面提到 Airbnb 的 DS 的三個路線；或是此篇文章內提到的 Type A、Type B 的資料科學家；或是更廣泛地分為資料科學家、資料工程師以及機器學習工程師。 我們同時也從這些專業的資料科學家的口中再度認識到溝通能力的重要。 比起建立複雜的深度學習模型，學會做一個好的簡報，並跟非技術專業的利害關係人溝通結果，進而影響企業決策才是對一個 DS 來說更為重要的事情。 人類大歷史 的作者 哈拉瑞 Yuval Noah Harari 最近也在 訪談 中提到未來 AI 時代裡頭，人類 4 個最重要的技能 4C： 批判性思考（Critical Thinking） 合作能力（Collaboration） 創造能力（Creativity） 溝通能力（Communication） 這呼應到我們最前面的第 2 點的發現： 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 結語 在這篇文摘裡頭，我們透過閱讀不少跟資料科學家相關的文章，了解到了幾個 DS 的職涯趨勢： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 這些都是不錯的發現，但如果你只能記住其中一個的話，我希望是最後一個。 如同我們在 Airbnb、Netflix 的例子以及多名專業的 DS 口中可以觀察到這個現象： 在不久的將來，非常有可能各個企業都依照分析領域的不同，再度細分一個 DS 的工作，並將其分為不同的路線，或是直接產生新的職業。 要我打個比方的話，就是像真實世界的 RPG 一樣。 在急著成為一個資料科學家之前，仔細思考數據科學領域裡頭，究竟什麼地方吸引你？ 你是喜歡做統計分析、執行 AB 測試來提供產品改善的洞見嗎？ 還是你熱衷於研究機器學習演算法，想辦法利用龐大數據改善企業的數據產品（Data Product）呢？ 或者你對建構能夠處理大規模資料的數據平台的工作感興趣呢？ 現在就開始思考你想要開的門、走的路線、想要成為的角色是什麼，專精它，並尋找渴望你專業的企業 不管你的答案是什麼，既然我們在玩 MMORPG 遊戲（好吧，可能只有我玩）的時候都會去認真地理解每個職業的優缺點、技能樹等等，為何不將各種數據職業視為一個個的 RPG 角色，了解自己的興趣以及跟這些職業的適合程度呢？ 玩遊戲很嗨，能把規劃數據相關的職涯當做遊戲來玩更嗨。 最後，讓我把文章開頭所問的問題交給你思考並回答： 數據科學 MMORPG 全球玩家齊聚上線。你，選好自己的角色了嗎？","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-4-choose-your-own-character-in-data-science-role-play-game.html"},{"title":"一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載","text":"這是一篇當初我在入門資料工程以及 Airflow 時希望有人能為我寫好的文章。 Airflow 是一個從 Airbnb 誕生並開源，以 Python 寫成的 工作流程管理系統（Workflow Management System） ，也是 各大企業 的資料工程環節中不可或缺的利器之一。 近年不管是資料科學家、資料工程師還是任何需要處理數據的軟體工程師，Airflow 都是他們用來建構可靠的 ETL 以及定期處理批量資料的首選之一。（事實上在 SmartNews ，除了 DS/DE，會使用 Airflow 的軟體工程師也不在少數） 我們在 「資料科學家 L 的奇幻旅程(1)：新人不得不問的 2 個問題 」一文提到 SmartNews 如何利用 Airflow 建立資料管道並管理各種 ETL 儘管它的方便以及強大，在完全熟悉 Airflow 之前，因為有些專業術語以及資料工程概念的存在，不少初學者（包含當時的我）在剛開始的時候容易四處撞壁。另外如果一開始就以 ETL 當作 Airflow 的入門的話，未免難度過高且缺少共鳴。 追連載：一個 Airflow 的輕鬆使用案例 這篇文章希望以一個簡易的漫畫連載通知 App 作為引子，讓完全沒有資料工程經驗的讀者也能夠透過這個 App 的例子，輕鬆地理解工作流程的概念、自動化排程以及 Airflow 的使用方式。閱讀完本文，你將對 Airflow 以及自動排程工作有更深的理解，並學會如何建立多個能在 Airflow 上穩定運行的工作流程。更重要的，我相信你能利用這些學到的基礎，開始自動化自己生活中以及企業的數據處理 pipeline。 如果你對資料工程有興趣，不太熟悉如 Airflow 這種工作流程管理系統，但有基本的 Python 程式基礎的話（或是純粹對用 Python 寫一個漫畫連載通知 App 有興趣），我相信這篇文章應該會很適合你。 Slack 截圖：追漫畫應該要是件輕鬆的事情。 我們將利用 Airflow 來實作一個像這樣會每天從 Slack 推送最新漫畫連載的 App 想重新複習 ETL 概念的讀者可以參考先前的文章： 資料科學家為何需要了解資料工程 。 章節傳送門 了解需求：所以為何要這 App ？ 工作流概念 & Airflow Python 實作 & Airflow 操作 建置 Airflow 環境 Airflow 基本概念 App 版本一：大鍋炒 App 版本二：模組化 App 版本三：填填樂 結語 為讓讀者完整了解開發這個 App 的背景脈絡、此 App 的執行邏輯以及使用 Airflow 來定期執行 App 的原因，在我們實際開始寫 Python 之前有兩小節的解說。 如果你已經有 Airflow 及工作流程的基礎知識，且迫不及待想看 Python 程式碼，可以直接跳到 Python 實作 & Airflow 操作 章節之後再回來查看前面段落。 這篇文章章節不少，你有時可能會需要回到前面章節回顧一些內容。活用左側放大鏡按鈕下面的章節傳送門能讓你更輕鬆地徜徉在本文的 Airflow 世界（此功能因為作者時間有限，目前只在寬螢幕實現） 所以為何要這 App ？ 平常有在網路上追漫畫連載的讀者們應該都了解，市面上的漫畫網站通常都不是會員制的。更不用說「在新連載出的時候自動通知您！」這種推送功能（Push Notification）了。也因為這樣，導致我常常三不五時上去這些漫畫網站，看每個關注的漫畫到底出了最新一話了沒。可想而知，答案通常是否定的。（一週出一次每天檢查也沒用啊啊啊） 如果你只看海賊王一個漫畫（索隆好帥！），這或許沒什麼負擔。但就像上面 Slack 截圖顯示的，我不只關注海賊王，還看很多其他漫畫。讓事情更糟的是，到最後你會發現： 不記得自己到底在追哪些漫畫 每一部漫畫最後到底是看到第幾話 上一話是什麼時候出的 有幾話是新出而你還沒看的 手動追最新連載經常讓我追到懷疑人生 追漫畫連載應該要是個輕鬆且享受的事情。在一個人人會寫 code 的時代，何不自己做個 App 幫我們自動檢查新連載呢？ 工作流概念 & Airflow 概念上我們可以把此 App 需要做的工作按照「先後順序」由上往下列出來： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 想像上述的工作清單由上往下流動，就形成了一個工作流程（Workflow）：前一個工作如寄 Slack 通知就是下一個工作：更新閱讀紀錄的上游工作（Upstream Task）。 反過來說，更新閱讀紀錄則是寄 Slack 通知的下游工作（Downstream Task）。 定義出工作之間的上下游關係的好處是什麼？ 可以讓我們確保工作之間的相依性（Dependencies）並讓如 Airflow 這種工作流程管理系統幫我們管理工作流程。一般而言，下游工作只能在上游「成功」完成之後被執行；如果上游工作失敗的話，下游工作應該被終止，通常也沒有繼續執行的意義（例：如果 App 在執行上游工作「取得使用者閱讀紀錄」時就失敗的話，不需要也不應該執行下游的「更新閱讀紀錄」工作）。 我們的 App 實際上就是一個完整的工作流程。 App 從工作 A 執行到工作 B 就像是水從上游 A 流動到下游 B 一樣。 我知道你在想什麼。 屏除剛剛介紹的工作流程概念，要實作這 App 的邏輯一點都不難。事實上我們只需要寫個 Python script，把每個工作各別用一個函式（Function）實作後再按照順序呼叫它們就好（你甚至可以只用一個函式實現所有邏輯！），為何需要 Airflow？ 在你往下滑前給個提示：我們這個 App 不是每一秒鐘都在執行。 對！顯而易見的，因為這個 App / 工作流程設計的方式不是即時工作（Realtime Job），而是批次工作，執行一次以後就結束它的生命了。 我們可不希望它只在明天早上（比方說早上 9 點）去檢查新連載。我們希望它明天、下個月或是明年的今天早上都在運作。這也是為何我們需要一個像是 Airflow 的工作流程管理系統： 定期執行工作流程 維護相依性，確保工作流程從上游到下游執行，不會在上游沒完成前執行到下游 各個工作失敗時自動重試（ 墨菲定律 ，所有你認為邏輯上萬無一失的工作都會因為各種無法預期的情況給你失敗的驚喜） 簡單易懂的 Web UI 方便管理工作流程 Airflow 非常適合用來管理相依性複雜，且具批次處理性質的工作流程。 Airflow 的 Web UI 讓我們能更輕鬆地管理及排程工作流程（後面我們會實際利用此 UI 管理並開發 App） 事實上我們也可以透過 Linux 排程工具 Cron 來定期執行我們的 App。但 Cron 本身沒有工作流程的概念，沒辦法管理上下游工作的相依性、失敗時無法自動重跑、當然也沒有易懂的 Web UI。因此以 2, 3, 4 項的角度來看，Airflow 是一個比較好的選擇。 到此為止，我們已經了解 為何要做這個 App 此 App 的工作流程以及工作流程（Workflow）的基本概念 為何要使用 Airflow 來幫我們管理 App 的工作流程 接著只差用 Python 將 App 的邏輯以 Airflow 工作流程的方式實現了，讓我們開始實作吧！ [Warning] 接下來不只給你 Python 程式碼，而是給你大量的 Python 程式碼 Python & Airflow 實作 程式碼都會放在這個 Github Repo 裡頭供你在閱讀完文章後參考。但如果你正在用電腦瀏覽的話且想趕快熟悉 Airflow 開發的話，可以 git clone 下來以後跟著文章走。 開啟一個新的 terminal，移動到你平常放新專案的資料夾，然後輸入： git clone https://github.com/leemengtaiwan/airflow-tutorials.git cd airflow-tutorials 之後沒特別明說的話，指令都會是在 airflow-tutorials 資料夾底下執行。 建置 Airflow 環境 雖然 production 環境需要很多調整，以建構測試環境來說，基本上參考官方的 Quick Start 就可以很輕鬆地完成。因為 Airflow 是以 Python 實作的，我們可以很輕易地用 pip install 來安裝所有需要的東西。用 Anaconda 則是能讓你事後管理不同專案的環境時輕鬆不少： conda create -n airflow-tutorials python = 3 .6 -y source activate airflow-tutorials pip install \"apache-airflow[crypto, slack]\" export AIRFLOW_HOME = \" $( pwd ) \" airflow initdb 以上的指令幫我們： 建立並啟動一個新的 Anaconda 環境 在此環境下安裝 Airflow 以及 支援 Slack 功能的額外函式庫 設定專用路徑以讓 Airflow 之後知道要在哪找檔案、存 log 初始化 Airflow Metadata DB。此 DB 被用來記錄所有工作流程的執行狀況 理想上把 AIRFLOW_HOME 加入到 ~/.bash_profile 裡頭之後會比較輕鬆，不過現在不做也沒關係。 【2018/08/27 加註】如果沒有設定 export AIRFLOW_HOME=\"$(pwd)\" 就執行 airflow initdb 的話，會讓 Airflow 使用作者當初測試時使用的路徑，而不是你 git clone 下來的 repo 的路徑而造成問題，務必記得設定。 在環境都搞定之後，我們可以啟動 Airflow 的網頁伺服器： airflow webserver -p 8080 接著在瀏覽器輸入 localhost:8080 就能看到 Airflow 簡潔的 Web UI 了： Airflow Web UI 首頁：顯示所有已定義的工作流程（DAG）。 圖中的 3 個 DAG 就對應到我們接下來逐漸改善 App 時產生的三個 App 版本 Airflow 基本概念 這邊值得注意的是 Airflow 利用 DAG 一詞來代表一種特殊的工作流程（Workflow）。如工作流程一樣，DAG 定義了我們有什麼工作、工作之間的執行順序以及依賴關係。DAG 的最終目標是將所有工作依照上下游關係全部執行，而不是關注個別的工作實際上是怎麼被實作的（這點在後面的 Operator 章節會有詳細解釋）。 另外從它的全名 有向無環圖（ D irected A cyclic G raph） 你可以看出它具備兩個特色：「有向」及「無環」。事實上我們的 App 邏輯就是一個理想的 DAG。首先，裡頭包含多個邏輯上的工作： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 很明顯地， App 是從上而下地執行每個工作，即為「有向」；同時 App 不會在更新閱讀紀錄以後（下游工作），還跑回去漫畫網站看有沒有新的章節（上游工作）：上游會指向下游，但下游不會指回上游，此即「無環」。 有了這個理解以後，我們的目標就很明顯了：將 App 的工作流程轉換成一個能在 Airflow 上執行的 DAG，然後排程它，就能讓它每天去找新連載！ App 版本一：大鍋炒 在 Airflow 世界裡，一個 DAG 是由一個 Python script 所定義的。 以下是我們 App 的第一個版本，也是最簡單的 DAG comic_app_v1 的程式碼（ airflow-tutorials/dags 資料夾底下的 comic_app_v1.py ）： import time from datetime import datetime , timedelta from airflow import DAG from airflow.operators.python_operator import PythonOperator default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } def fn_superman (): print ( \"取得使用者的閱讀紀錄\" ) print ( \"去漫畫網站看有沒有新的章節\" ) print ( \"跟紀錄比較，有沒有新連載？\" ) # Murphy's Law accident_occur = time . time () % 2 > 1 if accident_occur : print ( \" \\n 天有不測風雲,人有旦夕禍福\" ) print ( \"工作遇到預期外狀況被中斷 \\n \" ) return new_comic_available = time . time () % 2 > 1 if new_comic_available : print ( \"寄 Slack 通知\" ) print ( \"更新閱讀紀錄\" ) else : print ( \"什麼都不幹，工作順利結束\" ) with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 為了讓你能專注在 Airflow 及 DAG 最核心的概念，讓我先用 print() 假裝我們已經在一個函式 fn_superman 裡頭實作所有工作的邏輯了。在修改完代表一個 DAG 的 Python script 後，要確保 Airflow 能正確地將其視為一個 DAG，最基本的檢查就是用 Python 直接執行該 script。 你目前的 terminal 應該正被 Airflow 的網頁伺服器所使用。如果你還沒有把 AIRFLOW_HOME 加到 ~/.bash_profile 裡頭的話，開啟一個新的 terminal，重新進入 airflow-tutorials 資料夾以後執行： source activate airflow-tutorials export AIRFLOW_HOME = \" $( pwd ) \" 這邊我們為新的 terminal 啟動 Anaconda 環境，並告訴 Airflow 在 airflow-tutorials 資料夾底下找所有它要的東西。（之後要打開新的 terminal 也要做一樣的事情） 接著我們就可以用 Python 測試 script 的正確性： python dags/comic_app_v1.py 沒有特別設定的話， Airflow 會去 AIRFLOW_HOME 路徑底下的 dags 子資料夾找 DAG，這也是為何我們在上面路徑有個 dags 。（你可以去 Repo 確定檔案的路徑。） 如果沒有任何錯誤跑出來，恭喜！Airflow 能將其視為一個正常的 DAG 並顯示在 Web UI 上。之後只要你有修改 DAG 裡頭的程式碼，都應該做這個檢查。 這個 DAG 的程式碼雖不長，卻隱含了一些非常重要的概念。 輕鬆排程 with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : ... 靠近 Script 尾端的這行實際上就定義了我們的 DAG 並將它命名為 comic_app_v1 。而此 DAG 的排程（Scheduling）設定如 'start_date': datetime(2100, 1, 1, 0, 0) 代表從西元 2100 年開始第一次執行此 DAG 每次執行之間間隔多久。 'schedule_interval': '@daily' 代表每天執行一次 'retries': 2 則允許 Airflow 在 DAG 失敗時重試 2 次 DAG 失敗後等多久後開始重試（ 'retry_delay': timedelta(minutes=1) 代表等一分鐘） 更多更多 ... 乍看之下沒什麼了不起的，就是些設定。 但如果你有自己從頭實作過資料管道的經驗或者使用過 Cron 排程 ETL，就能體會 Airflow 這樣的「Configuration as Code」有多麽的強大：你只做一些設定（Config），Airflow 就幫你自動建立可靠、失敗時會自動重試的工作流程。 按幾個按鈕就能做出可靠的工作流程，將自動化、失敗重試、相依性管理全部交給 Airflow 這些排程設定為了方便管理，一般都另外定義在 default_args 變數並放在 script 的最上面。 Operator：將實作邏輯跟 DAG 排程分離 最有趣的是我們使用 with 關鍵字來定義一個只屬於 comic_app_v1 DAG 的領域。在這裡頭我們則定義了唯一一個工作 superman_task 處理所有事情（你應該能猜到為何它被這樣命名）： with DAG ( 'comic_app_v1' , ... superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 這段程式碼用白話翻譯的話，就是說： 在 DAG comic_app_v1 裡頭，利用 PythonOperator 建立一個名為 superman_task 的工作，而實際執行這個工作的時候，呼叫 fn_superman 函式。 一個非常重要且需要你搞懂的概念是，現在說的工作（Task），是指那些實際透過程式碼宣告，在 DAG 裡頭被定義出來的工作，如 superman_task 。 前面我們提到，App 概念上本身就包含了多個工作（步驟）： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 這些是「邏輯上」的工作，而在 comic_app_v1 DAG 裡頭，為了方便說明，我們將它們全部包起來，定義成唯一一個 Airflow 工作： superman_task 。（在 App 版本二：模組化 章節裡，我們則會分別為這些「邏輯工作」建立他們自己的 Airflow 工作）。 回到 Opeartor 的話題。在 Airflow 裡頭，DAG 只知道有哪些工作以及這些工作之間的執行順序。而實際上這些工作要怎麼被完成，其實作邏輯則是由各種 Operator 負責。 你可以想像 Opeartors 就是幫我們完成特定種類工作的小幫手，像是一些常見的例子： PythonOperator 執行一個 Python 函式 BashOperator 執行 Bash 指令 S3KeySensor 監測 S3 上的檔案存不存在 SlackAPIPostOperator 送訊息給 Slack ... 要建立一個 DAG 裡的工作（Task）就是依照你想要它完成的特定目標，來選擇合適的 Operator。比方說上面的 superman_task 就是透過 PythonOperator 來執行特定的 Python 函式 fn_superman ，而該函式則把 App 裡頭所有的「邏輯工作」實作了。 PythonOperator 可以說是 Airflow 裡最基本也最強大的 Opeartors 之一。學會使用方法以後，你可以將任何你定義的 Python 函式變成一個 Airflow 工作。 基本的使用方法非常簡單，你只要指定一個可呼叫的 Python 函式給 python_callable 參數以及設定一個工作名稱（task_id）即可： superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 在後面的 Airflow 變數以及 Jinja 模板 章節，我們則會看到如何使用其他 Operator 如 SlackAPIPostOperator 來新增一個可以幫我們送 Slack 訊息的工作。 測試開發 Airflow 工作 你現在應該已經理解 DAG 本身關注的是有哪些工作以及他們的相依性，而不是各個工作的實作邏輯。（雖然在 comic_app_v1 DAG 裡頭只有一個工作所以不存在相依性問題） 我們用 python dags/comic_app_v1.py 確保 DAG 本身沒有語法問題以後，接著就是要確保裡頭每個工作（Task）的執行結果如我們預期。 在 comic_app_v1 DAG 裡頭，我們只有一個工作 superman_task （其透過一個函式 fn_superman 幫我們做所有邏輯上的工作）： def fn_superman (): print ( \"取得使用者的閱讀紀錄\" ) print ( \"去漫畫網站看有沒有新的章節\" ) print ( \"跟紀錄比較，有沒有新連載？\" ) # Murphy's Law accident_occur = time . time () % 2 > 1 if accident_occur : print ( \" \\n 天有不測風雲,人有旦夕禍福\" ) print ( \"工作遇到預期外狀況被中斷 \\n \" ) return new_comic_available = time . time () % 2 > 1 if new_comic_available : print ( \"寄 Slack 通知\" ) print ( \"更新閱讀紀錄\" ) else : print ( \"什麼都不幹，工作順利結束\" ) with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 這樣的設計有什麼優點？ 一般來說 DAG 跟工作是一對多的關係（一個工作流程裡有多個小工作要做）：要讓一個 DAG 順利跑完，理所當然所有工作都要順利執行完畢。但 comic_app_v1 DAG 是個特例，它裡頭只有一個工作，一人吃全家飽。只要測試且確保 superman_task 工作的執行結果如我們預期，就代表 DAG comic_app_v1 能順利完成，簡單易懂！ 我們可以使用 Airflow 的 test 指令來幫我們測試這個工作： airflow test comic_app_v1 superman_task 2018 -08-18 這行指令是讓 Airflow 幫我們測試 comic_app_v1 DAG 裡頭的 superman_task 工作，並假設這個工作是在 2018-08-18 這個日期被執行。在我們的 App 例子中， superman_task 工作的執行結果基本上不會受到執行日期的影響，可以隨便你改。 但想像一個每天 24 點 0 分準備被啟動，從資料庫撈出數據並計算「當天」使用者數目的工作。其 SQL 查詢可能長這樣： SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '{execute_date}' 因為此工作的結果會受到執行日期的影響，在測試的時候，你就得仔細選擇執行日期（execute_date）。 拉回 superman_task 工作的測試。 從上面 fn_superman 函式的程式碼你可能已經注意到，我埋了個小彩蛋，每次執行都會有不同的執行結果。 幸運的話你會得到下面這種： airflow test comic_app_v1 superman_task 2018 -08-01 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 什麼都不幹，工作順利結束 喔耶！這執行結果如我們預期，可以讓 DAG 上線定期執行了！ 不過別高興得太早。多執行幾次看看。如果墨菲定律發生，你會得到失敗的結果： airflow test comic_app_v1 superman_task 2018 -08-01 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 天有不測風雲,人有旦夕禍福 工作遇到預期外狀況被中斷 假設此執行結果不是我們預期的結果，該怎麼辦？ 如果你反應夠快，會說： 「那又怎麼樣？墨菲定律不會每次發生，而且就算遇到而導致工作失敗的話， Airflow 不是會自己幫我們重試嗎？」 的確，這是我們在前面 輕鬆排程 章節提到 Airflow 的強處。畢竟我們這 App 只是在檢查最新連載，不是做什麼很複雜的運算。基本上就算 DAG 裡頭這唯一一個工作 superman_task 失敗了導致整個 DAG 要重跑，Airflow 也可以應付得來。 但問題在於，企業在運行資料管道的時候，常常需要分成很多步驟，某些步驟可能需要龐大的計算資源跟時間（像是將每天使用者使用 App 的幾億筆紀錄做匯總存入資料庫），有些則很輕量（如存取一個外部 API 取得外匯比例）。 現在假設你無視這些不同步驟的性質差異，將它們全部放在一個 fn_superman 函式裡頭並只建立一個 Airflow 工作，當該 Airflow 工作裡頭任何一個輕量的步驟失敗，Airflow 得重跑整個工作，導致所有龐大計算的步驟也得跟著重新執行，重試的時間/計算成本會大到你哭出來。 雞蛋不要放在同個籃子裡。為邏輯上獨立的工作/步驟分別建立 Airflow 工作，可以讓 Airflow 只從失敗的工作開始重新做起。 因此一個比較好的 Airflow DAG 設計模式是為我們 App 裡頭每個邏輯上獨立的工作： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 都分別建立如同 superman_task 的 Airflow 工作，並定義好它們之間的相依性（Dependencies）。而這將是我們下一節的重點。 題外話：你可能會納悶為何我們只測試 superman_task 工作而沒測試整個 comic_app_v1 DAG。當然「一人吃全家飽」是個理由：只要確定 DAG 裡頭唯一一個工作正確運作，我們就能保證此 DAG 沒問題。 事實上還有一個原因： airflow test 指令實際上只能用來測試單一工作，而不能測試整個 DAG。關於 DAG 的測試我們在後面的 Airflow 排程器 章節會詳細說明。 App 版本二：模組化 所以現在我們要做的改善（Refactoring）很簡單： 將 App 邏輯從 comic_app_v1 DAG 中的函式 fn_superman 中拿出來 為 App 的每個步驟分別定義一個 Python 函式 在 DAG 裡頭利用 PythonOperator 建立多個 Airflow 工作並分別呼叫這些函式 定義這些工作的執行順序 版本二的 App 完整的程式碼如下： import time from datetime import datetime , timedelta from airflow import DAG from airflow.operators.python_operator import PythonOperator , BranchPythonOperator from airflow.operators.dummy_operator import DummyOperator from airflow.operators.slack_operator import SlackAPIPostOperator default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } def process_metadata ( mode , ** context ): if mode == 'read' : print ( \"取得使用者的閱讀紀錄\" ) elif mode == 'write' : print ( \"更新閱讀紀錄\" ) def check_comic_info ( ** context ): all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'get_read_history' ) print ( \"去漫畫網站看有沒有新的章節\" ) anything_new = time . time () % 2 > 1 return anything_new , all_comic_info def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' def generate_message ( ** context ): _ , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"產生要寄給 Slack 的訊息內容並存成檔案\" ) with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : get_read_history = PythonOperator ( task_id = 'get_read_history' , python_callable = process_metadata , op_args = [ 'read' ] ) check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) update_read_history = PythonOperator ( task_id = 'update_read_history' , python_callable = process_metadata , op_args = [ 'write' ], provide_context = True ) generate_notification = PythonOperator ( task_id = 'yes_generate_notification' , python_callable = generate_message , provide_context = True ) send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = \"YOUR_SLACK_TOKEN\" , channel = '#comic-notification' , text = \"[{{ ds }}] 海賊王有新番了!\" , icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) do_nothing = DummyOperator ( task_id = 'no_do_nothing' ) # define workflow get_read_history >> check_comic_info >> decide_what_to_do decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing generate_notification >> send_notification >> update_read_history 天啊這可比 comic_app_v1 DAG 的程式碼長了不少！ 不過在你開始懷疑自己適不適合寫 Airflow DAG 之前讓我提醒你一下。就跟我們剛剛上面提到的，實際上這個 comic_app_v2 DAG 的架構從上到下也就分為三個區塊： 用 def 定義負責實作的 Python 函式（們） 在 DAG 利用各種 Operator 定義 DAG 工作（大部分是 PythonOperator ，並使用 python_callable 指定執行步驟 1 定義的函式） 定義這些 DAG 工作的執行順序（Workflow） 回頭再看一遍，有沒有清楚一點了？ 在細看 comic_app_v2 的程式碼前，先讓我們用 Airflow Web UI 研究一下這個 DAG 在做什麼： Airflow Web UI 裡頭的 Graph View 幫我們視覺化 DAG 的工作流程 （ 這個 GIF 展示如何從 Airflow UI 開啟此畫面 ） Airflow 工作寫成英文是為了方便使用 airflow test 指令測試每個工作。 儘管工作名稱都是英文，你應該不會覺得陌生。因為這就是我們 App 的邏輯： 取得使用者的閱讀紀錄（get_read_history） 去漫畫網站看有沒有新的章節（check_comic_info） 跟紀錄比較，有沒有新連載？（new_comic_available） 沒有（no_do_nothing） 有（yes_generate_notification） 寄 Slack 通知（send_notification） 更新閱讀紀錄（update_read_history） 看來這應該不是巧合：） Airflow 排程器 如同當初測試 comic_app_v1 DAG 裡頭的 superman_task 工作一樣，在我們放心讓 Airflow 幫我們排程 comic_app_v2 DAG 以前，應該分別測試裡頭所有工作，確保它們的執行結果如我們預期： airflow test comic_app_v2 get_read_history 2018 -01-01 取得使用者的閱讀紀錄 airflow test comic_app_v2 check_comic_info 2018 -01-01 跟紀錄比較，有沒有新連載？ airflow test comic_app_v2 new_comic_available 2018 -01-01 去漫畫網站看有沒有新的章節 ... 假設我們已經做完所有工作的測試，想讓 comic_app_v2 DAG 開始被 Airflow 排程，除了已經被開啟的 Airflow 網頁伺服器以外，我們需要另外開啟 Airflow 排程器（Scheduler）。 因為目前為止一直在運轉的 Airflow 網頁伺服器只負責： 顯示 DAG 資訊，如工作流程圖、各個 DAG 的運行狀況以及 Logs 讓我們輕鬆地終止/開始 DAG 排程（在有排程器的前提） 而實際要執行 DAG、分配每個工作的運算資源則需要 Airflow 排程器。Airflow 的架構圖能幫助我們理解這件事情： Airflow 架構圖 ：Scheduler 是實際做排程、呼叫 Worker 執行工作的傢伙；我們熟悉的 Webserver 則提供一個 Web UI 讓我們可以輕鬆檢視工作執行時產生的 Logs、DAG 的程式碼以及工作的執行結果；所有資料都被存在 Metadata Database 裡頭。 事不宜遲，讓我們啟動 Airflow 排程器吧！ 現在再打開一個 terminal，進入 airflow-tutorials 資料夾後設定環境： export AIRFLOW_HOME = \" $( pwd ) \" source activate airflow-tutorials 接著啟動排程器： airflow scheduler 到目前為止你應該有 3 個 terminals 各司其職： 用來輸入 airflow 相關指令的 terminal Airflow Webserver Airflow Scheduler 我保証不會再多了。 有了排程器以後，打開 UI，在左邊將 comic_app_v2 DAG 設成「On」後，點擊右邊「Trigger Dag」按鈕可以呼叫排程器馬上開始執行該 DAG。先讓我們按下去以後，再讓我解釋這樣做會發生什麼事情。 在左邊將 DAG 設成「On」後，可以利用右邊「Trigger Dag」按鈕呼叫排程器馬上開始執行該 DAG （ 這個 GIF 展示如何從 Airflow UI 觸發一個 DAG ） 為了避免預料之外的排程，Airflow 所有 DAG 的預設狀態都是暫停的（Paused），也就是上圖中如 comic_app_v1 左邊的「Off」。只有在你將 DAG 的狀態設定成如圖中的 comic_app_v2 的「On」，排程器才會開始為其做排程。 手動觸發 DAG 雖說將一個 DAG 取消暫停（Unpause）可以讓它成為 Airflow 的排程對象，實際上 Airflow 的排程又分兩種方式： 手動觸發（Manual） 常用在測試 DAG 或是有意外發生，需要手動重新執行 DAG 的時候 定期執行（Scheduled） 也就是所謂的「正式上線」。 依照 DAG 的 start_date 及 schedule_interval 設定決定何時執行 現在讓我們先專注在手動觸發。 當然你也可以在不透過 Web UI 的情況下，直接利用 terminal 取消暫停一個 DAG 並觸發它： airflow unpause comic_app_v2 airflow trigger_dag comic_app_v2 理論上我們剛剛手動觸發的 comic_app_v2 應該已經跑完了。重新整理你應該會看到 Airflow UI 顯示 DAG 已被成功執行的畫面： 從 Web UI 我們可以清楚地看到剛剛手動觸發的 comic_app_v2 DAG 已經被 Airflow 排程器拿去執行，產生一個新的 DAG Run 並成功執行。DAG 跟 DAG Run 的差異在於前者只是個定義好的工作流程，後者則是該 DAG 在某個時間點實際被排程器拿去執行（Run）過後的結果，會有一個執行日期（execute_date）。 接著點擊右邊 Links 中長得像太陽的 Graph View 按鈕後就可以看到這個 DAG Run 的執行狀況： 將游標放在右邊的「Success」狀態按鈕上可以顯示此 DAG Run 中被成功執行的工作（圖內的工作從左到右被執行） 注意圖中 DAG Run 的 ID： manual_2018-08-19... 表示這是一個在 2018-08-19 被手動觸發的 DAG Run。 我們可以清楚地看到這個 DAG Run 完美地模擬了我們 App 在檢查到新連載情報時送 Slack 訊息給我們的情境。我甚至收到一個 Slack 訊息： comic_app_v2 DAG 如果發現有新連載就會寄一個罐頭 Slack 訊息，包含 DAG 執行日期。因為我是在 2018-08-19 當天手動觸發此 DAG，因此日期即為 2018-08-19。後面我們會看到如何客製化 Slack 訊息內容。 定義工作流程 要在 DAG 裡頭定義出如上圖的工作流程也非常的直觀，讓我們參考這兩個工作： yes_generate_notification send_notification 它們在 dags/comic_app_v2.py 裡頭被這樣定義（節錄最重要的部分）： with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... generate_notification = PythonOperator ( task_id = 'yes_generate_notification' , ... ) send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , ... ) # define workflow generate_notification >> send_notification >> ... 你可以發現在 comic_app_v2 DAG 裡，我們分別定義好這兩個工作以後，在最下面用 >> 語法告訴 Airflow 這兩個工作的相依性： yes_generate_notification 工作要在 send_notification 之前執行 另外眼尖的讀者會發現，Python 變數名稱 generate_notification 跟實際上的工作名稱（task_id） yes_generate_notification 並不一致。我們將實際的工作 PythonOperator 命名為 generate_notification ，只是為了後面在定義工作流程的時候好提到它。參考下面的程式碼： with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... task1 = PythonOperator ( task_id = 'yes_generate_notification' , ... ) task2 = SlackAPIPostOperator ( task_id = 'send_notification' , ... ) # define workflow task1 >> task2 >> ... 這段程式碼跟上一段程式碼在定義工作流程上有一模一樣的效果，只是後者的 naming convention 在定義工作流程的時候比較易懂。 雖然要多打幾個字，為了其他 DS/DE 以及未來的自己，一般推薦 Python 變數名稱取跟 task_id 類似的名字。 針對其他工作，我們也是用相同語法將它們串起來： ... decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) ... get_read_history >> check_comic_info >> decide_what_to_do decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing generate_notification >> send_notification >> update_read_history 然後我們就得到前面看過的工作流程圖了： 你也可以回到 App 版本二：模組化 章節，確認 comic_app_v2 完整的程式碼後再利用左邊的傳送門回來，我等你。 Airflow 變數以及 Jinja 模板 現在你應該已經了解如何使用 PythonOperator 建立一個新的工作，並利用 >> 語法定義 Airflow 的工作流程（DAG）了。我們也實際觸發 comic_app_v2 DAG 讓 Airflow 排程器幫我們排程，最後收到一個 Slack 訊息。 現在讓我們仔細研究一下負責寄 Slack 訊息的工作，也就是下圖的 send_notificiation ： 依照 Opeartor 種類不同，工作在 Web UI 上顯示的背景顏色也有所不同，方便區分。 你會發現它的顏色跟其他工作不一樣，這是因為它並不是一個 PythonOperator ，而是一個 SlackAPIPostOperator 。由此 Operator 定義的工作並不會呼叫一個 Python 函式，而是直接呼叫 Slack API 來傳送訊息。下面是我們在當初落落長的 comic_app_v2 DAG 裡頭定義的 send_notificiation ： send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = \"YOUR_SLACK_TOKEN\" , channel = '#comic-notification' , text = \"[{{ ds }}] 海賊王有新番了!\" , icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) 注意 text 參數的值。 {{ ds }} 實際上是 Jinja 語法，它允許我們將 Python 變數渲染（Render）到字串裡頭，動態地產生文本。這就像是我們有個變數 ds ，然後利用 format 語法一樣： text = \"[{ds}] 海賊王有新番了!\".format(ds=ds) 而這邊的重點是 Airflow 在執行一個 DAG 的時候會提供一些預設的 環境變數 供我們使用，像是： ds ：代表 DAG Run 的執行日期（execute_date），以 YYYY-MM-DD 形式表現 yesterday_ds ：DAG Run 的執行日期的前一天，以 YYYY-MM-DD 形式表現 tomorrow_ds ：DAG Run 的執行日期的後一天，以 YYYY-MM-DD 形式表現 ... 而因為我們在 2018-08-19 的時候，利用下面這個指令手動觸發 comic_app_v2 DAG： airflow trigger_dag comic_app_v2 Airflow 會將實際執行該 DAG 的日期設定為執行日期（execute_date）。因此 ds 即為 2018-08-19 ， SlackAPIPostOperator 裡頭的 \"[{{ ds }}] 海賊王有新番了!\" 就會被渲染成 [2018-08-19] 海賊王有新番了! 。 最後我們就得到這個 Slack 訊息： 現在你也了解使用 Jinja 語法可以動態地調整每次 DAG 運行的邏輯以及執行結果。讓我們實際將 comic_app_v2 DAG 丟上線試試看吧！ 執行日期：排程最重要的概念 經過前面的幾個章節，我們已經對 comic_app_v2 DAG 的測試及開發下了不少功夫： 利用 airflow test 指令分別測試每個 Airflow 工作執行如預期 python dags/comic_app_v2.py 確保 DAG 定義無誤 使用 Web UI 點擊「 Trigger Dag 」按鈕或是透過 airflow trigger 來手動觸發 DAG 確認結果 這些都是將一個 DAG 正式上線前必須完成的步驟。在這些測試都完成以後，是時候將我們的 comic_app_v2 DAG 交給 Airflow 排程器，讓 Airflow 幫我們每天執行這個 DAG 了！ 在 手動觸發 DAG 章節我們有看到，要讓 Airflow 排程器開始排程一個 DAG，首先要終止暫停（Unpause）該 DAG。而為何當時 Airflow 沒有在我們 comic_app_v2 一終止暫停 就開始自動排程，而要等到我們手動觸發呢？ 這是因為當時的 comic_app_v2 的排程設定如下： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 'start_date': datetime(2100, 1, 1, 0, 0) 代表我們希望 comic_app_v2 DAG 的第一個執行日期（execute_date）為西元 2100 年 1 月 1 號 0 點。 你可能覺得為何要把話說得那麼複雜，就說： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 不就好了嗎？ 不這麼說的原因，就是因為上面的理解是錯的。事實上這是很多人在利用 Airflow 排程時最容易搞錯的 概念 之一，值得花點篇幅徹底搞清楚。 假如西元 2100 年我們架的 Airflow 排程器還在運作的話，它會在： start_date 2100 年 1 月 1 號 0 點 0 分 + 1 * schedule_interval = 2100 年 1 月 1 號 0 點 0 分 + 1 * @daily = 2100 年 1 月 1 號 0 點 0 分 + 1 * 24 小時 = 2100 年 1 月 2 號 0 點 0 分 也就是 2100 年 1 月 2 號 0 點 0 分的時候，將 comic_app_v2 DAG 拿出來做第一次執行，而該 DAG Run 的執行日期為 2100 年 1 月 1 號 0 點 0 分。 我知道你現在可能滿臉黑人問號，但讓我們好好想一想這到底是怎麼一回事。 要理解為何我們一開始的猜想： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 是非常矛盾的，讓我們做個我最愛的假想實驗。還記得在 測試開發 Airflow 工作 章節提到的 SQL 查詢嗎？ SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '{execute_date}' 現在假設我們給這個工作跟 comic_app_v2 一模一樣的排程設定： 'start_date': datetime(2100, 1, 1, 0, 0) 'schedule_interval': '@daily' 根據本章節一開始的敘述，這個 DAG 的第一個執行日期（execute_date）為 2100-01-01 。而按照我們在 Airflow 變數以及 Jinja 模板 章節所說的，此 SQL 查詢裡頭的 Jinja 語法會被渲染成： SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '2100-01-01' 接著假設我們一開始的猜想： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 是對的話，該 SQL 查詢會取回什麼資料？ 答案是什麼都沒有。 因為如果這猜想是對的話，這個 SQL 查詢工作會馬上在西元 2100 年 1 月 1 號的 0 點，想辦法去把西元 2100 年 1 月 1 號整天的使用者資料全部撈出來。而因為此 SQL 查詢執行時， 1 月 1 號才剛開始，這個查詢不會取得任何資料。 很明顯哪裡出了差錯了。 而如果照我剛剛解釋的版本，就會顯得合理許多： 在 2100 年 1 月 2 號 0 點的時候，以下的 SQL 查詢會被執行 SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '2100-01-01' 這代表我們在 1 月 1 號 23 點 59 分結束以後，也就是 1 月 2 號 0 點的時候，將 1 月 1 號所有的使用者資料做彙總。 一般而言，Airflow 會在 start_date 加上一個 schedule_interval 之後開始 第一次執行某個 DAG ，而該 DAG Run 的 execute_date 為 start_date 。這樣的設計就是為了避免像是上面那個 SQL 查詢在當天才剛開始的時候就想要搜集該天所有資料的窘境。 Airflow 擅長的是管理那些允許「事件發生時間」跟「實際數據處理時間」有落差的批次工作。因此 Airflow 都會在 start_date 加上 schedule_interval 長度的時間過完 以後 ，才開始處理發生在 start_date 到 start_date + schedule_interval 之間的資料。 再換句話說， 一個 DAG Run 中的執行日期，只等於它「負責」的日期，不等於它實際被 Airflow 排程器執行的日期。一個被自動排程且執行日期為 dt 的 DAG Run，實際上是在 dt + schedule_period 後被 Airflow 執行。 我們換了好幾種說法，希望你能百分之百地掌握這個 Airflow 排程的概念，因為這實在太重要了。 有了這章節的排程概念以後，我們可以正式開始排程 comic_app_v2 DAG 了！ 正式排程 經過上一章節排程概念的洗禮，想必你還記得 comic_app_v2 DAG 的開始排程日期（start_date）是遙遠的西元 2100 年 1 月 1 號： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 作者目前撰寫這段落的日期為西元 2018 年 8 月 20 號，所以大概還要再等 82 年，而且我啟動的 Airflow 排程器還活著，這個 DAG 才會被第一次執行。我們可等不了那麼久。 在完全地理解上一章 執行日期：排程最重要的概念 所提到的概念以後，你可能會說： 「那我們可以把 start_date 設為 2018 年 8 月 20 號，並維持 schedule_interval 為一天，這樣等到 8 月 21 號 0 點的時候，這個 DAG 就會被執行，然後我們就知道它 work 不 work 了！」 好傢伙（好姑娘？），我給你 100 分！ 這句話已經抓到 Airflow 排程的精髓中的精髓，只不過別誤會，我趕時間。何不讓我們當個時空旅人，將 start_date 設為 8 月 20 號以前的日期，比方說 8 月 17 號？ 畢竟我們在上一章提到： 一個 DAG Run 中的執行日期，只等於它「負責」的日期，不等於它實際被 Airflow 排程器執行的日期。 將 start_date 設為今天（8 月 20 號） 以前 的日期，並啟動 Airflow 排程器的話，就會讓 Airflow 排程器馬上開始排程執行日期為 start_date 的 DAG Run，並且一直執行到最新的 DAG Run 為止。 Airflow 排程器彷彿就像台時光機器，幫我們排程那些執行日期在過去的 DAG Run，重建過去。 所以現在讓我修改 comic_app_v2 DAG 的程式碼以排程「過去」的 DAG Run： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2018 , 8 , 17 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 保持好習慣，修改完程式碼以後用 Python 確認 DAG 沒語法錯誤： python dags/comic_app_v2.py 通常 Airflow 沒多久就會重新載入最新的程式碼。如果你懷疑程式碼沒有被更新，可以點擊 Airflow UI 首頁中 comic_app_v2 DAG 最右邊 Links 裡頭的「Refresh」按鈕。 問題時間。 將 comic_app_v2 DAG 的 start_date 設定成 2018 年 8 月 17 號以後，在作者撰文的 8 月 20 號晚間 10 點為止， Airflow 會排程幾次 DAG Runs？它們分別的執行日期為何？花個幾秒鐘思考，確定你知道答案。（schedule_interval 一樣為 @daily ） 滴答滴答，你能在我們的時光機完成工作之前想出答案嗎？ 答案揭曉，Airflow 排程器總共排程三個 DAG Runs，他們的執行日期分別為： 2018-08-17 2018-08-18 2018-08-19 8 月 20 號的 DAG Run 則要等到 8 月 21 號 0 點才會被執行。 喝杯水重新載入 UI，我們可以從 Airflow UI 裡頭確認 comic_app_v2 DAG 總共有 4 個成功的 DAG Runs： 除了第一個 DAG Run 是我們之前手動觸發以外（你可以從它的 Run Id 以及最右邊的 External Trigger 看出），其他三個都是 Airflow 排程器實際排程並執行的結果（一樣你可以從它們的 Run Id 看出端倪）。 同時我的 Slack 作響。我們可以看到儘管執行日期相異，三個被排程的 DAG Runs 按照順序通知我有新番。 嗯 .. 海賊王一週出一次，想必其中有幾個是 fake news。 不管如何，我們在這章節成功讓 Airflow 排程器從好幾天前開始自動排程 comic_app_v2 DAG 並確認結果成功！ 如果我不將 Airflow 排程器關掉的話，之後每天的 0 點（UTC）它都會幫我執行 comic_app_v2 DAG。沒有意外的話，或許 Airflow 排程器可以幫我們持續排程此 DAG 到西元 2100 年，希望到時海賊王已經完結，不用叫孫子燒給我了。 App 版本三：填填樂 目前為止，本文為了讓你能專注在理解 Airflow 及工作流程的核心概念（而非個別工作的實作細節），以 print() 代替我們 App 的實作邏輯。 在此章節，我們則會一窺實作所有邏輯的 comic_app_v3 DAG，也就是實現本文開頭展示的 App 的程式碼。 但為何說「一窺」呢？ 因為 comic_app_v3 DAG 為了處理 JSON 檔案、利用 Selenium 存取網頁等事情，其程式碼變得比只用 print() 的 comic_app_v2 DAG 要長得多，且其程式碼很大一部份已經不直接跟 Airflow 相關了。 我相信大部分的讀者是為了學習 Airflow 而來，而不是看我東 try 西 try 來實作這個 App。當然，如果你有興趣且想要練習如何建立一個自己的漫畫連載 App，你可以嘗試將實作邏輯填入到 comic_app_v2 DAG 裡頭的各個 Python 函式即可，或者直接執行我已經實作好所有邏輯的 comic_app_v3 DAG，這個我們在後面的 如何建立你自己的連載通知 App（懶人法） 章節會有詳細講解。 填填樂：以 comic_app_v2 建立好的工作流程為基礎，實作每個工作的邏輯就像是填空題一般，將邏輯填入對應的 Python 函式就好。（comic_app_v3 也是從 comic_app_v2 為基礎開發的，工作流程一模一樣） 在這章節，我想跟你分享一些在實作 comic_app_v3 DAG 時用到的 Airflow 知識及技巧。 重複利用 Python 函式 在 App 版本二：模組化 章節我們看到，大部分的 Airflow 工作都是由一個 PythonOperator 所定義，而每個 PythonOperator 分別呼叫不同的 Python 函式。但在 comic_app_v3 DAG 裡頭，我們只利用一個 Python 函式 process_metadata 專門負責讀 / 寫使用者的閱讀紀錄： def process_metadata ( mode , ** context ): if mode == 'read' : ... elif mode == 'write' : ... with DAG ( 'comic_app_v3' , default_args = default_args ) as dag : get_read_history = PythonOperator ( task_id = 'get_read_history' , python_callable = process_metadata , op_args = [ 'read' ], provide_context = True ) ... update_read_history = PythonOperator ( task_id = 'update_read_history' , python_callable = process_metadata , op_args = [ 'write' ], provide_context = True ) 你會發現上面兩個 Airflow 工作的 python_callable 都呼叫 process_metadata ，因為它們做類似的事情： get_read_history 負責讀取閱讀紀錄 update_read_history 負責更新閱讀紀錄 而這兩個工作則利用不同的 op_args 來使用 process_metadata 函式的不同功能。這樣的好處是我們不需要為每個類似的 PythonOperator 都分別建立一個新的 Python 函式，而是利用參數 op_args 來改變同個 Python 函式的執行結果。 當然，傳遞參數給 Python 函式這件事情本身就是很常見，這時候 op_args 就會派上用場。 Xcom：工作之間的訊息交換 Xcom（Cross Communication） 是 Airflow 工作之間交換訊息的方式。一個被 PythonOperator 呼叫的 Python 函式所回傳（return）的值，都可以被其他 Airflow 工作透過 Xcom 存取： def check_comic_info ( ** context ): print ( \"檢查有無新連載\" ) ... return anything_new , all_comic_info def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' ... with DAG ( 'comic_app_v3' , ... ... check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) 你可以看到最底下的 check_comic_info 工作呼叫上方的 check_comic_info 函式，而該函式回傳 anything_new, all_comic_info 。 接著 decide_what_to_do 函式則利用以下語法來取得該結果： anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) 下游工作可以透過這樣的方式取得上游工作的執行結果，來決定接下來要做的任務。 值得注意的是 XCom 的所有資料在 pickle 之後會被存到 Airflow 的 Metadata Database（通常是 MySQL）裡頭，因此不適合交換太大的數據（例：100 萬行的 Pandas DataFrame），而適合用在交換 Metadata。 def check_comic_info ( ** context ): ... 裡頭的 **context 的語法是為了取得 Airflow 在執行工作時產生的 環境變數 ，其中就包含 XCom。除了要在 Python 函式設置 **context 以外，我們還必須將 PythonOperator 的 provide_context 參數設置為 True ，Airflow 才會把環境變數傳給該工作： check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) 在工作流程內加入條件分支 有時候我們會想要在工作流程裡頭加入分支，當某條件符合的時候執行這個分支，當不符合的時候執行別的分支。 比方說我們的 App 就含有這樣的邏輯： 利用 BranchPythonOperator 實現 Airflow DAG 裡的條件分支 圖中的 check_comic_info 「上游」工作會去漫畫網頁檢查有沒有新的連載，依照結果的不同，我們希望不同分支被執行： 如果有的話，執行上面分支的 yes_generate_notification 「下游」工作 沒有的話，則執行下面分支的 no_do_nothing 「下游」工作 要在 Airflow 裡頭實現這樣的邏輯，可以在上下游工作「之間」新增一個 BranchPythonOperater （如圖中的 new_comic_available 工作）： 砍掉原上游工作跟下游工作之間的 >> 將原上游工作 >> 該 BranchPythonOperator 工作 將該 BranchPythonOperator 工作 >> 原下游工作 資料工程很大一部份的工作就是在建立資料管道/工作流程，接個水管合情合理對吧？ from airflow.operators.python_operator BranchPythonOperator def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' ... with DAG ( 'comic_app_v3' , default_args = default_args ) as dag : ... decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) generate_notification = PythonOperator ( ... ) do_nothing = DummyOperator ( task_id = 'no_do_nothing' ) decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing 而 BranchPythonOperator 一樣會呼叫一個 Python 函式（上例的 decide_what_to_do 函式），由該函式決定到底最後哪個下游工作會被執行。基本上該函式會依照實際情況決定哪個下游工作被執行，並將該下游工作的 task_id 回傳。 而因為在這個例子中，我們希望依照上游工作 check_comic_info 回傳的一個布林值 anything_new 來決定要執行哪個下游工作，因此可以使用 xcom_pull 取得該結果以後回傳要執行的下游工作 ID task_id 。 好啦，這就是我想跟你分享在實作 comic_app_v3 DAG 時的幾個實用技巧，希望對你上手 Airflow 有所幫助。 接下來我們將針對那些想要建立自己的連載通知 App 的你，提供一個快速起手指南。 不過如果你現在沒有打算做這件事情的話，可以放心跳到最後面的 結語 。 如何建立你自己的連載通知 App（懶人法） 此章節提供一個懶人指南，讓那些想要建立自己的 App 的你，在（幾乎）不需要改變 comic_app_v3 程式碼的前提下完成這件事情。 如同我們在 建置 Airflow 環境 提到的，首先你當然得先把跟這篇文章相關的 Github Repo 複製下來： git clone https://github.com/leemengtaiwan/airflow-tutorials.git cd airflow-tutorials 如果你在之前就有複製 Repo 下來跟著走，你只需要再另外安裝 Selenium 。Selenium 是一個自動化網頁測試的工具，在這個 App 裡頭被我們用來當網路爬蟲，去漫畫網站看連載資訊。 接著啟動目前為止 Airflow 一直在使用的 Anaconda 環境，然後安裝 Selenium： source activate airflow-tutorials conda install -c conda-forge selenium 如果你之前沒有建置任何環境，可以利用 Repo 裡頭的 environment.yaml 從頭安裝 Airflow 以及 Selenium： conda env create -n airflow-tutorials -f environment.yaml source activate airflow-tutorials 在這個 App 裡頭，要讓 Selenium 正常運作，你還需要 Chrome Driver 。下載最新版本以後把它放在你的 $PATH 讀得到的地方。Mac 使用者的話可以放到像是 /usr/local/bin 資料夾下面。如果還是不懂可以查看 這裡的 Chrome Driver 安裝教學 。 環境設定好以後，你會需要一個新的 Slack App 來送訊息到你的 Workspace。建立一個新的 Slack App，給予它寫訊息的權限以後，安裝到你自己的 Workspace。這時候你應該會得到一個開頭為 xoxp- 的 Slack Token。將該 Token 複製下來，打開 airflow-tutorials 資料夾底下的 data/credentials/slack.json 。 將你的 Token 複製貼上如： { \"token\" : \"xoxp-.....\" } 搞定網路爬蟲以及 Slack 認證以後，你需要改變 comic_app_v3.py 裡頭的一行程式碼，以讓 Airflow 送訊息到你 Workspace 底下指定的頻道（channel）： send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = get_token (), channel = '#comic-notification' , text = get_message_text (), icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) 將上述的 channel='#comic-notification' 改成你自己的頻道，如 channel='#my-new-channel' 。 接著你會需要一個正常運作的 Airflow 排程器。啟動方法參考 Airflow 排程器 章節。 在 Airflow 排程器、Selenium 以及 Slack 都就緒以後，你可以直接手動觸發 comic_app_v3 DAG 來測試 App 的第一則訊息。如同我們在 手動觸發 DAG 章節提到的，你可以透過 Web UI 或者 terminal 來終止暫停（Unpause）並手動觸發一個 DAG： airflow unpause comic_app_v3 airflow trigger_dag comic_app_v3 一切順利的話，幾秒鐘之後，你會在自己的 Slack Workspace 及 channel 底下收到這個測試訊息： 圖中的 channel 會隨著你實際的設定改變 目前 comic_app_v3 DAG 將使用者的閱讀紀錄儲存在 data/comic.json 裡頭，底下則是為了產生上面這個 Slack 訊息的假閱讀紀錄： { \"1152\" : { \"name\" : \"海賊王\" , \"previous_chapter_num\" : 900 } } 上頭 comic.json 裡頭，海賊王的 \"1152\" 就代表該漫畫主頁在動漫狂的連結中的數字（1152.html） 目前此 App 只能從 動漫狂 （歡迎你丟 PR 改善！）找最新的漫畫連載。為了新增你自己的漫畫，你需要找出該漫畫主頁在動漫狂的連結，將連結中的數字如上述的例子一樣新增在 data/comic.json 裡頭。假設你想開始關注「進擊的巨人」，然後最近看到 100 話的話，可以把 data/comic.json 改成這樣： { \"1221\" : { \"name\" : \"進擊的巨人\" , \"previous_chapter_num\" : 100 , } } 這樣一來， comic_app_v3 DAG 就會用該數字去「進擊的巨人」的頁面，幫你查看有沒有最新的連載。當然你也可以像我一樣，在 comic.json 裡追加多個漫畫： { \"1152\" : { \"name\" : \"海賊王\" , \"previous_chapter_num\" : 911 }, \"1221\" : { \"name\" : \"進擊的巨人\" , \"previous_chapter_num\" : 107 }, \"4485\" : { \"name\" : \"西遊\" , \"previous_chapter_num\" : 152 }, \"1121\" : { \"name\" : \"浪人劍客\" , \"previous_chapter_num\" : 327 }, \"1122\" : { \"name\" : \"王者天下\" , \"previous_chapter_num\" : 565 } ... } 修改完 comic.json ，最後你會想要修改 comic_app_v3.py 裡頭的排程設定： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } 將 start_date 改成你想要他它開始的日期，接著 Airflow 排程器就會每天幫你執行 comic_app_v3 DAG 並查看最新連載。搞定收工！ 結語 首先，由衷感謝你花了那麼多寶貴時間與力氣跟隨著我們的 Airflow 冒險。 回顧一下，這一路上你已經學會不少資料工程相關的知識以及 Airflow 的開發技巧： 了解工作流程、上下游工作、相依性的概念以及其與 Airflow DAG 的關係 模組化工作流程的重要性 了解如何利用 PythonOperator 建立一個 Airflow 工作並呼叫自定義 Python 函式 利用 airflow test 指令以及 Web UI 測試 Airflow 工作以及 DAG 了解如何利用 Python 定義一個工作流程以及決定工作間的相依性 利用 Web UI 及 terminal 手動觸發 DAG 並確認執行結果 了解 Airflow 排程概念（如執行日期）並實際讓工作流程上線（ comic_app_v2 ） 了解一些 Airflow 開發時的技巧，如建立條件分支以及使用各種不同的 Operators 建立工作 先給自己鼓個掌吧！ 如同我在文章開頭所述： 這是一篇當初我在入門資料工程以及 Airflow 時希望有人能為我寫好的文章。 當時的我找不到這篇文章，而現在我自己寫了這篇文章。 希望這篇文章能幫助到跟過去的我一樣，正在嘗試學習資料工程以及 Airflow 的你。 雖然使用 Airflow 來實作本篇的漫畫連載 App 可能是一個殺雞用牛刀的例子，但我希望你能參考本文的 App 例子，開始思考如何用本文學到的知識，去實際解決、自動化你自身或是所在企業的數據問題。 儘管這篇的 Airflow 故事即將進入尾聲，你的 Airflow 之旅才剛剛展開。 Keep learning and happy Airflowing！","tags":"Miscellaneous","url":"https://leemeng.tw/a-story-about-airflow-and-data-engineering-using-how-to-use-python-to-catch-up-with-latest-comics-as-an-example.html"},{"title":"資料科學文摘 Vol.3 Pandas、Docker 以及數據時代的反思","text":"不同於上週的 文摘 Vol.2 產品理解以及 DS / DE 之路 ，這週的選文比較技術以及實作導向。本週將導讀 3 篇使用 Python 以及 Pandas 的文章，並鼓勵讀者實際動手學習。我們也會看到如何使用 Docker 來讓資料科學變得更簡單，並提供一個有趣的貓咪圖片辨識 App 給有興趣的讀者參考。最後，讓我們分別看看哈佛商業評論以及美國前首席資料科學家 DJ Patil 談談如何讓資料科學在企業內普及，以及數據時代我們面臨的各種道德議題。 本週閱讀清單 Pandas、Python How to Master Your Skills for Pandas? How to rewrite your SQL queries in Pandas, and more Learn Functional Python in 10 Minutes Docker Docker for Data Scientists Cat Recognizer: A flask app showcasing how to recognize cats using Tensorflow 數據時代的反思 The Democratization of Data Science Data's day of reckoning How to Master Your Skills for Pandas? Python 裡頭最著名的資料處理 library 非 Pandas 莫屬了。 這篇文章 使用互動式的環境，列出挺完整的 Pandas 指令讓讀者可以邊參考 sample code 邊自己動手玩玩看。 其中包含各種利用 Series 以及 Dataframe 兩種 Pandas 常見的資料格式來對數據進行各種操作，適合沒碰過 Pandas 的新手以及想要重新 refresh 語法的人。 How to rewrite your SQL queries in Pandas, and more 提供常見的 SQL 查詢以及其對應的 Pandas 寫法。一個有效率的資料科學家通常需要 SQL 及 pandas 兼具。雖然這篇一開始的目標讀者是那些已經熟悉 SQL 並打算使用 Pandas 的讀者，我認為熟悉 Pandas 但還不了解 SQL 的同學們也能從這篇學到點東西。 這篇適合至少懂 Python 或是 SQL 並想學習另外一個語言的讀者。如果你想要深入了解 SQL 或是其與 Python 之間的差異，你可以看看我之前寫的 為何資料科學家需要學習 SQL 。 Learn Functional Python in 10 Minutes 這篇 Hackernon 文章 則簡單介紹 Functional Programming 在 Python 可以如何被實作，函式（function）是怎麼被視為 Python 的第一公民以及我們能如何活用函式如 Map、Filter 函式。 如果你剛起步，想要有效率地學習 Python 的話，我建議可以從 List comprehension 開始學起。 一個簡單的例子是假設我們想從一個 List 中取得大於 50 的數字： l = [ 5 , - 3 , 100 , 70 , 2 ] larger_than_50 = [ e for e in l if e > 50 ] print ( larger_than_50 ) [100, 70] 文章的後半段則透過 The Zen of Python （Python 的禪學）來說明為何使用 List comprehension 會比使用傳統 Functional Programming 中的 Map、Filter 函式來得簡單。 Python 有一個著名的彩蛋，你可以利用 import this 來顯示 The Zen of Python，它提供使用 Python 的人一個簡單的開發準則，具體如下： import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Docker for Data Scientists 很簡單地說明常見的 Docker 術語以及使用 Docker 可以為資料科學家帶來的好處： 節省建置開發 / 分析環境所需的時間 增加可重現性（Reproducibility） 抽象化作業系統（OS）的概念，再也沒有只能在 Mac 跑而不能在 Windows 跑的問題 這篇提供非常初級的指令來開始在本機環境使用 Docker，可以嘗試看看。 在 Smartnews 我則是使用 Amazon Elastic Container Service 來快速部署一些資料科學家們常會用到的分析工具，如大家的好朋友 Jupyter Hub 、Airbnb 開發的 BI 工具 Superset 。之後有機會會另外撰文分享經驗。 Try It Yourself Docker 讓我們可以快速重現其他人的分析環境或者是有趣的 application。如果你想馬上感受 Docker 的威力，可以看看我之前利用 Tensorflow 以及 Flask 實作的一個貓咪圖片辨識的 Github repo （feat. CNC ）： Cat Recognizer ：利用 Tensorflow, Flask 實作 App 並使用 Docker 快速與他人分享成果 雖然 Github repo 上也有教學指南，想要最快速地在你的電腦上使用這個 App 的話，下載 Docker 並開啟 Daemon 後，使用命令列輸入以下指令： docker pull leemeng/cat docker run -it -p 2468 :5000 leemeng/cat 接著在瀏覽器輸入 localhost:2468 應該就能開始使用了。如果你想多了解點 Docker，可以參考我寫的 給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上） 。 不過現在讓我們繼續看剩下的 2 篇好文章：） The Democratization of Data Science 哈佛商業評論（Harvard Business Review, HBR） 在這篇文章裏頭敘述為何不只是針對資料科學家，提升所有人的「資料素養」對一個企業來說是一件非常重要的事情。 最明顯的優點是可以讓數據團隊專注在： 解決更高層次的企業問題 建立分析工具以加速所有部門的數據分析 而不是處理每個部門的「資料瑣事」。 這個議題並非只跟企業的管理階層相關。對一個資料科學家來說，想辦法利用資料工程（Data Engineering）等方式來自動化如「建立簡單儀表板」的工作，並教導各個部門實際的使用方式，可以讓你一勞永逸，避免永遠在處理非常瑣碎的「資料瑣事」，專著在更大的目標。 你不會因為自己不是會計師就不遵守專案預算；你也不會因為不是資料科學家就不提升資料素養。 Data's day of reckoning 生活在數據驅動時代的我們或許都能感受到世界變化的快速。 美國前首席資料科學家 DJ Patil 認為不管是資料科學、機器學習還是人工智慧領域，「道德倫理」以及「安全隱私」議題都應該越來越被重視。 電腦科學（Computer Science）時代最著名的安全議題非 SQL 注入 （SQL Injection） 莫屬了。如同這個議題，在數據驅動時代，我們也會面臨類似道德以及數據保護的議題，像是人工智慧模型產生具有偏見的預測、以及最近的 GDPR 等等。 在教育方面，DJ Patil 認為我們應該教育下一代在數據處理時，應該遵守的準則並將其被納入課綱；以數據驅動的公司則需要將這些想法都納入企業文化，在招聘資料科學家的時候，除了考慮他 / 她的分析能力以外，也要評估道德倫理的部分。 身為一個資料科學家，除了技術層面的提升，也應該稍微了解這些議題。 We can build a future we want to live in, or we can build a nightmare. The choice is up to us. 結語 Pandas、SQL、Docker、資料素養的培養以及數據時代的道德倫理問題等等，這週我們也看了不少資料科學相關的文章，希望你有從這篇文章裡頭學到點東西。 雖然因為篇幅關係沒辦法把所有實際的 Python 指令列在這邊，我希望透過摘要的方式能讓沒時間的你也能學習、初步地了解資料科學並進一步發現自己有興趣的地方鑽研。 有時間的話我推薦實際閱讀這些文章（當然也可以閱讀其他你自己收藏的文章，也歡迎分享），也可以試試看我寫的 Cat Recognizer 並留言跟我說說你的想法。 之後一樣會定期更新，希望收到第一手消息的話可以點擊下面的訂閱。另外如果你有其他會對這篇文章有興趣的朋友，也請幫忙分享給他 / 她：） That's it for this week, stay tuned and happy data science!","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-3.html"},{"title":"資料科學文摘 Vol.2 產品理解以及 DS / DE 之路","text":"這週一樣會透過導讀一些優質文章，讓讀者了解 3 個問題： 為何一個專業的資料科學家需要具備「產品理解」？ 何謂「顧客流失分析」？ 我們該如何使用 Python（XGBoost）來建立簡單的預測模型以改善產品？ 此外，我們也將簡單介紹在資料科學領域中逐漸崛起的「資料工程師」，其職責以及專精領域跟「資料科學家」有何不同。 最後也會分享一些與資料科學家/資料工程師相關的文章。 後文為了減少累贅，可能會穿插以下縮寫： 資料科學家 = D ata S cientist = DS 資料工程師 = D ata E ngineer = DE 另外有興趣了解此文摘緣由的讀者可以參考前篇： 資料科學文摘 Vol.1 AutoML、Airflow 及 DAU 。 讓我們開始吧！ 本週閱讀清單 產品理解 Why Data Scientists Must Focus on Developing Product Sense Product Scientist @ Medium Python、客戶流失預測 Introduction to Churn Prediction in Python DS / DE 相關 Data engineering: A quick and simple definition How To Become A Data Scientist in 12 Months Infographic – 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them 就跟以往一樣，儘管下文會依照此順序列出文章與摘要，你仍然點擊上面的任意門，從最有興趣的文章開始閱讀：） Why Data Scientists Must Focus on Developing Product Sense 作者闡述為何產品理解（Product Sense）對一個 DS 很重要，適合新手 DS 閱讀參考。 通常在企業裡頭，一個資料科學家要發揮最大的影響力，就是透過手上的資料，產生可執行的洞見（Actionable Insight），進而影響產品（Product）的發展方向。不管我們做了多少分析、多少層的神經網路模型，或是產生多少特徵值（Features），如果最後這些產物沒有對產品的發展有任何影響，一切都白搭。 我們可以透過深刻地了解自家產品（比方說使用自家的 App）、暸解競爭對手、與使用者直接溝通等方式，來培養「產品理解」。有了產品理解以後，甚至可以反過來幫助我們做特徵工程（Feature Engineering），知道在建立預測模型時什麼特徵會是重要的；還能培養從資料看不出來的敏銳直覺（intuition）。 理解產品的 DS 能同時從資料看出的趨勢以及業界直覺來改善產品並解決人們問題。 在 SmartNews ，資料科學團隊也是在產品部門之下，與此有異曲同工之妙：） Product Scientist @ Medium Medium 的人說明他們在找的 Product Scientist 應該要有什麼特質：簡單來說就是有「產品理解」的 DS，能將資料轉換成更好的產品的人材。 一個好的 DS 需要強大的溝通能力來向其他人說明洞見、了解 A/B 測試的統計顯著性（statistical significance）、以及能合理地解釋 KPI 成長的背後因素。以及最重要的：你渴望改善某個產品。 最後一點大概是所有分析領域的人都不可或缺的： 你要先對某個產品抱持著熱情，才會想方設法地找出洞見並改善它。 不管是什麼領域的 DS，都要想辦法了解自家的產品，以提供可執行的洞見。反過來說，你應該選擇進入真的有興趣的公司 / 產業。老生常談：擇你所愛，愛你所擇。 Introduction to Churn Prediction in Python 這篇適合沒用過 XGBoost 也不熟悉 App 產業的讀者。 此文主要解釋了何謂客戶流失（Customer Churn）、如何利用 Python 來建立一個簡單的 XGBoost 模型，以及如何對一個簡單的資料集做預測。 就跟預測使用一個產品 / 服務的使用者人數相同，能準確預測有多少使用者會在什麼時候放棄使用某產品（Churn）這件事情，對了解一個產品（如手機 App）的發展趨勢是很重要的事情。如果我們把「預測使用者會不會放棄使用產品」這個問題視為一個二元分類問題： 1 = 客戶流失，停止使用某產品 0 = 客戶持續使用某產品 並使用如 XGBoost 等 tree-based 的模型的話，可以直接從模型中得到各個特徵的重要程度（Feature Importance），由此獲得改善產品功能的靈感 / 線索。這同時告訴我們一個重要的事情： 除了準確度，選擇解釋性高的預測模型可以讓 DS 更容易解釋模型給決策者並影響企業決策。 當然，如何定義何謂「客戶流失」就需要資料科學家掌握領域知識。另外值得注意的是，隨著產品功能的進化，客戶流失的定義也有可能跟著改變。 Data engineering: A quick and simple definition 適合想成為資料工程師（DE）並對大數據處理有興趣的讀者。 不同於我們之前以 DS 的角度討論 為何資料科學家需要了解資料工程 ，這篇直接以 DE 角度探討 DE 對企業的重要：處理大數據的能力。 以各別負責的領域來區分的話，DS 通常負責從資料找出可執行的洞見，DE 則是負責資料管道的開發以及保證數據品質。 儘管一個 DS 也需要具備基本的 ETL 素養以及資料清理能力，這些分析專家能提供最大的價值是在找出洞見，而不是清理資料。 因此稍具規模的公司都會尋找具備大數據處理能力（ Spark 、 Flink 、 Kafka 等）的 DE 來處理數據，以讓 DS 能更專注在商業分析。 在資料科學領域越趨成熟的情況下， 我們需要更多 DE 與 DS 分工合作，以從海量數據中創造更大價值。 How To Become A Data Scientist in 12 Months 這篇適合想開始學習資料科學的讀者作為參考。 一個誤打誤撞，闖入資料科學世界的工程師述說他是如何從自學程式語言到成為一個資料科學家。列了一些他個人給新人的建議、不少線上課程以及值得追蹤的資料科學家們。 雖然每個人際遇不同，看完這篇可以確定的是，想要成為一個資料科學家絕對不是學完幾堂線上課程就可以了。關鍵在於持續學習新知。（這道理當然可以套用到各行各業上） Infographic – 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them 一個簡單的資料圖表說明新手 DS 常犯的 13 個錯誤。 每個項目值得一一查看，不過裡頭有幾點我認為值得特別強調： 資料科學在於應用。總是要想著該怎麼實際應用學到的知識，而不是死記硬背。 專注在能「解決什麼問題」，而不是該學什麼「工具」。 業界想要解決的問題不是你在線上課程學到的那麼單純。要解決一個真正的企業問題，你還需要培養資料工程、領域知識（Domain Knowledge）以及良好的溝通能力。 結語 這週我們學到在分析任何數據之前，為何理解產品對一個資料科學家來說非常重要；我們也看到一個簡單利用 Python 嘗試預測顧客流失的案例；最後我們看到資料工程師的崛起以及閱讀了 2 篇跟資料科學相關的文章。 程式能力以及分析能力固然重要，但對產品的理解、良好的溝通能力都是成為一個專業的 DS 不可不缺的能力。 閱讀完之後如果有任何想法，或者有其他想推薦給其他讀者們閱讀的文章，都歡迎在底下留言跟其他讀者們分享。 目前預計每週會發佈新文摘，不過當有別的系列文章要寫的話可能會順延一週。如果希望在新文摘出爐的時候馬上收到通知的話，可以點擊下面的訂閱：） Stay tuned and happy data science!","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-2.html"},{"title":"資料科學文摘 Vol.1 AutoML、Airflow 及 DAU","text":"作為「資料科學文摘」系列文的第一篇，在開始介紹一些優質的文章之前，請讓我稍微說明一下為何會有這系列文章的誕生。先說結論：我希望透過分享一些優質文章的重點摘要，讓更多人能更快地掌握資料科學領域的知識，找出自己有興趣的領域專研，並激盪出更多的討論。 在開始閱讀之前 或許所有職業皆如此，但個人認為資料科學家是前幾不「佛系」的一個職業，你需要擁有非常多知識來讓工作更為順利： 特定程式語言如 Python、SQL 及 R 的使用方法 統計分析方法 建構資料管道（Data Pipeline） 訓練並部署機器學習模型 業界趨勢 ... 當然，你也可以選擇當個佛系資料科學家（如果你知道有哪間企業在應徵，站內信 500 P）。 不過身為一個（非佛系）資料科學家，我常需要閱讀大量的相關文章、新知，並且嘗試模仿文章內出現的演算法、分析手法，加以實踐並應用在自己的工作上。 愛爾蘭詩人 奧斯卡．王爾德 曾說過一句 名言 ： You are what you read. - Oscar Wilde 以資料科學為例，你讀越多相關文章，你就越接近一個資料科學家。不管要精通什麼能力，最快的方式都是透過「模仿」專家怎麼做的。透過閱讀大量的相關文章並從它們學習及模仿，我們可以更快地，且有效率地成為一個稱職的資料科學家。 以往我在閱讀完不錯的文章以後，都會在 Evernote 裡頭寫重點摘要以供自己之後做參考、連結不同領域的知識。在回顧的時候節省了自己大量的時間。有鑑於現在越來越多人對資料科學有興趣，透過分享自己的文摘，希望能讓沒有什麼時間的人也能快速地了解新知，並進一步閱讀自己有興趣的文章。 前言說得夠多了，讓我們來看看這週的文摘吧！ 本週分享：機器學習、資料工程及 App 分析 這週想分享 5 篇文章的文摘，大致上可分為三個主題。這週因為想一次分享 Rachel Thomas 在 fast.ai 談 AutoML 的三篇系列文章，機器學習的文章比例會佔得比較重。 機器學習 What do machine learning practitioners actually do An Opinionated Introduction to AutoML and Neural Architecture Search Google's AutoML: Cutting Through the Hype 資料工程 Airflow: a workflow management platform App 業界分析 DAU/MAU is an important metric to measure engagement, but here's where it fails 除了 AutoML 的系列文以外，閱讀順序不限：） What do machine learning practitioners actually do 這篇介紹機器學習工程師平常在做些什麼，在理解這點以後，我們才知道中間有什麼地方可以自動化，以讓機器學習專案更有效率。 一個完整的機器學習專案通常會包含這些步驟： 理解企業脈絡 清理＆準備資料 訓練模型 實際部署 事後監控模型表現 針對每個步驟，文內都有進一步的項目細分以及解釋，推薦閱讀。儘管一個機器學習工程師不需要自己做所有步驟，了解它們會讓專案更為順利。 就算是專業的研究者，訓練一個深度學習的模型也不是一件非常簡單的事情。而這是 AutoML 以及其子領域，神經結構搜索（neural architecture search）嘗試要解決的。 Google 甚至號稱「只要我們有現在的一百倍計算能力，就可以取代所有機器學習人才」。 An Opinionated Introduction to AutoML and Neural Architecture Search 神經結構搜索或者 AutoML 領域可以幫助我們在「訓練模型」這個步驟的時候，訓練並選擇出最好的超參數（Hyperparameters）。 但如同上篇文所述，這通常只是機器學習專案的其中一小部分，資料科學家或機器學習相關人才並不會因此被全部取代且失業。 現在 AutoML 是非常計算密集（Computation-intensive）的：拿大量的 GPU 計算能力換取研究員的時間。但沒有大量計算能力的人，等 Google 等大公司把最佳化的架構推出來再使用或許是一個比較實際的方案。 DARTS 也是 CMU 與 DeepMind 在嘗試解決「神經結構搜索」這個問題時提出的一個架構，不過他們的假設是所有可行的模型之間是「連續的」，因此可以用常見的「梯度學習」的方式找出最佳模型。這個概念使得他們所需要的計算資源大量減少，值得關注。 Google's AutoML: Cutting Through the Hype 作者認為 Google 在推廣 AutoML 的主張：「我們需要更多計算能力來做神經架構搜尋」值得懷疑，因為就算我們能自動化搜尋出最好的神經模型架構，如何用這些模型解決真正的企業問題、如何實際部署並持續改善機器學習應用等課題，都需要人動腦筋來解決，而這部分還無法自動化。 另外畢竟不是所有做機器學習的人都需要、且有（計算）能力使用神經架構搜尋來訓練自己的模型。但我們可以透過轉換學習（Transfer Learning）來使用已訓練過的模型（pre-trained model）來解決類似問題。與其想著自己也要做最夯的神經架構搜尋，不如多多善用如 Dropout、Batch Normalization 以及 ReLU Linear Unit 來強化模型的預測能力。 不過 Google Colab Notebook 是不錯的免費計算資源，可以善加利用。 Airflow: a workflow management platform 資料科學家在進行各式各樣的分析前，首先需要做的事情通常是蒐集、整理並匯總各式各樣的資料來源以供分析。舉幾個例子： 建立數據倉儲（Data Warehousing） 做 A/B 測試的效果分析 Sessionization：了解使用者在一個 session 裡頭的探訪的網頁、點擊的廣告等活動 為了做這些分析，資料團隊需要建立可靠的資料管道及 ETL，來確保有資料可供分析以及保證資料的品質。 Airflow 是一個由 Airbnb 開發，以 Python 實作的工作流管理系統（Workflow Management System, WMS）。 Airflow 被設計來幫助資料科學家們專注在建構資料管道的邏輯，而不是擔心如果資料管道中間出了什麼差錯時該怎麼維護、重新啟動工作流。（Airflow 有會自己重試失敗工作、當失敗時通知工程師等方便功能）。 Airflow 現在已經進入 Apache 孵化器 ，前景可期。其作者 Maxime Beauchemin 在這篇用淺顯易懂的方式解說 Airflow，值得一看。手癢的朋友可以參考 Quickstart 以及 Tutorial 。 SmartNews 也有在使用 Airflow，我也寫了一篇給新手看的 Airflow 的指南：「 一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載 」，你可以參考看看：） DAU/MAU is an important metric to measure engagement, but here's where it fails 多年前由 Facebook 開始使用，DAU / MAU 是一個 App 產業常用的指標，用來衡量使用者利用自家 App 的程度。 DAU：每天活躍人數（Daily Active Users） MAU：每月活躍人數（Monthly Active Users） DAU / MAU 則是這兩者的比例。可以想像當此比例越高，代表在每月活躍的使用者人數（MAU）中，每天活躍的人數（DAU）越高，可以說明使用者的黏著度越高。 但這篇重點在於說明不同服務、產品因為本身性質的不同，並不都適合用這個指標。像是 Airbnb 這種公司，有些使用者每年可能只使用一次（活躍次數一年才一次），但一次的消費金額很驚人。以 DAU 的角度來看這種顧客的話價值不高，但使用者的生涯價值（Life Time Value）卻很高。 雖然業界很常使用，不盲目使用 DAU / MAU 這個指標，而是依照自己的產品種類，選擇最能代表使用者價值的指標，並將其最大化才是上策。 結語 呼！以上就是這週的文摘內容了！儘管我們在這篇文摘裡頭只包含了 5 篇文章， 3 篇還是系列文，你應該也能感受到不同領域的知識在腦海中互相激盪吧！ 在整理這幾篇文章的重點時我學到不少，希望你也一樣。之後會定期更新，可以隨時回來看看有沒有新文章。如果懶得每天打卡，但希望在新文摘出來的時候馬上收到通知的話，可以點擊下面的訂閱。如果你在閱讀完後有其他感想，也歡迎跟我分享：） Stay tuned and happy data science!","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-1.html"},{"title":"資料科學家 L 的奇幻旅程 Vol.1 新人不得不問的 2 個問題","text":"身為一個資料科學家，我平常會寫些相關領域的文章，像是 揭開資料科學的神秘面紗 為何資料科學家需要學習 SQL 淺談資料視覺化以及 ggplot2 實踐 。 它們都獲得不錯的迴響，我也得到不少很棒的回饋。 不過如果只是介紹特定跟資料科學（Data Science，以下簡稱 DS）相關的工具或概念的話，我們可能會陷入「見樹不見林」的窘境：知道很多 DS 的知識，但卻不曉得這些知識是如何實際被運用在解決人們或是企業的問題。實際上我相信大多數企業的資料科學家在做的事情，並不像很多線上課程那麼單純；有時候你需要結合多種領域的知識，如資料工程、分析手法以及領域知識（Domain Knowledge）來解決一個商業問題。 為了讓有志成為資料科學家，或是單純想要了解的讀者們能理解 DS 是如何實際被（企業）應用，以及讓自己多一點反思的機會，趁著最近開始在 SmartNews 的新工作，我打算開始（不定期地）紀錄自己平常的工作內容以及一些經驗分享（當然，在不洩漏隱私資訊的前提下）。 我相信透過寫作，能讓更多人了解資料科學並幫助自己釐清重要概念 這系列文章將以類似說故事（奇幻旅程，喔耶！）的手法，闡述我在 SmartNews 遇到的一些挑戰，以及作為一名資料科學家（Data Scientist），我如何利用手邊各式各樣的工具以及手法來解決這些問題。透過問題導向（Problem-oriented）的方式，我希望能讓更多人理解 DS 是如何實際被應用在企業之中，進而思考自己該如何預先準備，減少進入這個領域的障礙。（歡迎分享你的想法！） 在後面幾篇文章，我們將有機會深入探討一些分析手法、如何建置預測模型，以及建置可靠的資料流（Dataflow）。但在那之前可別忘了：「巧婦難為無米之炊」。我們才剛剛開始資料科學家的工作，就連筆電裡頭也是什麼軟體都還沒被安裝呢！ 因此在大展身手之前，在這篇文章我們將討論作為一個資料科學家，如何在開始第一個分析專案的同時，「有效率」地熟悉新環境。後面你將會發現，這個初始步驟看似瑣碎，卻能讓之後的工作進行地更為順利。 熟悉環境 = 安裝軟體？ 讓我們想像一下剛從 IT 管理部門手上拿到新筆電的情境。 通常拿到公司配的新電腦以後，一個資料科學家會思考的幾個問題是： 「我要在新電腦上面裝什麼軟體？」 「公司的資料科學家們用什麼軟體？」 「我要怎麼存取公司的資料？」 面對一片空白的環境，我們的腦袋可不能也是一片空白 這些問體的確很重要也很實際（practical），也是我當初能馬上想到的問題。但後面我們會看到，該安裝什麼軟體、該怎麼存取資料庫都是「熟悉環境」裏頭最簡單的部分。為什麼？ 因為通常 manager 會準備好一個清單告訴你該裝什麼，只要照著做就好了。這個清單當然會依照公司內部使用的技術而有所差異，但在大 Google 搜尋時代之下，要在自己的筆電安裝任何東西（應該）都不是太困難的事情。 就算公司沒有給你清單，沒問題！事實上，我也不過就安裝了以下軟體： Python & Anaconda R 語言 & RStudio Jupyter Notebook Docker iTerm2 PyCharm SourceTree ... 當然隨著專案的增加，你可能還會需要其他工具，但基本上沒有想像中的那麼多。有了開發/分析工具以後， IT 管理部門也會跟你說明該如何透過加密的方式，存取一些重要的資料庫以及伺服器。等到這些都搞定以後，理論上我們已經可以準備寫落落長的 SQL 查詢來結合多個資料庫的表格，並使用各種酷炫的 Python packages 進行分析了！ 不過在進行分析的同時，有一些問題值得我們花幾天慢慢地思考。這篇我想特別強調 2 個： 公司內有什麼 關鍵績效指標（KPI） ？ 這些 KPI 是怎麼被產生並顯示在儀表板（Dashboard）上的？ 你可能覺得這些事情看起來並不直接跟資料分析相關，但接下來你會看到，為何在進入公司早期就理解它們，對一個資料科學家來說很重要。首先讓我們看看第一個問題。 公司內有什麼關鍵績效指標？ 為什麼了解公司內有什麼關鍵績效指標（Key Performance Indicator, 後簡稱 KPI）很重要？ 因為這些 KPI 代表著一企業或團隊衡量成功的方式，同時也決定了資料科學家們將要努力的方向。沒有這些 KPI，我們將不能評估我們是不是走在對的路上，也不知道前進的速度。講得浮誇點，一個資料科學家能提供的最大價值就是： 分析數據、從中找出洞見讓企業做出更好的決策，以在最短的時間內最大化 KPI 以這樣的角度來看，KPI 的概念就跟機器學習中的 目標函數（Objective Function） 的概念相同，差別只在於我們是用電腦去最佳化目標函數；用人腦去最佳化企業的 KPI。 資料科學家的工作說穿了，就是如何利用資料以及 DS 的力量，來最大化儀表板上的 KPI 因為 SmartNews 是一個新聞 APP，讓我們舉些 手機 APP 產業中常被使用的 KPI 為例： 安裝次數（#Installs） 每人平均使用時間（Session Time） 瀏覽頁面數（#Page Views） 每天活躍人數（#Daily Active Users） 每月活躍人數（#Monthly Active Users） 重度使用者人數（#Heavy Users） 每日廣告營收（Ad Revenue Per Day） 儘管相同產業可能用類似的 KPI，每家公司給的實際定義（Definition）可能有所出入。不同公司之間的定義有差異是正常的，但該定義合不合理就是另外一回事了。 就跟我們訓練一個機器學習模型的時候會注意目標函數的定義是否合理一樣，在了解有什麼 KPI 以後，我們也應該積極地去詢問相關人員，了解這些 KPI 的定義是否合理。像是： 怎樣的行為可以算是完成安裝？是使用者第一次打開 APP 的瞬間算安裝，還是完成新手教學的時候呢？ 何謂活躍？使用者要做什麼操作才算活躍？打開 APP，更改設定就關掉也算活躍嗎？ 何謂重度使用者？過去一個月使用超過 7 天的人算嗎？ 把握 onboarding 的機會，詢問所有你能質疑的問題 如果 KPI 的定義不合理，糟一點的結果就是你的努力方向對了， KPI 卻沒有上升；更糟的結果則是你往錯的方向最佳化：錯誤的 KPI 提升了，你則沾沾自喜。儘管定義合適的 KPI 需要大量的領域知識，在剛開工的時候，你仍應該對現有的 KPI 做出適當的質疑，嘗試理解它們的合理性。 現在假設 KPI 的定義沒有明顯問題，不管什麼公司都會希望能將這些 KPI 即時地顯示在儀表板上以方便監控自己的營運狀況。但如果一個 APP 的 下載次數超過 2500 萬 ，每天產生上億筆使用者存取紀錄的話，幾個衍生出來的問題就是：KPI 該怎麼從這些原始資料產生出來的？如何保證中間沒有出錯？我們能信賴這些計算出來的值嗎？ 讓我們在下小節討論這個問題。 儀表板上的 KPI 是怎麼產生的？ 實作方式會依照公司有所不同，但讓我們以 SmartNews 為例。 我們的儀表板是使用 CHARTIO ，但基本上 CHARTIO 這種儀表板服務也只是一個 Web UI，它並不會自動幫我們把使用者的存取紀錄轉成 KPI。為了理解每天人們使用 APP 的情況，我們必須自己將所有網路伺服器（Web Servers）上的使用者存取紀錄（Log Data）做一系列的處理以後，轉變成儀表板上的 KPI。 而一個使用者的使用行為大致上會經過以下幾個步驟轉變成 KPI： 使用者打開 SmartNews App，手機客戶端向網路伺服器做出請求（Request） 網路伺服器回傳結果，並將該請求紀錄存在自己的硬碟上 fluentd 搜集所有伺服器上的請求資料，將它們存到 Amazon S3 上 工作流管理系統 Airflow 進行 Batch 處理，定期將被存到 S3 上的使用者存取紀錄轉成 Apache Hive SQL 表格（Tables） CHARTIO 透過分散式 SQL 查詢引擎 Presto 對該表格作查詢，顯示 KPI 從左到右來表示這個流程的話會如下圖： SmartNews 的資料平台 ：將大量原始日誌資料轉成儀表板上有用的 KPI（當然不只用在顯示 KPI） 乍看之下，你可能會想： 「這看起來跟資料科學完全沒相關啊！」 「我只要能存取關聯式資料庫（Relational Database）裡頭的表格不就好了？」 沒錯，嚴格來說這是一個資料工程（Data Engineering）的問題。但正如我們在 資料科學家為何需要了解資料工程 一文裡頭提到的，身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 事實上，了解儀表板上的 KPI 是怎麼產生的，有以下幾個優點： 理解工程師的痛點。能事先以他們的角度思考建立新表格所需的成本的話，他們會更願意幫你建立 通常新的分析會需要新的 ETL，而你可以利用跟產生 KPI 一樣的 ETL 來產生自己的資料管道（Data Pipeline） 確保資料品質。一旦使用的資料有瑕痴，做出來的分析也不會有意義。 一個資料科學家會去了解企業內的資料是怎麼流動的，確保資料的品質並建立自己需要的資料流 最後一點尤其重要。在 SmartNews 的例子裡頭，資料科學家實際上想要分析的是「APP 使用者的存取行為」，而跟使用者行為最直接相關的其實是那些被存在網路伺服器上的 log。只是因為該資料量太大，我們必須建立資料管道做前處理，從大量原始資料中萃取、匯總出我們「可能」有興趣的資料存入關聯式資料庫供之後分析。 以這種角度來看的話，資料彷彿是從網路伺服器（上游）經過一連串的河道（資料管道）流向資料庫（下游）。這也就暗示著兩個可能的風險： 資料在經過河道的時候被污染，資料品質下降 資料在經過河道的時候被限縮，有些有價值的資料沒辦法抵達下游 一個資料科學家如果只專注在下游的資料，就可能冒著以上的風險而不自知。這就是為什麼我們需要了解企業內的資料是如何流動的。 資料的流動當然不限於 KPI 的產生，但我認為用這個問題： 「儀表板上的 KPI 是怎麼產生的？」 來理解一個企業的資料流是一個很好的起始點。畢竟 KPI 是公司最重視的資訊，用來建構其的資料管道也會是最完善且重要的。 結語 在這篇文章裏頭，我們討論了一個資料科學家在進入新公司熟悉環境的時候，除了問該裝什麼工具以外，可以問的兩個問題： 公司內有什麼 KPI？ 儀表板上的 KPI 是怎麼產生的？ 表面上看來是兩個再簡單不過的問題，實際上第一個問題跟業界的領域知識（Domain Knowledge）息息相關；第二個問題則牽涉到大量的資料工程專業。而透過深刻地思考這兩個問題並詢問相關人員，一個資料科學家可以更全面的理解企業並掌握大局觀，做出最有影響力的分析。 當然，除了這兩個問題以外，你還需要問很多其他重要的問題如： 公司的資料文化如何？ 我在 Data Science 團隊裡頭的定位為何？ many more .. 但作為「資料科學家 L 的奇幻旅程」系列文的第一篇文章，為了避免累贅，我把這些問題留給你們（可以留言跟我說你覺得還有什麼問題重要！） 最後的 Bonus 問題：為何是資料科學家「L」？","tags":"Miscellaneous","url":"https://leemeng.tw/journey-of-data-scientist-L-part-1-two-must-ask-questions-when-on-board.html"},{"title":"從彼此學習 - 淺談機器學習以及人類學習","text":"說到近年最熱門的機器學習（Machine Learning）或者人工智慧（Artificial Intelligence），因為知識背景以及觀點的不同，幾乎每個人都有不一樣的見解。雖然我們有千百種定義、無數的專業術語，這篇文章希望用直觀的方式以及具體的例子，讓讀者能夠在跳入一大堆 ML 的教學文章以及線上課程之前，能以一個更高層次且人性化的角度理解機器學習，並進而思考要如何開啟自己的機器學習旅程。 不僅如此，你將發現機器學習並不是冷冰冰的科學，隨處可見人類的巧思；就算不是資料科學家，你也能從『機器學習』獲得啟發，將一些概念用在改善『自己的學習』。 讓我們開始吧！ 目錄 何謂機器學習 機器學習實例：智慧咖啡機 如何讓機器學得更好 如何改善我們的學習 結語 何謂機器學習 多虧了媒體的大量宣傳，我們現在都知道 機器學習 被應用在各個領域。一些常見的例子包含： 自然語言處理，如 Google 翻譯、iPhone 的 Siri 語音辨識 推薦系統，如 Amazon 的 『買了這個商品的人同時也購買了 ...』功能 垃圾郵件自動判定，如同我們在 《直觀理解貝氏定理及其應用》 一文中談到的 電腦視覺，如 Facebook 的人臉辨識 、Youtube 的影片推薦，影像分類 〈這張照片是貓還是狗？〉 等 例子不勝枚舉。有那麼多應用機器學習的例子，不禁讓人思考，究竟什麼是『機器學習』？ 依照目前機器學習的應用，一個大致上的定義是： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 有了這個定義，讓我們再看一下上面提到的幾個例子： 自然語言處理：將得到的英文字串〈輸入〉，轉成中文文字〈輸出〉 推薦系統：將使用者過去的購買記錄〈輸入〉，轉成使用者可能想要購買的商品列表〈輸出〉 垃圾郵件判斷：將郵件內文〈輸入〉，轉成該郵件為垃圾信的機率〈輸出〉 電腦視覺：將一個 400 x 400 像素的圖片，轉成多個標籤的機率〈輸出〉 Google Machine Learning Practica : Google 教你做影像分類，利用機器學習，將充滿著像素的圖片轉換成一個個標籤 嗯嗯，我想這定義還算合理。 眼尖的讀者會發現，這邊的例子說明了上述定義的一半：將輸入 X 轉換成輸出 y。為了進一步解釋後半段『衡量成功的方式』，下面讓我們以一個虛構的咖啡機舉例。 機器學習實例：智慧咖啡機 假設你是個咖啡愛好者，家裡有好幾台高檔的咖啡機，但每次泡出來的咖啡都不合你胃口。 經過無數的失敗，忍無可忍，你最後決定向月巴克公司買台『智慧』咖啡機。該咖啡機宣稱可以了解你的個人需求，泡出世界上最符合你胃口的咖啡。 拆開咖啡機包裝，你興奮地把咖啡豆、砂糖以及牛奶加到該咖啡機裡頭。幾分鐘過後，號稱世界上最好喝的咖啡完成了！ 外觀看起來不錯，你滿懷期待地啜了一口。 『太甜了吧！砂糖太多了，什麼鳥機器！』 你怒吼著，幾乎馬上萌生退貨的想法。這時候咖啡機感應到你的抱怨，用很委屈的聲調說： 『目前調配咖啡的方式為原廠設定。經過統計分析，要得到一個正常台灣人的最佳評分，平均一杯咖啡裡頭的咖啡豆顆數、砂糖匙數以及牛奶的小杯數的比例應該要是 10 比 2 比 3。』 你只覺得莫名其妙，心想這什麼神奇的比例。而且咖啡機剛剛是在拐彎抹角地說我不正常嗎？ 這時候咖啡機又說話了： 『為了做出最符合您口味的咖啡，滿分 100 的情況下，請按鈕輸入你認為此杯咖啡值幾分。另外請告訴我是哪邊出了問題，如糖份比例太高還是牛奶太多，以讓我能記住您的喜好。』 你翻了個白眼，喝杯咖啡還要教機器怎麼調配？哪裡智慧了？ 但為了喝到最符合自己喜好的咖啡，你決定給咖啡機一個機會，好好地調教它。針對眼前這杯咖啡，你把自己的回饋〈評分、調配比例的建議：砂糖太多〉老老實實地輸入進去。 於是乎就這樣，不知不覺中你已經與月巴克咖啡機一起踏上了調配世界上最好喝咖啡的學習之旅。 為了實際了解咖啡機怎麼學習，你翻開咖啡機使用手冊，看到以下內容： 《月巴克智慧咖啡機說明指南》 基本假設；使用者評分 = 使用者滿意程度 預測使用者給咖啡的評分 y' = w1 * 咖啡豆顆數 + w2 * 砂糖匙數 + w3 * 牛奶小杯數 + 基本分 b 目標：找出一組調配比重 w1, w2, w3, b ，使得咖啡機預測的評分 y' 越接近實際的使用者評分 y 越好 咖啡製作：使用上述比重調配咖啡，使得預測評分 y' 接近 100 各原料調配比重〈原廠設定〉： w1 = 10, w2 = 2, w3 = 3, b = 60 ... 你恍然大悟，原來月巴克公司為了讓咖啡機最大化你給咖啡的評分，在咖啡機裡頭建構了一個簡單的 線性回歸〈Linear Regression〉 模型。 在這模型裡頭，使用者針對一杯咖啡的評分 y 會受到多個原料的量的影響。每個原料量的影響程度則透過個別的 w 來描述。理想上，如果咖啡機可以找出一組比重〈weights〉 w ，使得咖啡機『預測』出來的評分 y' 跟『實際』使用者給的評分 y 非常相近的話，咖啡機就可以利用該模型來合理地選擇咖啡豆、砂糖以及牛奶的量，調配出一杯預期能獲得你最高評分的咖啡。 那咖啡機要如何實際『學習』呢？ 或者換句話說，咖啡機要怎麼樣知道它現在用的參數〈 w1 、 b 等〉夠不夠好呢？如果不夠好的話，要怎麼修正呢？ 在一開始完全沒有任何使用者回饋的時候，咖啡機可以很合理地使用原廠設定來計算使用者評分 y' 。等到你輸入了一些評分 y 以後，將所有從你得到的評分 y 跟咖啡機自己預測的 y' 做比較，看咖啡機做的預測評分跟你的給分差了多少，據此修正原料的比重 w 以及 b 。 y' 跟 y 的差異讓我們暫時稱作 diff_y 。 修正以後，一般來說我們會得到新的比重 w' 以及 b' 。當咖啡機使用 w' 以及 b' 產生新的預測評分 y'' ，其跟你的實際給分 y 也會有一個差距，我們則將其稱作為 diff_y' 當使用新的參數〈 w1 、 b 等〉產生的 diff_y' 比原來的 diff_y 來小的時候，我們就能很開心地表示：『這咖啡機幹得真不錯！學到了點東西，能更準確地找出我的喜好！』。 而在每次獲得你回饋的時候重複上述步驟，咖啡機不斷地修正它用來預估你給咖啡分數的參數，讓預測出來的值 y' 跟你過去所有評分 y 之間的差異都更小。雖然我們這邊不會細講，但在線性回歸裡頭，一個常被用來計算預測值 y' 跟實際值 y 差異的方式是 最小平方法〈Least Squares〉 ： diff_y = 針對每次使用者的評分 y，機器利用當下的參數產生相對應的 y' 以後，用兩者計算 (y' - y) 的平方並加總它們 你可以看到，當 diff_y 越小，代表咖啡機越能準確地依據目前的原料量，來預測你會給咖啡的評分。 咖啡機學得很快。經過幾個怒吼以及失望的夜晚，透過你給的回饋，它現在做出的咖啡已經能很穩定地讓你給出 90 分以上的評價。 透過詢問咖啡機，你現在知道，為了獲得你的高評價，咖啡機學到了以下的模型： 你給咖啡的評分 = 咖啡豆顆數 * 13 + 砂糖匙數 * 1.2 + 牛奶小杯數 * 1.5 + 基本分 40 分 這跟一開始為了滿足所有人的原廠設定相比，還差真不少： 一般使用者評分 = 咖啡豆顆數 * 10 + 砂糖匙數 * 2 + 牛奶小杯數 * 3 + 基本分 60 分 依照你過去的回〈ㄊㄧㄠˊ〉饋〈ㄐㄧㄠˋ〉，咖啡機發現跟一般人相比，咖啡豆量對你來說，是一杯咖啡好不好喝的重要指標〈13 vs 10〉，砂糖跟牛奶量則反而顯得沒那麼重要。而從基本分來看，咖啡機甚至學到你對咖啡的要求程度比一般人要來得嚴格〈40 vs 60〉，實實在在地說明機器了解你是個專業的咖啡愛好者。 現在再讓我們看一次前面定義的機器學習： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 經過上面的咖啡機例子，我們能清楚地歸納出以下幾點： 咖啡機是在進行機器學習，學習如何用一連串運算，將原物料的量〈咖啡豆顆數等〉 X 轉換成使用者評分 y 機器學習裡所謂的一系列運算，在咖啡機的例子裡是進行線性回歸，即 y = w * x + b 咖啡機衡量成功的方式是計算『預測評分 y' 跟實際評分 y 之間的差異大小』，此差異越小，代表學得越好 衡量成功的方法很重要，因為咖啡機可以知道『努力/學習的方向』 咖啡機透過反覆地修正參數，進而最小化上述差異，成功地『學習』 機器學習是學習一組最符合目標的『參數』〈如基本分的 40 、咖啡豆顆數的 13 〉 我們可以總結說，咖啡機在你給的回饋以及監督之下，想辦法從三種原料〈咖啡豆、砂糖、牛奶〉中，『學習』出一個最棒的調配比例，以做出一杯能得到你最高評價的咖啡。在機器學習領域裡頭，這實際上被稱作 監督式學習〈Supervised Learning〉 。 太棒了，你跟月巴克咖啡機從此過著幸福美滿的日子‧ 如何讓機器學得更好 如果你閱讀完上面例子，開始思考以下問題： 『除了原物料的量以外，或許還可以搜集其他類型的資料，像是咖啡機主人的性別、年齡甚至泡咖啡的時間，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『與其預測使用者評分，能不能建立新的模型，直接預測使用者喜好？』 『如果咖啡機得到更多我的回饋資料，是不是會更準？』 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我得說聲恭喜，你已經擁有機器學習的思維且準備好進入機器學習的殿堂了。 但在你摩拳擦掌，準備進入殿堂時，有些人可能會跟你說，近年因為機器學習在各領域發展神速，且機器能使用的訓練資料〈Training Data〉也越來越多， 強人工智慧〈Strong AI〉 很快就會出現。不久的未來，我們甚至也不用自己設計演算法以及模型，A.I.會自動幫我們全部做好。也就是：機器會自己讓機器學得更好。 當強人工智慧出現以後，或許人類就不再被需要了。因為機器會自己讓機器學得更好。 真的嗎？沒有人能真正的預測未來，所以我們無從知曉。 但至少在接下來幾年，要讓機器學習或者人工智慧再繼續進步，『人類的思考』是不可或缺的重要因素。主要體現在兩個地方： 機器並沒有意識判斷『為什麼』以及何謂『對的方向』 機器的世界觀是人類教的 機器並沒有意識判斷『為什麼』以及何謂『正確』 電腦因為有著強大的記憶以及運算能力，在很多任務上面都已經可以超越人類的表現。 ILSVRC 歷屆的深度學習模型 : 近年電腦視覺〈Computer Vision〉領域發展快速，機器學習在影像分類〈Image Classification〉的表現已經超越人眼。 但能達到這樣的成果的前提，都是因為有人類在設計模型、監督機器學習。 目前機器學習或是 A.I. 的應用其背後的模型，當你去看裡頭一行行的程式碼的時候，裡頭並不會定義『為什麼』要做這些任務。實際上，在機器學習的過程中，機器並沒有意識到為什麼要做這些任務；而如果沒有人類的介入的話，機器也不會自己定義什麼樣的結果叫做『成功』或『正確』，而也就不知道該往什麼方向學習。 該讓機器學習什麼 怎麼定義『正確/成功』，讓機器遵從並往該方向改善 這兩件事情只有依靠人類來做決定。而其決定將大大地影響機器學習的成果以及品質。機器不會跟你說： 『我覺得把影片裡面的貓咪識別出來，比識別出交通號誌燈來得重要。』 『喔... 我覺得我們學的方向怪怪的，讓我們往這個方向學習如何？』 一個定義出錯的目標函式〈Objective Function〉將永遠無法讓機器學出我們想要的結果。 針對這點，我們應該： 找出值得解決的問題，下定我們的目標並明確定義何謂『正確』，以讓機器往該方向學習。 機器的世界觀是人類教的 第二點應該也不難理解。一個機器的世界觀基本上取決於兩點： 人類指定使用的模型〈Model〉 餵給它的資料〈Data〉 如同前面咖啡機的線性回歸，我們透過一個簡單的線性模型，教會咖啡機看世界。在咖啡機所認知的世界裡頭，使用者的評分就只會受到三種原物料量的影響：咖啡豆、砂糖及牛奶。這是一個非常簡單的世界，方便我們理解機器學習，但在真實世界上基本上不會成功運作，你需要考慮更多因素。 如同我們前面有提到，你可能會思考以下問題： 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『除了原物料的量以外，或許還可以搜集其他類型的資料，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我們怎麼看世界，將直接影響機器怎麼看世界。 事實上你已經在思考如何擴充機器的世界觀了。你可以使用各式各樣的模型、更多的資料以讓機器能用更全面的方式來理解這個世界。而這個新的世界觀只能由你來定義。 〈現在的〉機器不會突然跟你說： 『嗯... 我覺得我們應該考慮泡咖啡時有沒有下雨，因為這可能會嚴重地影響使用者心情，進而影響評分。』 『我只依照你的評分做最佳化，可能會有 過適 問題喔！』 這些問題都是我們必須自己發現並解決，不能只期待機器自動解決〈至少這幾年〉。在機器學習領域裡頭，最怕的不是模型完全不行，而是上述的 過適〈Overfitting〉 問題：機器所看到的資料本身太過侷限，導致其雖然只看到真實世界的一小部分，就誤以為那是全世界。換句話說，機器裡存在著強烈的偏見〈bias〉。前陣子常聽到的案例是 白人設計出來的臉部辨識模型對黑人有偏見 。 以我們咖啡機的例子來說，如果你家裡只有你一人，咖啡機只需要服務你一人即好；但如果你們是一個家庭，家裡的人都希望咖啡機能為它們弄出好喝的咖啡，則每個人都需要給予咖啡機回饋，以讓咖啡機了解每個人喜好。如果仍然只有你一個人給予咖啡機回饋，其他人不給分，則咖啡機會以為得到的評分來自所有人，誤以為只要最佳化這些評分，就能滿足所有人，其實不然。 為了讓機器看得更遠更全面，我們應該： 想辦法在機器學習的模型內融入更多我們的直觀想法〈intuition〉，並讓機器看到更全面的資料，以拓展機器的『世界觀』。 如何改善我們的學習 閱讀到此，相信你對機器學習已經有個高層次的理解了。 在對機器學習有個基本的了解以後，我們在前面章節提到為了讓機器學得更好，一個可行的方向是將我們的直觀想法、世界觀轉換成機器可以運算的模型或是目標函式，以讓機器能從聰明的我們身上學習。但換個角度思考，在我們教機器『學習』的時候，應該也能從機器『學習』到什麼才對。 事實上，很多我們應用在機器學習領域的想法，緊密地跟我們的個人生活息息相關。 舉個例子，在機器學習中，過適〈Overfitting〉是我們最想要避免的問題。我們不會希望機器只學到事物的表象，或者受到 outliers 的影響，而是希望機器學到更重要的模式〈Pattern〉、趨勢〈Trend〉。所以研究者們透過各種方式來讓機器不要過適： 輸入更多資料 用更簡單的模型 減輕 outliers 的比重 正規化〈Normalization〉 ... 而當機器成功地學到了事物的本質，就能精準地預測未來並且概括所有情況〈Generalize〉： 預測股票漲幅 預測誰最後會當上總統 預測詐騙交易 預測一張照片裡頭有什麼物件 畢竟，一個只看過貓跟狗照片的機器，不管未來看到什麼，就算是汽車或是人類，也只會將視它們為一定程度的貓或者是狗。 知名心理學家 馬斯洛 曾 說過 ： 如果你只有一個槌子，你可能就會把每個問題都視為釘子。 不管學習的是機器還是人類，學會概括〈Generalize〉並避免過適〈Overfitting〉是最重要的課題。手中只有槌子的人，什麼問題都看起來像釘子。 同樣道理可以應用在人類的學習上。 當我們只注重在參加各式各樣的線上深度學習〈Deep Learning〉課程，而不去了解機器學習背後的原理就是一種過適；當我們掙扎著要用 Python 還是 R 畫漂亮的圖，而不去理解為何要這樣畫，才能讓觀眾更容易理解時也是一種過適。更不用說一個只了解 決策樹〈Decision Tree〉 的同學，看到什麼問題都會想要用決策樹來解的案例了。 學習表象比較簡單沒錯，但不能帶你走很遠。了解趨勢或者模式則讓你看到未來： 卓越的歷史學家忽視單一歷史事件，透過了解世界整體的歷史脈絡來預測未來 愛因斯坦觀察到世界的運作原理而推出有名的質能轉換公式 E = mc² 好的學習方式是理解事物背後的運作的趨勢、模式。為何我們要機器學習？為什麼深度學習會崛起？注重在詢問更多的『為什麼』以理解事物本質。 從一些已經被應用在機器學習的概念獲得啟發，我們可以重新思考並改善我們人類自己的學習。 結語 我們在這篇文章前半段以一個虛構的智慧咖啡機為例，深入探討機器學習的一些基本但十分的重要概念以及運作方式。 在掌握機器學習的基本概念以後，我們討論了如何以『人』為本，融入我們人類的智慧以讓機器更聰明地學習、了解這個世界。接著，我們用了一點篇幅，討論了看似不相關的『人類學習』以及『機器學習』之間一個共同且最重要的核心目標：『學習如何去概括〈Generalize〉事物並避免過適〈Overfitting〉』。 現在機器學習〈尤其是深度學習〉跟其他學術領域如統計、電腦科學相比，是一個相對新的領域，大家都還在摸索階段。但正如當年新興的程式設計已經普遍被重視，甚至加入國高中教育一般，我想再過幾年，等機器學習更為成熟後，人們也會開始呼籲將『機器學習』領域的知識納入課綱，成為我們下一代的基本素養之一。 未來教育模式的改變： 或許『機器學習』會如同『程式設計』素養一般，成為下一代必備的基本知識素養之一 或許那就是本篇所提到的『從機器學習中學習』。 但在那時代到達之前，讓我們開心〈機器〉學習吧！","tags":"Miscellaneous","url":"https://leemeng.tw/some-thought-on-learning-from-machine-learning.html"},{"title":"從經驗中學習 - 直觀理解貝氏定理及其應用","text":"貝氏定理（Bayes' theorem） 是機率論中，一個概念簡單卻非常強大的定理。有了機率論的存在，人們才能理性且合理地評估未知事物發生的可能性（例：今天的下雨機率有多少？我中樂透的可能性有多高？），並透過貝氏定理搭配經驗法則來不斷地改善目前的認知，協助我們做更好的決策。 英國數學家 哈羅德·傑弗里斯 甚至 說過 ： 貝氏定理之於機率論，就像是畢氏定理之於幾何學。 因為其簡單且強大的特性，被廣泛應用在醫療診斷以及機器學習等領域。網路並不缺貝氏定理的教學文章，但多數以 機率公式 出發，不夠直觀（至少以我個人來說），就算理解了也不易內化成自己的知識。 貝氏定理公式 : 喔喔喔感覺到數學的力量了嗎？ 因此這篇將利用生活上我們（或 人工智慧 ）常需要考慮的事情當作引子，如： 今天的下雨機率是多少？ 這封 email 是垃圾郵件的可能性有多高？ 醫生說我得癌症了，這可靠度有多高？（好吧，或許這沒那麼常發生） 來直觀地了解貝氏定理是怎麼被應用在各式各樣的地方。我們甚至可以效仿貝氏定理的精神，讓自己能更理性地評估未知並從經驗中學習。 廢話不多說，讓我們開始吧！ 今天會下雨嗎？ 在實際說明貝氏定理的公式把你嚇跑之前，讓我們先做個簡單的假想實驗來說明貝氏定理的精神。 假設大雄一早準備出門跟靜香見面，正在考慮要不要帶傘出門。 起床的時候他想： 「這地區不太會下雨，不需要帶傘吧！」 往窗外一看，大雄眉頭一皺，發現烏雲密佈。 「痾有烏雲，感覺下雨機率上升了，但好懶得帶傘 .. 先吃完早餐再說吧。」 走到廚房，發現餐桌上一大堆螞蟻在開趴。 「依據老媽的智慧，螞蟻往屋內跑代表 下雨機率又提升了 。真的不得不帶傘了嗎 .. 不不不！我不要帶好麻煩！」 想著想著，這時候靜香打電話過來了： 「胖虎說他也要去喔！」「蛤你說什麼！？」 胖虎是有名的雨男，每次跟他出遊都會下雨。依照這個經驗以及前面看到的幾個現象，最後大雄放棄掙扎，帶著雨傘出門了。 在上面的例子中，大雄觀察到三個現象： 烏雲密佈 螞蟻開趴 胖虎出沒 依據他過往的經驗，這些現象都會使得降雨的機率提升，讓他逐漸改變剛起床的時候「今天不太會下雨」的想法，最後決定帶傘出門。 這個決策的轉變過程，其實就是貝氏定理的精神： 針對眼前發生的現象以及獲得的新資訊，搭配過往經驗，來修正一開始的想法。 實際上，大雄已經在腦海中進行了多次貝氏定理的運算而不自知（我家大雄哪有那麼聰明）。現在讓我們用比較數學的方式來重現大雄腦海中的運算。 讓我們帶點數字進去 在了解貝氏定理的目的以後，讓我們以 發生比（odds） 的方式來闡述定理。發生比很簡單，就只是列出兩個（或以上）的事件分別（可能）發生的次數。 使用發生比的好處是可以很容易地比較不同事件發生的相對次數。後面會看到，我們也能把發生比轉成機率。 假設依據過往氣象紀錄，大雄住的地區一年 365 天中有 270 天放晴，下雨的天數為 365 - 270 = 95 天。則下雨的發生比為： 雨天數：晴天數 = 95：270 你可以把發生比想像成是一種相對關係，上面這個發生比代表，在大雄所住的地區，每觀測到 95 個雨天的日子，我們同時會觀測到 270 個晴天。晴天約是雨天的三倍之多（270 / 95）。 轉換成機率來看的話，就是把雨天的天數，去除以所有天數： 95 / (95 + 270) = 0.26 = 26% 一年也就只有 26% 的降雨機會，這也是為何大雄一開始在還沒觀察到新現象（烏雲、螞蟻及胖虎）的時候，合理認為今天「應該」不會下雨的原因。 我們再繼續假設，依據大雄的過往經驗，他發現： 雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍 這個 9 倍是怎麼來的呢？ 這其實是所謂的 概度比（likelihood ratio） 。分別計算雨天及晴天發生的情況下，出現「烏雲密佈」現象的機率以後，再將兩者相除： 雨天時烏雲密佈的機率 = P(烏雲|雨天） ------------------------------- 晴天時烏雲密佈的機率 = P(烏雲|晴天） 這兩個機率又被稱為 條件機率（conditional probability） 。一般 P(A|B) 代表在事件 B 發生的情況下，事件 A 發生的機率。 假設平均來看，在 10 個雨天裡頭，早上烏雲密佈的天數為 9 天（也就是說平均有 1 天的雨天是早上沒有烏雲密佈的），則我們可以說，「給定雨天的條件下，烏雲密佈」的機率是： P(烏雲|雨天） = 9 / 10 另外在 10 個晴天裡頭，早上烏雲密佈的天數平均為 1 天（也就是說早上雖然烏雲密佈，但最後並沒有下雨的天數），則「給定晴天的條件下，烏雲密佈」的機率是： P(烏雲|晴天） = 1 / 10 則烏雲密佈的概度比即為： P(烏雲|雨天） 9 / 10 ----------- = --------- = 9 P(烏雲|晴天） 1 / 10 雖然「概度比」一詞很饒舌，但它就是一個比例，也就是「幾分之幾」的概念。 9 就是「一分之九」＝ 9 倍，而因為分母是「晴天」，你可以解讀這個 9 為 「在烏雲密佈發生的情況下，每觀測到 1 個晴天，就會同時觀測到 9 個雨天」。 也可以像是大雄觀察到的： 「雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍」。 經驗告訴我們，早上烏雲密佈的情況下，該天是雨天的機率就隨著上升 貝氏定理初顯鋒芒 所以有了這個倍數可以做什麼？直覺及經驗告訴我們，在觀測到烏雲密佈的前提下，下雨機率理論上會有所提升。 換句話說，在烏雲密佈，且觀測到的晴天數不變的情況下，觀測到的雨天數應該要有所上升，這樣下雨的天數在所有天數裡頭的比例才會上升。 而其上升的倍數就是前面的概度比（ 9 倍）。因此在烏雲密佈發生的情況下，新的下雨發生比（odds）可以寫成： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 95 * 9 ： 270 = 855 ： 270 新的下雨發生比是我們利用觀測到的現象重新計算的，因此一般稱為「事後發生比」（posterior odds）；而一開始的發生比則被稱為「事前發生比」（prior odds）。 事後發生比告訴我們，在烏雲密佈的情況下，每觀測 855 個雨天，就會同時觀測到 270 個晴天。跟事前發生比相反，現在雨天數反而超過晴天的三倍（855 / 270）。 要計算新的下雨機率，我們一樣把雨天數去除以所有天數： 855 / (855 + 270) = 0.76 = 76% 跟一開始的 26% 相比，在觀測到烏雲密佈這個現象以後，下雨的機率足足上升了 50 個百分點，現在我們有更充分的理由請大雄帶把傘了。 實際上，透過上面的計算，我們已經套用貝氏定理的公式了（ 發生比版本 ）： 事後發生比 = 概度比 * 事前發生比 如同大雄的例子，一般應用貝氏定理的情境如下： 對一件未知事物有初步的猜測（事前發生比） 觀測到跟該事物相關的現象 利用先前跟該現象有關的經驗計算出概度比 利用概度比修正該猜測，得到修正後結果（事後發生比） 重新評估、做決策 又觀察到新現象，重複步驟 3 到 5 透過貝氏定理，我們可以很快速地利用過去的經驗改善自己的想法，並產生更好的決策。 大雄不死心：單純貝式 雖然觀察到了烏雲密佈，且利用過往經驗修正下雨的機率到了 76%，懶惰的大雄一開始還是不想帶傘出去。但為何最後還是帶傘出門了呢？那是因為除了烏雲密佈以外，他還觀察到了其他兩個影響下雨機率的現象： 螞蟻開趴 胖虎出沒 貝氏定理本身雖然強大，但其中一個使它被廣泛利用的是 單純貝式（Naive Bayes） 的概念：假設不同現象之間出現的機率為 獨立 )。 設成獨立有什麼好處？事情變得很簡單，我們不用考慮現象 A 跟現象 B 之間的關聯性，能針對每個現象，分別去計算概度比，修正從「前面」的現象得到的結果，持續改善我們的認知。也就是上一節提到的貝氏定理的應用步驟 6。 如法炮製，讓我們假設大雄針對其他兩個現象的經驗是： 雨天時，螞蟻出現在室內的天數是晴天的 2 倍 雨天時，胖虎出遊的次數是晴天的 3 倍 讓我們再次套用貝氏定理，但這次不是套用在一開始什麼都不知道的事前發生比： 雨天數：晴天數 = 95：270 而是在觀察到烏雲密佈後的事後發生比： 烏雲密佈下的雨天數：晴天數 = 855：270 首先，讓我們套用跟螞蟻相關的經驗： 雨天時螞蟻出現在室內的天數是晴天的 2 倍 概度比已經算好，所以依照貝氏定理的公式： 事後發生比 = 概度比 * 事前發生比 新的（螞蟻）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 855 * 2 ： 270 = 1710 ： 270 下雨的機率則提升為： 1710 / (1710 + 270) = 0.86 = 86% 比起只有烏雲密佈，在螞蟻也出現的情況下，降雨機率又提升了接近 10%。大雄是一個降雨機率不大於 90% 就不帶傘的傢伙，讓我們看看胖虎出沒能不能使他改變心意。 同樣，再次套用定理到上一個（螞蟻的）發生比，則新的（胖虎）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 1710 * 3 ： 270 = 5130 ： 270 在轉換成機率之前，我們發現新的雨天數是晴天數的 10 倍以上，因此可以想像新的機率至少是 90% 以上。而實際計算下雨的機率： 5130 / (5130 + 270) = 0.95 = 95% 在觀察到烏雲密佈、螞蟻以及胖虎出沒以後，大雄預估降雨機率上升至 95%，這下不得不帶傘出門了。 重複套用貝氏定理以修正想法的過程就像是在創作：把眼前所有所見（顏料）一個一個納入考量，做出最後的判斷（作品） 動動腦時間 到了這邊，我相信你現在應該已經可以在腦中直觀地運用貝氏定理：針對眼前發生的現象，運用過去相關的經驗（計算概度比），來理性地評估某事件可能發生的機率。 事實上在你繼續讀下去之前，我建議先停一停，思考幾個可以實際在生活中運用（或者已經在用）此定理的現象，以幫助你內化（internalize）這些概念。 如果你一時想不到點子，這邊提供幾個例子： email 內文裡頭出現「週年慶」時，該郵件為垃圾信的機率 新聞內文出現「柯文哲」時，文章主題為政治的機率 醫生說你得胰腺癌 時，你真的得病的機率 玩 英雄聯盟 時， KDA 超過 4 的對手排位是鑽石以上的機率 在東京藥妝店血拼，旁邊講中文的人是台灣人的機率 如果你有其他有趣的例子，歡迎留言跟大家分享。（現在留言不用登入了！） 如同上述，貝氏定理有非常多應用。不過這邊想深入探討第一個 email 的案例：給定一封電子郵件的內文，你要怎麼判斷該信是不是垃圾信件？ 從人腦到電腦：讓機器幫我們做判斷 一個典型的垃圾郵件內文 你說沒有什麼事情難得了我們人腦。依照過往經驗： 垃圾信件裡頭出現「週年慶」一詞的機會是一般信件的 20 倍 垃圾信件裡頭出現「折扣」一詞的機會是一般信件的 10 倍 在假設所有信件裡頭一半是垃圾信件（發生比 1：1）的前提下，依照單純貝氏的公式，這封信是垃圾信件的可能性上升 200 倍（20 * 10），我們可以放心把這封信丟入垃圾信分類。 但是沒有人會想要在腦中對每封信做這個運算。人類是懶惰的，能自動化的東西就請電腦幫我們解決就好了。 另外你也不可能記得每一個詞的倍數，實際上也沒有必要。只要讓電腦幫我們記住每個詞分別在垃圾郵件以及一般信件出現的次數，就能計算所有詞彙的概度比（odds）。 等到一封新的信件來以後，找出裡頭的字對應的倍數做相乘以後，電腦就能自動分類郵件了。事實上這就是 機器學習 中 單純貝氏分類器（Naive Bayes Classifier） 在做的事情。 讓機器取代人腦自動判斷，有幾個顯而易見的好處： 判斷速度倍增 記憶能力超強 （可以把分類郵件空出的時間拿去看貓咪影片） 唷呼！垃圾郵件自動變不見！小鎮村又變得更美好了。 當然你想自己實作單純貝氏分類器的話，Python 可以使用 scikit-learn 來實作。 小心！你的經驗可靠嗎？ 我們花了很長的篇幅講了幾個貝氏定理/單純貝氏的應用，也看到它既簡單又強大的特性。但在你摩拳擦掌並實際應用此定理的時候，有幾點需要注意： 不同現象/事件真的獨立嗎？ 一開始的猜測以及經驗可靠嗎？ 很多現象不一定是完全獨立而是相關的。不過一個常見的解決方法是想辦法增加更多的現象/事件/特徵值（features）來讓做出來的貝氏分類器比較可靠。貝氏定理當然不完美，但正如統計學家 喬治·E·P·博克斯 所說 ： 所有模型（models）都是錯的；但有些是有用的。 儘管「獨立」這個假設在某些情況下不合常理，但在如垃圾郵件分類等問題上，貝氏分類器有不錯的表現。而且重點是它實作簡單，可以拿來當作 baseline。 而一開始的猜測跟經驗可不可靠這個問題，英國數學家 卡爾·皮爾森 ，針對貝氏定理則給出一個我很愛的 名言 ： 一個信奉貝氏定理的人常常做這樣的事情：模糊地期待著馬的出現，瞥見驢子的蹤影，強烈地相信他是見到了一匹駝子。 「先入為主」大概是應用貝氏定理最忌諱的點了。下次再套用定理時，記得先思考自己一開始的假設以及經驗是否值得信任或者有什麼盲點。需不需要搜集更多資料來修正一開始的想法。 我真的是驢子不是駝子啊！（豆知識：駝是馬跟驢生下的動物） 總結 我們在這篇開頭首先用「大雄評估下雨」的例子來直觀地理解貝氏定理背後的精神，接著透過簡單的數學概念、發生比（odds）以及概度比（likelihood ratio）來推出基本的貝氏定理公式。 接著進一步延伸至單純貝氏（naive bayes）的概念，讓機器透過過去累積的資訊，為我們自動分類垃圾郵件。 最後我們提到一些應用貝氏定理需要注意的事情。 即使基本的貝氏定理不難，延伸的領域非常的廣。這篇沒辦法包含所有範圍，但希望透過這篇基礎介紹，能讓讀者能利用貝氏定理的概念，更理性地評估未知並從經驗中學習（或者是建立自己的貝氏分類器）。 另外如果你有其他有趣的例子可以應用在貝氏定理，歡迎留言跟大家分享。（現在留言不用登入了！）","tags":"Miscellaneous","url":"https://leemeng.tw/intuitive-understandind-of-bayes-rules-and-learn-from-experience.html"},{"title":"揭開資料科學的神秘面紗","text":"幾乎每天我們都能看到跟資料科學（Data Science）相關的新聞與文章，像是最近 Google 利用遞迴神經網路建立可以跟真人對話而不被發現的語音助理 、 成為 Apple 等公司的資料科學家前必讀的面試題目 等等。 市面上有大量資料科學相關課程、書籍供我們自由學習，事實上，多到一個人不可能看完。你有想過為何我們需要學習資料科學嗎？為什麼資料科學現在那麼夯？我們應該拿資料科學來做什麼？ 抽離技術實作或者分析手法的討論，這篇文章試著用簡單的經濟學原理回答這幾個問題。 希望閱讀完本文的讀者能了解為何資料科學在資訊時代扮演重要角色，以及我們要怎麼有效率地把握「資料科學力」以創造更大的價值。 目錄 本文大致上會分成以下段落： 聽說你想當資料科學家？ 資料科學到底在夯什麼？ 啊所以那個資料科學勒？ 充實你的資料科學力 結語 讓我們開始吧！ 聽說你想當資料科學家？ 資料科學大概是近年最夯的流行語之一了。不管在哪邊，你都可以聽到媒體相關的報導： 食農教育科研成果農業大數據結合資料科學 想成為資料科學家？來挑戰 Google、FB、Apple 等六間公司人工智慧最新面試題 台灣產業AI化 最大問題人才不足 成為搶手資料科學家應具備什麼技能？先學Python準沒錯 ... 族繁不及備載。 而因為企業對擁有資料科學能力的人才需求大，想成為資料科學家（Data Scientist）的同學們也不少，相關的教學文章、線上課程如雨後春筍般湧現。這邊我沒辦法把它們一一列出，但你可以前往一些知名的線上課程平台如 Coursera 、 Udemy 、 DataCamp 並搜尋「資料科學」（或者 Data Science）就知道我的意思了。 DataCamp : 基本上全部都是資料科學相關課程，寫程式寫到飽 如果我們把這些新聞報導或者教學課程，依照主題/領域做個粗略分類的話，還可以得到一些關鍵字： 大數據（Big Data） 人工智慧（Artificial Intelligence） 資料視覺化（Data Visualization） 機器學習（Machine Learning） 深度學習（Deep Learning） 統計分析（Statistical Analytics） 雲端運算 （Cloud Computing） Python、R、SQL ... 資料科學 : 涵蓋大量領域，各領域的專業知識就像一棟棟大樓將你包圍吞噬 想學習資料科學的同學這時候就頭疼了： 「全部都要學嗎？從哪邊開始 .. 」 「選 Python 或是 R 語言 ？還是先學 SQL ？」 「資料視覺化要學 Python 的 Matplotlib 還是 R 的 ggplot2 ？」 現在有些網站很用心，為了解決你的煩惱，還將相關的課程集結起來成一個 專業課程（Specialization） 讓你一步一步跟著學。 勤學如你，上了幾門課以後學會如何利用 Python 做簡單的機器學習模型 、 使用 R 做資料視覺化 ，甚至也懂得 使用 SQL 存取資料庫 。 恭喜！你是個資料科學家了！ ... 痾.. 這麼簡單？好像哪裡怪怪的？ ... 你會不會開始思考： 所以到底啥是資料科學？資料科學到底在夯什麼？為什麼我要學資料科學？ 實際上會這樣想的不止你一人。在仔細思考並給上述問題一個合理的解釋之前，就算學了再多門課，充其量只是在不斷擴充自己的「資料科學工具盒」，但卻不知道「為何要買這些工具」、「要拿這些工具做什麼」。 資料科學工具箱 : 琳瑯滿目，酷！但你要用這些工具創造或是改善什麼？ 因為你學的是方便實踐資料科學的程式語言、工具、方法論（Methodology），而不是「為什麼資料科學重要」。我會用剩下的篇幅試著對此問題給出一套解釋。解釋方法有很多種，所以非常歡迎在底下留言分享你的看法。 不過現在，且聽我娓娓道來。 資料科學到底在夯什麼？ 除了耳熟能詳的「技術發展快速」、「資料量龐大」的理由以外，資料科學之所以那麼夯，背後還有一個可想而知的巨大推手：「商業利益」。 要進一步解釋這個概念，我們可以從 Google 首席經濟學家 哈爾·范里安 在 2009 年接受麥肯錫的訪問，探討 網際網路對企業的挑戰 中看出一些端倪。（真知灼見，建議作課外閱讀） 近年網際網路快速發展。要在網路上發表內容，對任何人或者任何企業來說都是輕而易舉的事情。這邊說的內容（Content）可以是任意資訊，比如説： 一則 Facebook 粉絲團貼文 一則銷售青島啤酒的網頁 一個教你學習資料科學的線上課程網頁 一篇部落格文章（像你正在看的這篇） 因為傳播媒介以及科技的進步，要在網路上發布這些資訊並讓他人注意到的成本趨近於零，而其導致的結果就是 全球的資訊量急速成長 。被稱為人工智慧之父之一的經濟學家 赫伯特·西蒙 針對這種現象就曾說過一句 名言 ： 在一個資訊豐富的世界裡頭，資訊量的富裕導致人們注意力的貧窮。 以個人的角度來看，在時間以及精力有限的情況下，我們每天能接受資訊的時間以及注意力都是有限的。如何分配這些寶貴的注意力以接收對的資訊，變成現代人的課題。 痛點即商機。很多企業透過解決這個 資料超載（Information Overload） 的問題來提供使用者價值： 漫畫網站把所有知名漫畫整理在一起供你閱讀 價值：統整、數位化、自動更新散落各地的漫畫資訊 Google 提供搜尋功能給你 價值：讓你快速找到存在地球上的任何相關資訊 Youtube 讓你免費看到飽 價值：讓你隨時看全世界最新的貓咪影片 只要喊「+1」Facebook 粉絲團就免費把「珍貴」的內容給你 價值：給你數位內容如新產品資訊、整理過後的旅遊資訊等 天下沒有白吃的午餐，企業願意這麼做必定有得到什麼。你的確取得了免費的數位內容（文章、影片、漫畫），但又付出了什麼？ 資訊時代最珍貴的資源 : 人們（與喵）的關注 實際上，不管是閱讀文章、觀看影片、瀏覽漫畫，你都是在拿了你最寶貴的「注意力」跟企業交換這些價值。而在成功獲得你目光的同時，這些企業則透過秀廣告給你來獲利（例 1 - 3，暫不考慮 AdBlock）。 註：在這邊，「注意力」跟「時間」有些微秒差異。不過你只要回想昨天晚上跟朋友或是家人吃飯的時候，各自滑手機的景象就可以了：你把「時間」花在跟身旁的人吃飯，卻把「注意力」（或者說是關注）放在手機裡頭的數位資訊。（如果你沒用手機，我很抱歉。） 例 4 很有趣，你是拿「你自己以及你朋友圈的人的注意力」來做價值交換（你的留言讓 Facebook 的演算法自動推播該貼文到你朋友的動態牆上，粉絲團賺到他們的關注），但基本上是同樣的道理。 資訊時代最常見的價值交換 : 給我你的關注，我就給你免費資訊（外加廣告） 以經濟學的角度來重述前面的觀點，現在的資訊時代最不缺的資源就是「資料」；稀有、價值高且需要小心分配的稀有財是「人們的注意力」。在這個資訊爆炸的時代，企業透過加工處理大量的原始資料，產生新產品、服務及價值來換取該稀有財： 誰能善用資料科學的力量、從現有數據創造新價值、服務或產品，並以此吸引人們珍貴的關注，就能獲得商機。 這就是為何資料科學那麼夯的其中一個原因：從資料中創造新價值，進而產生商業利益。 啊所以那個資料科學勒？ 聽到上面的例子，有些人的想法可能是： 「哇這些企業好狡猾把我的注意力都偷走了！」 「這樣回覆 +1 好有罪惡感喔嗚嗚」 「好險我用 AdBlock 嘻嘻」 但這邊重點是要說明，這種依靠廣告的商業模式已經行之多年。Facebook、Google 等企業為了抓住我們的目光，持續不斷地在精進，以求能有效率地儲存、處理以及分析由我們產生的大量數據。 而他們用來處理、分析、視覺化以及理解數據的這些程式語言、工具、方法論的總集合就構成所謂的資料科學。 搜集、理解、分析、處理、視覺化資料數據並從中萃取有用的價值就是資料科學。 讓我們以一個簡單的 Google 搜尋做更進一步的解釋。 想像你在 Google 上搜尋「 data science courses 」後可能跑出以下結果： Google 日常：搜尋結果之上有幾個相關廣告 沒什麼特別的，Google 日常不是嗎？ 現在試著做以下步驟： 開一個新的分頁/視窗 隨便搜尋一個你有興趣的商品/產品，記下出現的幾個廣告還有它們的順序。 隨便點幾個連結或者什麼都不做 重複步驟 2 跟 3 幾次以後，你應該可以觀察到顯示的廣告消失或者順序改變了：而這是因為背後有 Google 的廣告競價系統在運作。下面是這系統的超級簡化示意圖： Google 廣告競價：運用使用者的行為資料，即時地推算出該使用者點擊各廣告的機率。搭配業主的出價，選出適當的廣告顯示。 要完成此系統需要強大的資料科學技術支持。只有一個人搜尋的時候事情還好辦，但你得知道，在本文撰寫當下，Google 平均 1 秒鐘處理 67, 000 筆 搜尋。試著想像一下，為了實現這個系統，Google 可能需要完成以下幾件事情： 使用深度學習進行自然語言處理，判斷使用者輸入的語言以及想要表達什麼 即時處理所有使用者查詢的串流數據 利用使用者過往的瀏覽紀錄來預測點擊某廣告的機率 在公司內部監控目前台灣使用者的搜尋趨勢（類似 Google Trend ) 機器學習、統計分析、大數據 ... 這些工作運用到的技術，不就是那些我們在 聽說你想當資料科學家 章節裡頭看到的關鍵字嗎？ 我們這篇只以 Google 的廣告系統為例，但實際上現在幾乎可以說是全世界都在想辦法利用資料科學的力量來處理資料並創造新的價值、服務、公司。看看現在的新創，有哪些沒有用到資料科學？ 所以你現在知道為何資料科學那麼重要了。 全世界都在想辦法活用資料科學以從龐大數據中為使用者創造更多價值。 充實你的資料科學力 綜觀資料科學一詞萌芽到最近的過程，全世界的資料量 持續成長 ，而人們也不斷地在想辦法追趕這些資料： 用最有效率的方式儲存這些資料 用最快的速度處理及分析這些資料 對這些資料做實驗，重複再重複測試不同的假說及演算法 快速地從資料萃取出新的洞見（Insight） 以這些洞見創造新的價值、產品、服務 加速以上步驟所需要的循環時間 如同前面 Google 的例子，這些都是資料科學。 你會發現，所謂的資料科學（Data Science）就是對資料（Data）做科學、有系統地（Scientific）的處理罷了。資料科學一詞或許誕生沒多久，但對資料做科學這概念老早就存在了。只是近年因為 數據量的快速成長（如 物聯網裝置的火紅 ） 運算能力的進步 人工智慧的突破 等等原因，讓我們更急迫地想辦法用以往做不到的方式來理解這個世界的龐大數據。 Youtube 現在能夠分析出你喜歡看貓咪影片 ， Google 可以建立跟真人對話而不被發現的語音助理 。這些都是他們利用資料科學，從現有的大量數據創造額外價值的例子。如同 這篇 所說的： 未來是屬於那些能從大量資料數據創造價值的企業以及人才的。 一個好消息是： 一企業擁有的資料量 一企業裡能夠處理、分析此資料量的資料科學人才數量 這兩者在多數企業都是不成比例的（後者短缺），因此擁有資料科學能力的人才薪水可以說是水漲船高。而這當然也變成為何近年那麼多人想成為資料科學家的動機（儘管有些人可能不知道背後原因）。 了解資料科學相關知識的人才 : 是大多數的企業積極尋找的對象 在了解這點以後，你可以先想想自己的興趣在哪裡、想用資料科學創造什麼價值。這邊想強調的是，先思考你能透過資料科學，創造什麼新的「價值」，而不是什麼「商業利益」。 如同我們前面看到的，資料科學是現行廣告經濟的背後推手，但為何我們願意看 Google、Facebook 丟給我們的廣告？那是因為他們「先」從資料創造了價值（方便的搜尋功能、社群網路功能）從而取得我們的關注。 實際上，在取得關注以後，你的商業模式不是一定要秀廣告給使用者看。訂閱制（Subscription）或會員制是一個替代方案： NetFlix 和 Amazon 都是這樣。甚至，你可以 不像 Google 一樣思考 ，使用新的商業模型。 但「商業模式」不是這篇想討論的議題。重點是「價值」： 在資訊爆炸的時代，各行各業的每個人都需要學習善用資料科學，從資料數據創造新的使用者價值。 事實上，與其想著要成為一個資料科學家，不如先好好想想，在自己目前所在的業界、公司、職位能怎麼利用手邊的資料數據搭配資料科學來創造新的價值。 結語 如果你耐心地看到這邊，代表我得到你最珍貴的關注了，賺賺賺！ 稍微複習一下，我們在這篇文章開頭假想了一個有志學習資料科學的同學。在他/她學習資料科學的過程產生了幾個疑問：「為何資料科學那麼夯？」「為何我們需要資料科學？」 而本篇則以非常簡單的經濟學供給概念，加上 Google 以及 Facebook 的運作方式來說明現在的企業是怎麽利用資料科學來創造新的使用者價值來交換人們的關注。 我們接著說著為何今後各行各業都需要「資料科學力」來處理日益增加的資料數據並為人們建立新的價值。事實上很多職稱不是「資料科學家」的人現在都已經在做著資料科學： 搜集、理解、分析、處理、視覺化資料數據並從中萃取有用的價值 當年網際網路開始蓬勃發展，軟體工程師是最夯最潮的行業。儘管現在工程師的重要性並沒有下降，隨著人們的程式能力穩定上升，軟體工程師回歸平凡，甚至還有人戲稱為「碼農」、「程式猿」。 歷史總是不斷重演。 或許再過幾年，等人們的資料科學力上升到一定階段，資料科學變成呼吸喝水般的知識以後，資料科學家們也會被人戲稱為「資料農」。 或許當你幾年後遇到我，我可能這樣回你： 嘿！我就只是個資料農！你也是嗎？","tags":"Miscellaneous","url":"https://leemeng.tw/demystify-the-hype-of-data-science-and-its-value.html"},{"title":"為何資料科學家需要學習 SQL","text":"這篇簡單討論 結構化查詢語言（SQL） 在概念上跟命令式程式語言如 Python 有什麼不同之處，以及在什麼樣的情況下我們會想要利用 SQL 做資料分析。 這篇注重在為何你會想要使用 SQL 做資料分析，而非 SQL 本身功能的教學。如果要學習 SQL 本身，可以參考最後面的 推薦閱讀 。 使用 SQL 與數據對話 身為資料科學家或者是分析人員，我們都知道 SQL 基本上是必備的分析工具。 簡單來說， SQL 是一種程式語言 ，我們可以透過它對被儲存在 關聯式資料庫 裡頭的資料進行查詢或操作。 SQL 是資料科學家與資料庫（Database）溝通的語言 在沒接觸過 SQL 之前，你可能會想 「做為一個程式語言，為何 SQL 有那麼多人在使用？ 」 「我們有 Python、R，不學 SQL 應該也沒關係吧？」 「又要學一個程式語言好麻煩。」 為了釐清這些疑問，讓我們做一個假想實驗。比方說我們現在想要知道某個特定顧客過去的所有購買記錄。 如果你熟悉 SQL 的話，可以對資料庫下一個簡單的查詢（Query）： SELECT c . name AS customer , o . totalprice , o . orderdate FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey WHERE c . name = 'Customer#000000001' ORDER BY o . orderdate ; 上面這個查詢翻為白話就是： 從顧客清單 customer 還有購賣紀錄 orders 裡頭 FROM customer AS c INNER JOIN orders AS o ON c.custkey = o.custkey 找出名為 Customer#000000001 的顧客的所有購買紀錄 WHERE c.name = 'Customer#000000001' 並把那些紀錄依照購買日期排序 ORDER BY o.orderdate 最後只回傳顧客名稱、總購買金額、購買日期幾個項目 SELECT c.name AS customer, o.totalprice, o.orderdate 這個查詢對第一次寫 SQL 的人可能會覺得很複雜，但注意，我們並沒有告訴資料庫「如何」取得這些資料，比方說： 怎麼合併顧客跟購買紀錄？ 怎麼過濾特定顧客？ 怎麼排序？ 我們只告訴它該給我們「什麼資料」。而得到的結果是： customer | totalprice | orderdate --------------------+------------+------------ Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 如同我們預期，只有該顧客的購買紀錄被回傳，且依照購買日期 orderdate 從早排到晚。 實際上，資料庫可能需要做以下運算來取得資料： 將顧客表格 customer 以及購買紀錄的表格 orders 分別命名為 c 及 o 依照共通的鍵值 custkey 合併（ JOIN ）兩表格 找出特定顧客 Customer#000000001 的購買記錄 將該紀錄依照購買日期 orderdate 排序 選出要顯示的欄位 這些運算最後都得依照「某個」順序執行，但是我們不需要考慮這些事情，完全依靠資料庫的 查詢最佳化器（Query Optimizer） 來幫我們決定。 寫 SQL 敘述時，你可以理解成我們是指定「要的資料」，而查詢最佳化器會依照此需求，找出一個最佳路徑來取得必要的資料。 SQL 查詢 : 專注在你的目標，查詢最佳化器會負責找到達成目標的最佳路徑 換句話說，當我們在寫 SQL 的時候，是在進行 宣告式程式設計（Declarative Programming） ：我們只告訴資料庫，我們想要什麼資料（What），而不是怎麼取得（How）它們。 這跟一般常見的 命令式程式語言（Imperative Programming） 如 Python、Java 有所不同。在寫 SQL 時，我們告訴資料庫它該達成的目標 - 取得什麼資料（What）；在寫 Python 時，我們得告訴程式該怎麼達成該目標（How）。 為了進一步闡述這個概念，接著讓我們試著使用 Python 來取得跟上面的 SQL 查詢一樣的結果。 用 Python 達到 SQL 查詢效果 首先先假設所有顧客資料是透過一個 list 儲存，裡頭包含多個 dict 。每個 dict 則代表一個顧客的資料： customers = [ { \"name\" : \"Customer#000000001\" , \"custkey\" : \"1\" }, { \"name\" : \"Customer#000000002\" , \"custkey\" : \"2\" } ] 而購買記錄則是一個 dict ， dict 的鍵值為所有顧客的 custkey ；鍵值對應的值則是包含該顧客所有購買紀錄的 list ： orders = { \"1\" : [{ \"totalprice\" : 152411.41 , \"orderdate\" : \"1993-06-05\" }, { \"totalprice\" : 270087.44 , \"orderdate\" : \"1997-03-04\" }, { \"totalprice\" : 165928.33 , \"orderdate\" : \"1995-10-29\" } ] } 所以 orders[\"1\"] 就代表 custkey = 1 的顧客的購買紀錄。 了解背後的資料結構以後，我們可以寫一段 Python 程式碼來取得資料： print ( \"customer | totalprice| orderdate \" ) print ( \"------------------ | ----------| --------- \" ) # 從所有顧客找符合條件的人 for c in customers : # 跳過我們沒興趣的顧客 if c [ 'name' ] != 'Customer#000000001' : continue # 利用 custkey 取德該顧客的購買紀錄 c_orders = orders [ c [ 'custkey' ]] # 依照 orderdate 排序購買紀錄 c_orders_sorted = sorted ( c_orders , key = lambda x : x [ 'orderdate' ]) # 將所有排序後的記錄回傳 for o in c_orders_sorted : values = [ c [ 'name' ], str ( o [ 'totalprice' ]), str ( o [ 'orderdate' ])] print ( \" | \" . join ( values )) # 已經找到該顧客，提早結束迴圈以減少處理時間 break customer | totalprice| orderdate ------------------ | ----------| --------- Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 所以我們使用 Python 達到跟上面的 SQL 查詢一樣的結果了。但兩者在執行上有什麼差異？ 使用命令式程式語言來處理資料時，我們需要： 了解資料結構以操作資料（顧客是存在 list 還是 dict ？） 明確地定義執行步驟（先排序購買記錄 orders 還是先把顧客 customers 跟購買紀錄合併？） 最佳化（如最後的 break ） 再看一次先前的 SQL 查詢（+註解）： -- 給我以下幾個欄位：顧客名稱、總購買金額、購買日期 SELECT c . name AS customer , o . totalprice , o . orderdate -- 將有相同 custkey 的顧客跟購買紀錄合併 FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey -- 只需要此顧客的購買紀錄 WHERE c . name = 'Customer#000000001' -- 依照購買日期排序 ORDER BY o . orderdate ; 這裡頭我們不需要了解資料被以什麼形式儲存，也不需要定義要以什麼順序執行查詢，更不用做最佳化。這些事情全部交給背後的資料庫處理，使得資料科學家可以專注在更高層次的問題：「我們需要什麼資料？」 而這正是 SQL 最強大的地方： SQL 讓資料科學家可以專注在需要「什麼」資料而非要「怎麼」取得。 結語 雖然我們這篇只舉了一個十分簡單的例子，但一般來說 SQL 非常適合以下的使用情境： 將多個資料來源（例：表格）合併起來並依照一些條件篩選結果 依照取得的資料做一些簡易的 aggregation （如：加總、平均、最大值） 簡單的資料轉換（例：把 datetime 欄位取出年份） 如果需要十分複雜的資料轉換或者計算時，一般我還是推薦使用 Python 或 R。但是下次當你有機會使用 SQL 取得想要的資料時，不妨試著專注在「想要什麼資料」而不是「怎麼取得資料」。說不定一個 SQL 查詢就能幫你省下一些花在搜集資料的時間。 推薦閱讀 DataCamp - Intro to SQL for Data Science DataCamp - Joining Data in PostgreSQL LinkedIn Learning - Advanced SQL for Data Scientists","tags":"Miscellaneous","url":"https://leemeng.tw/why-you-need-to-learn-sql-as-a-data-scientist.html"},{"title":"資料科學家為何需要了解資料工程","text":"透過描述資料科學家的一天日常，本文將簡單介紹資料工程（Data Engineering）的概念、其如何跟資料科學相關。以及最重要的，作為一個資料科學家（Data Scientist）應該如何學習並善用這些知識來創造最大價值。 身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 目錄 資料科學家的一天 資料準備 第一挑戰：資料量大增 第二挑戰：非結構化資料 資料為本 資料管道 資料倉儲 資料湖 如何實際應用資料工程？ 結語 資料科學家的一天 一說到資料科學，在你腦海中浮現的幾個關鍵字可能是： 資料分析 資料視覺化 A.I. / 機器學習 等為人津津樂道的面向。 的確，這些都在資料科學的範疇裡頭，但實際上佔用多數資料科學家大部分時間，卻常被忽略的部分是資料準備： Forbes : 依據調查，多數資料科學家花 80 % 的時候在準備資料 資料準備 說到資料準備，你可能會聯想到我們在前一篇 淺談資料視覺化以及 ggplot2 實踐 裡頭，使用 R 語言做的簡單資料清理： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read.csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as.numeric ( ramen_all $ Stars ) # Subset 資料 ramen <- ramen_all %>% filter ( Country %in% count ( ramen_all , Country , sort = TRUE )[ 1 : 6 , 1 , drop = TRUE ]) %>% filter ( Style %in% count ( ramen_all , Style , sort = TRUE )[ 1 : 4 , 1 , drop = TRUE ]) 在做分析之前，我們做了以下的步驟來準備資料： 讀進 ramen-ratings.csv 轉變某些欄位的資料型態 依照一些條件取出想要分析的資料 雖然資料量不大，你仍然可以試著想像我們實際上建立了一個 ETL 工作： 將資料從來源（硬碟）擷取出來（ E xtract） 做了一些轉換（ T ransform） 載入（ L oad）目的地（記憶體） 假設我們把一般的資料分析專案分為以下兩階段： 資料準備：將資料轉換成適合分析的格式 資料分析：探索資料、建構預測模型 上面的 ETL 就屬於第一個步驟。又因為此資料集大概只包含 5,000 筆資料，步驟 1 所花的時間跟步驟 2 的所需時間相比，可以說微乎其微，它不會是你做資料科學的一個 bottleneck。 但如果你要處理的資料量是這個的 1,000 倍大呢？你還能馬上進入分析階段嗎？ 第一挑戰：資料量大增 實際上一個資料科學家每天需要分析的資料量可能要乘上幾個級數。現在假設你從銷售部門拿到一個包含數百萬筆銷售紀錄，大小為 60G 的 CSV 檔案，我想你應該不會想要直接打開它，即使它在某些人眼裡還不夠資格稱為大數據 (´；ω；｀) 你殫精竭慮，最後決定去問公司內一位資深的 資料工程師（Data Engineer） 該怎麼解決這問題。 該仁兄施了點你不曉得的魔法，過了幾分鐘從 Slack 丟來個神秘的 URL。連到上面，你發現熟悉的 Jupyter Nook 介面，而且 CSV 還幫你載好了 Σ(ﾟдﾟ; Bonus : Jupyter Lab 是 Jupyter Notebook 的改善版，大推 你開心地在資料工程師幫你搞定的機器上做出分析，最後在大家面前做口頭報告。大家針對報告的反應不錯，但坐在底下的廣告部門的人這時候提問了： 「可以把這些銷售紀錄跟廣告點擊的串流日誌（log）合在一起分析嗎？這樣我們會有更多有趣的結果！」 你的頭又痛了起來。 第二挑戰：非結構化資料 除了資料量級的差異，一個資料科學家在企業裡頭會遇到的另外一個挑戰是非結構化資料（Unstructured Data）的快速增加。你如何將各種不同格式（JSON、存取日誌、CSV 等）的資料以有效率的方式跟平常熟悉的關聯式資料庫如 PostgreSQL 裡頭的資料結合以供分析？ AWS Reinvent : 非結構化資料快速增加，但因為不存在關聯式資料庫裡，無法直接被拿來分析 如果我們能寫一個簡單的 SQL 查詢，把銷售資料（sales）跟廣告點擊（clicks）資料依照共有的鍵值 sale_id 合起來該有多好： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 你想著想著就到下班時間了。 「算了，還是先回家睡個覺，明天再厚著臉皮問資料工程師吧！反正之前他也幫我在 資料倉儲（Data Warehouse） 加了新的表格。」 資料為本 從上面這個資料科學家的一天，我們得到什麼啟示？ 資料（的基礎設施）為資料科學之基礎 - 巧婦難為無米之炊 老實說這個例子裡頭的資料科學家已經非常幸運：公司裡有資料工程師能幫他把大量、複雜格式的資料做 ETL 並以資料倉儲中的一個新表格（Table）的方式呈現轉換過後的資料以供他使用。硬要說稍微不方便的地方，頂多就是該資料科學家得等資料工程師搞定好資料就是了。 然而因為資料工程師是一個很新的職位，多數的企業現在並沒有這樣的人存在。大多數的資料科學家只能自己下海，想辦法生出可以用的資料。實際上， Monica Rogati 在 The AI Hierarchy of Needs 提到，一些常見的資料科學專案像是 建置 AI 建置簡單的機器學習模型 資料分析 都得建立在「有完善且可靠的資料」這個基礎之下： 資料科學的金字塔層級要求 : 你需要建立好資料科學的基礎設施才有本錢往「上」發展 以金字塔最下三層為例，要讓資料科學的專案順利進行，你最少要（由下而上）： 持續搜集（COLLECT）原始資料 將該資料轉移（MOVE / STORE）到適合分析的地方如資料倉儲、 資料湖 轉換（TRANSFORM）被轉移的資料，進行前處理以方便分析 我認爲資料工程的重頭戲在上面的 2, 3 點：將資料以「轉換好」的形式「送」到可供分析的地方。（當然也可以先送再轉換，或者不轉換，詳見下面章節的 資料湖 ） 身為資料科學家，如果你夠幸運，公司內部有專業的資料工程師幫你把上面這件事情做好，恭喜！你可以多專注在分析以及建置預測模型上面； 但假設公司裡頭只有資料科學家，而企業又想要處理大數據的話，抱歉，你得擔起這個攤子，想辦法把資料的基本設施搞定： 每個成功的資料科學家背後都有個偉大的資料工程師。或者該資料科學家就是那個資料工程師。 身為資料科學家，如果我們也能了解資料工程相關的知識的話，不就能更快地、更有效率地進行資料分析了嗎？ 這個想法即是所謂的 從鄰近專業（Adjacent Disciplines）學習 ：透過學習跟本業息息相關的資料工程，資料科學家可以加速資料科學的專案進行，並為個人以及團隊創造更大價值。想閱讀更多，可以看看 在 Airbnb 工作的資料科學家怎麼說 。 接著讓我們稍微聊聊到底什麼是資料工程以及一些相關例子。 資料管道 依照前面的論述，資料工程最主要的目的就是建構資料科學的基本設施（Infrastructure）。而這些基礎設施裡頭一個很重要的部分是 資料管道（Data Pipeline） 的建置：將資料從來源 S ource 導向目的地 T arget 以供之後的利用。有必要的話，對資料進行一些轉換。 一些簡單的例子像是我們之前部落格提到的： 將 NoSQL（MongoDB） 資料導向資料倉儲（Redshift） 將串流資料（Kinesis）導向資料湖（AWS S3） 從上面的例子也可以看到，實際上資料管道是一個涵蓋範圍很廣的詞彙，包含 即時的串流資料處理 Batch 處理（如：每 12 小時作一次） ETL 做的事情跟資料管道類似，但偏重在 Batch 處理，這篇文章將 ETL 視為資料管道裡頭的一個子集。 ETL : 從資料來源擷取、轉換資料並將其導入目的地 資料的來源或目的地可以是： 分散式檔案儲存系統（如 HDFS 、 AWS S3 ） 一般的資料庫 / 資料倉儲（如 AWS Redshift ） ... ETL 最重要的是轉換步驟，一些常見的轉換包含： 改變欄位名稱 去除空值（Missing Value） 套用商業邏輯，事先做資料整合（Aggregate） 轉變資料格式（例：從 JSON 到適合資料倉儲的格式如 Parquet ） 資料工程師 : 建構資料管道以讓大量的資料可供分析 這些轉換都是為了讓之後使用資料的資料科學家們能更輕鬆地分析資料。為了建置可靠的資料管道 / ETL 流程，我們常會需要使用一些管理工具像是 Airflow 、 AWS Glue 以確保資料的處理如同我們預期。 一些關鍵技術 Hadoop 生態環境 分散式系統上的 ETL 設計 SQL-on-Hadoop 的專案了解（如 Apache Hive, Spark SQL, Fackbook Presto） 資料流程管理（如 Airflow、AWS Glue） 那經過資料管道處理後的資料要怎麼存取/分析？依照存取方式的不同，資料管道的架構方式也會有所不同。 而存取資料的方式大概可以分為兩種： 資料倉儲（Data Warehousing） 資料湖（Data Lake） 資料倉儲 資料倉儲的概念就跟實際的倉儲相同：你在這邊將原料（原始資料）轉化成可以消化的產品（資料庫裡頭的經過整理的一筆筆紀錄）並存起來方便之後分析。 這邊最重要的概念是：為了方便商業智慧的萃取，在將資料放入資料倉儲前，資料科學家 / 資料工程師需要花很多的心力決定資料庫綱目（Database Schema）要長什麼樣子。 也就是說資料庫的綱要（Schema）在建立資料管道的時候就已經被決定了：這種模式稱之為 Schema-on-Write。這是為了確保資料在被放進資料倉儲的時候就已經是可以分析的形式，方便資料科學家分析。 你可以想像資料工程師在建構資料管道 / ETL 的時候，得對原始資料做大量的轉換以讓資料在被 寫 入資料倉儲時就已經符合一開始定義的 Schema。而資料倉儲最常被拿來使用的一個資料模型（Data Model）是所謂的 Dimensional Modeling （Stars / Snowflaks Schema）。 資料倉儲最被廣泛使用的 Data Model : Stars Schema 資料工程師將企業最重要的事件（如：使用者下了訂單、發了一個 Facebook 動態）放到最中間的 Fact Table，並且為了可以使用所有想像得到的維度來分析這些事件，會把事件本身的維度（Dimensions）再分別存在外圍的多個 Dimension Tables。常見的維度有： 時間（此事件什麼時候產生、年月份、星期幾等） 商品的製造商的資料、其他細節 ... 因為看起來就像是一個星星，因此被命名為 Stars Schema。Snowflakes 則是其變形。 一些關鍵技術 在資料倉儲的部分，關鍵的技術與概念有： 了解正規化（Normalization）的好處 分散式 SQL 查詢引擎的原理（如 Presto ） 分析專用的資料模型的設計原理（如 Stars / Snowflakes schema） 了解分散式系統背後各種 JOIN 的原理（Sort-Merge JOINs、Broadcast Hash JOINs、Paritioned Hash JOINs 等） 資料湖 「每天新增的資料量太多，要把所有想分析的資料都做詳細的 Schema 轉換再存入資料倉儲太花人力成本。總之先把這些資料原封不動地存到分散式檔案儲存系統上，之後利用如 AWS Glue 等服務將資料的 schema 爬出來並分析。」這就是以資料湖為核心的資料管道架構想法。一般這種存取資料的方式我們稱之為 Schema-on-Read，因為 Schema 是在實際載入原始資料的時候才被使用。 AWS Athena 就是一個 AWS 依照這樣的想法打造的服務。 舉個簡單的例子，假設我們現在想把 資料科學家的一天 提到的銷售資料以及廣告資料合併起來做分析，在 AWS 上我們可以實作一個這樣的資料管道： 利用 AWS Athena 及 AWS Glue 實作以資料湖為基礎的分析架構 : 即時合併並分析不同格式的資料 我們將存在關聯式資料庫的銷售資料透過 ETL 存到資料湖（AWS S3）裡頭以後，利用 AWS Glue 將資料的中繼資料（Meta Data）存在資料目錄（Data Catalogue）底頭。常見的中繼資料有 表格定義（有哪些欄位，如： sale_id ） 各個欄位的資料型態 各個欄位實際在原始資料（如 CSV ）裡頭的排列順序 接著我們就可以利用提到的 SQL 查詢把銷售資料跟廣告資料合併： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 收到以上的 SQL 查詢，Athena 會分別把銷售資料以及廣告資料依照對應的資料目錄解析資料後合併再回傳結果給我們。 我認為今後這種以資料湖為基礎的分析架構會越來越熱門，原因如下： 非結構化資料量越來越大，花費人力在事前為資料倉儲建立完整的 schema 越來越不實際 分散式 SQL 查詢服務像是 Athena 抽象化複雜的資料格式，允許資料科學家下 SQL 查詢做 ad-hoc 分析 透過 Parquet / ORC 等資料格式來自動減少資料湖沒有做正規化而導致的效能損失 一些關鍵技術 雖然再過幾年，等到資料工程的人才增加，資料科學家或許可以完全不用介意背後的資料基礎設施的建置，但近幾年這部分可能還是要靠資料科學家自己實作。 資料湖的概念 AWS Glue + AWS Athena 的運用（Bonus: Serverless 分析架構，不需管理機器） Hive MetaData Store 在資料湖的例子我主要都用 AWS 的服務來舉例，但你可以自由使用其他雲端服務或者 Hadoop。 如何實際應用資料工程？ 首先你得先了解目前環境的資料基礎設施。而為了釐清這點，你可以問自己或者相關人員以下問題： 資料科學的金字塔，我們建到哪一層了？ 我們過去有哪些專案是在取得、準備資料階段就陷入瓶頸？ 我們有專業的資料工程師或相關人員在做資料倉儲或者是資料湖的準備嗎？ 我們的資料是儲存在什麼分散式檔案儲存系統上面？ HDFS 還是 S3？ 我們是怎麼管理/監管 ETL 工作的？ 要考慮用 Airflow 嗎？ 要建構一個新的資料管道的話，要自己架 Hadoop 群集還是使用雲端服務？ ... 在你思考過以上幾個問題以後，你就會發現為何過往有些資料科學的專案進展緩慢了。這時候與其一直在等待資料的到來，你可以把你想到的幾個問題拿去跟相關的工程師討論。相信我，從你開口跟他們討論如何解決資料基礎設施的瓶頸這點開始，他們將不再視你為「那個只想要拿到資料」的敵人，而是同伴。 Hadoop 的分散式基礎設施 : 要學的東西太多，不如就用雲端服務吧 假如很不幸，你們公司沒有專業的工程師，而你得自己想辦法兜出一個可以處理這些大量資料的方法，我會建議先從現存的全受管（Full-Managed）雲端服務找能解決痛點的方案。 使用現成的雲端服務來建置資料基礎有幾個好處： Pay-as-you-go，通常是用多少花多少 Proof-of-concept，你可以直接開始嘗試建立最重要的商業邏輯而非架機器 Serverless 架構，不需管理機器（如 AWS Glue + Athena） 導入成本降低（相較於自己架 Hadoop Cluster） 結語 我嘗試在這篇文章說明資料工程對資料科學家的重要，以及你可以如何開始學習資料工程。 在這個大數據時代，資料科學家的價值在於找出「大量」資料背後的潛在價值，不要反而讓「資料量太多」這邊成了你最大的限制。 從雲端服務開始，多學一點資料工程，讓你的資料科學專案前進地更快吧！ 如果你有任何想法想要提出或分享，都歡迎在底下留言或者透過社群網站聯絡我 B-)","tags":"Miscellaneous","url":"https://leemeng.tw/why-you-need-to-learn-data-engineering-as-a-data-scientist.html"},{"title":"淺談資料視覺化以及 ggplot2 實踐","text":"這篇主要描述自己以往在利用 Python 做資料視覺化 (data visualization) 時常犯的思維瑕疵，而該思維如何在接觸 R 的 ggplot2 以後得到改善。 本文會試著說明資料視覺化的本質為何，以及在設計視覺化時，概念上應該包含什麼要素以及步驟。最後展示如何透過 ggplot2 活用前述的概念，來實際做資料視覺化。 目錄 文章內容大致上會分為以下幾個小節： 資料視覺化是資料與圖的直接映射？ 資料視覺化應該是 .. ggplot2 實踐 結語 References 資料視覺化是資料與圖的直接映射？ 身為一個 Python 起家的資料科學家，在做資料視覺化的時候，我很自然地使用 Python ecosystem 裡像是 matplotlib 以及 seaborn 等繪圖 packages。針對手中的資料，我會想辦法找到一個「對應」的圖然後把資料塞進去。簡單無腦 (:3 」∠) 舉例來說，當我們手上有三個變數 x, y, z 且其各自的資料型態為： x: 定量變數 (quantitative) y: 定量變數 z: 定性變數（categorical） 則我們想要進行資料視覺化的時候有幾種選擇： 想分析 x, y -> 都是定量資料 -> 散佈圖 (scatter plot) 想分析 x, z -> 一定量一定性 -> 長條圖 (bar chart) 在這，「資料視覺化」的定義是一種映射關係 (mapping)：也就是如何將資料直接對應到某個「特定」圖表形式（折線圖、散佈圖 etc.）。基本上這種映射關係在做簡單的分析的時候沒有什麼問題，但是當想要同時分析/呈現的變數超過兩個 （例： x & y & z ）的時候就不容易找到適合的圖。一個折衷的方法是我們把變數兩兩畫圖做比較，但這樣會侷限我們能分析的資料維度數目，錯過一些有趣的洞見。 資料視覺化應該是 .. 先確認觀眾及目的 在完成一些 ggplot2 的 tutorials 後，可以發現資料視覺化一般依用途可以分為兩種： 探索、了解資料特性 說故事：將探索過後得到的洞見 (insight) 傳達給其他人 Image Credit : 搞清楚資料視覺化的目的以及觀眾是重要的第一步 依照目的以及觀眾的不同，資料視覺化的方式會有所不同。一個常見的例子是當我們第一次接觸某個資料集。這時候資料視覺化的觀眾是自己，目的是在最短的時間了解資料特性。則這時我們在做圖的時候的要求就可以很寬鬆，像是不加上標題，或是只要能做出自己能理解的視覺化即可。 正式定義 在確認觀眾及目的以後，我們終於可以開始進行資料視覺化了！資料視覺化的定義因人而異，而這邊我想給出一個非常直觀的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 一些常見且肉眼容易識別的視覺變數 / 刻度（visual variables / scales）包含： 位置（x / y axis） 顏色（color） 大小（size） 透明程度（alpha） 填滿（fill） 形狀（shape） 用更口語的方式來解釋：在做資料視覺化的時候，我們希望能將 肉眼難以分析的資料 對應到： 肉眼容易解讀的視覺元素 透過這個映射關係，我們可以將原本的變數的數值變化也映射到視覺變數的變化。而因為我們人類容易區別視覺變數的變化（位置差異、大小長度變化 etc），我們能更容易地理解原始資料的樣貌、變化以及模式。 舉例來說，我們可以： 把不同捷運路線（文湖線、板南線）對應到不同顏色 把各國的 GDP 對應到點的大小 把某個資料的年份對應到 Ｘ 軸，越右邊代表越接近現代 一個簡單例子 事實上，我們可能平常每天都在做資料視覺化而不自知。比方說我們有一個數列 y ： y = [ - 2.055 , - 1.132 , - 0.522 , - 1.229 , 0.013 .. ] 光是看這個數字，肉眼無法看出什麼模式，但我們可以簡單畫個圖： 這邊我們利用視覺變數「Y軸位置」來呈現數值的變化，可以馬上看出數列裡頭的值都落在 -3 到 3 之間，而這是因為我們肉眼很容易辨別「位置」這個視覺變數的變化。 圖像的分層文法 在 A Layered Grammar of Graphics 裡頭， Hadley Wickham 闡述所謂的圖像（包含由資料視覺化產生的圖像）實際上如同我們平常使用的語言，是有文法的。而其文法可以拆成 7 個部分（層）。前述的 原始資料 = 資料層（Data） 視覺變數層（Visual variables = Aesthetics） 則恰好是這個架構裡頭最底下的兩層。視覺變數是我為了方便理解建立的名詞，在原文以及 ggplot2 裡頭被稱作 Aesthetics 。（中文翻作「美學」，當初看好久也無法理解啊 (╯°Д°)╯ ┻━┻） Image Credit : 圖像的分層文法 看到這你一定會「哇靠那我每次畫個圖都要實作七層？」。實際上不需要，上面幾層像是主題（Theme）比較像是裝飾品，給我們更大的自由與彈性來訂製（customize）視覺化結果。在下一節我們會看到，ggplot2 會自動幫我們設定合適的主題或座標。（如果沒特別指定的話） 但一般而言，一個圖像最基本的組成是底下三層。也就是除了前述的兩層（資料、視覺變數）以外還需要加上 幾何圖形層（Geometries） 為何還要這層？假如我們有了資料，決定了視覺變數（第二層，例：把資料中的變數 A 對應到 X 軸；變數 B 對應到 Y 軸）後，實際上就可以畫一個充滿點（point）的散佈圖了不是嗎？ 這樣的思維如同 資料視覺化是資料與圖的直接映射？ 部分所提到的，有所瑕疵。如果變數 A 是分類型變數（Categorical）的話，單純以 點 為圖形的散佈圖就會變得十分難以理解（下圖左）；這時候以 長條 為圖形（下圖右）的方式會比較清楚： 獨立幾何圖形層 : 讓資料視覺化不再侷限於「我要畫什麼圖」，而是「我想要怎麼畫」 將「幾何圖形」這個選擇獨立出來一層讓我們在資料視覺化的時候有更大的彈性。有了這些基本概念以後，我們可以開始嘗試使用 ggplot2 來實際做一些資料視覺化。 ggplot2 實踐 在這個章節裡頭我們將使用 Kaggle 的 Ramen Ratings 來做資料視覺化。這資料集紀錄了各國泡麵所得到的星星數。首先我們要先載入這次的主角：R 語言裡頭最著名的視覺化 package ggplot2。 dplyr 則是 R 語言用來處理資料的 package。 載入 packages library ( ggplot2 ) library ( dplyr ) 值得一提的是它們都是同屬於 TidyVerse 的一員。TidyVerse 是 R 裡頭常被用來做資料科學的 packages 的集合，以 Python 來說大概就像是 Pandas + Matplotlib + Numpy 的感覺吧。 載入資料 + 簡單資料處理 如下註解所示，這邊將資料集讀入，做一些簡單的資料型態轉變後選擇一部分的資料集（subset）來做之後的視覺化： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read . csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as . numeric ( ramen_all $ Stars ) # Subset 資料，選擇拉麵數量前幾多的國家方便 demo ramen <- ramen_all %>% filter ( Country % in % count(ramen_all, Country, sort = TRUE)[1:6, 1, drop=TRUE]) %>% filter ( Style % in % count(ramen_all, Style, sort = TRUE)[1:4, 1 , drop=TRUE]) 除了我們使用 dplyr 的 filter 依照條件 subset 資料集以外，值得一提的是 pipe 運算子 %>% 。它是前面提到的 TidyVerse 裡頭的 packages 共享的介面（interface），將前一個函示的輸出當作下一個函式的輸入，讓我們可以把運算全部串（chain）在一起。在 Linux 裡頭就是如同 | 的存在。 而實際我們的資料長這樣： head ( ramen ) Review.. Brand Variety Style Country Stars Top.Ten 2580 New Touch T's Restaurant Tantanmen Cup Japan 37 2579 Just Way Noodles Spicy Hot Sesame Spicy Hot Sesame Guan-miao Noodles Pack Taiwan 7 2578 Nissin Cup Noodles Chicken Vegetable Cup USA 16 2577 Wei Lih GGE Ramen Snack Tomato Flavor Pack Taiwan 19 2575 Samyang Foods Kimchi song Song Ramen Pack South Korea 47 2574 Acecook Spice Deli Tantan Men With Cilantro Cup Japan 39 簡單資料視覺化 有了資料，讓我們再確定一下資料視覺化的目的及觀眾： 目的：探索資料 觀眾：我們自己 這樣的條件讓我們知道視覺化的條件是快速做出結果，不需調整如標題、主題的設定。 現在讓我們問一些簡單的問題。像是 泡麵的包裝（碗裝、袋裝等）各佔多少比例？ 不同國家各有多少泡麵在資料集裡頭？ 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 其中一種能解決第一個問題的資料視覺化是： ggplot ( ramen , aes ( x = Style )) + geom_bar () 在 ggplot ( ramen , aes ( x = Style )) + geom_bar () 裡頭，我們實際上已經建構了圖表最基礎的三層元素： 資料層： ramen 告訴 ggplot2 使用此資料框架 視覺變數層： aes(x = Style) 告訴 ggplot2 我們將使用「 X 軸位置」這個視覺變數來反映泡麵包裝 Style 這個變數的變化 因為包裝的值有四種可能，你可以想像 ggplot2 已經準備好要幫你在 X 軸上的四個位置畫圖 aes 是我們前面提到 aesthetics 的縮寫 幾何圖形層： geom_bar() 告訴 ggplot 去計算對應到 x 視覺變數的變數裡頭，所有值的出現次數後將結果以 長條 來呈現 我們通常透過 + 來疊加不同層的結果。 基本層數缺一不可 上面的例子很簡單，但假如我們沒有指定幾何圖形層的話，圖會長什麼樣子呢？ ggplot ( ramen , aes ( x = Style )) 就像我們剛剛所說的，雖然 ggplot2 已經知道要用什麼資料框架、要用什麼視覺變數，不知道要用什麼圖形表示的話就會是空白一張圖。 另個簡單例子 讓我們依樣畫葫蘆，來解決第二個問題： 不同國家各有多少泡麵在資料集裡頭？ ggplot ( ramen , aes ( x = Country , fill = Style )) + geom_bar () + coord_flip () 這邊有兩個值得注意的地方： 除了基本的三層以外，我們透過 + coord_flip() 額外對座標層（Coordinates）做操作，請 ggplot2 把 x, y 軸互換。 透過 aes(..., fill = Style) 裡頭的 fill = Style ，我們告訴 ggplot2 將長條圖裡頭的填滿空間（fill）這個視覺變數，依照泡麵包裝（Style）做變化 第二點是在做資料視覺化的時候，想辦法增加 資料墨水量（Data Ink Ratio） 的例子。透過增加顯示在同張圖上的變數數目，進而提高該圖能傳達的訊息量。 舉例而言，我們可以很明顯地看到，在這資料集裡頭，台灣的杯裝泡麵（Cup）沒有被記錄到多少；而日本被記錄到的泡麵量最多，且袋裝（Pack）數目最多。這些是在我們沒有用「填滿」這個視覺變數時無法察覺的。而在 ggplot2 裡，要實現這種視覺化非常容易。 複雜例子 讓我們解決最後一個問題： 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 資料視覺化一個有趣的地方就是：同個問題不同的人會有不同的做法。而針對這問題其中一種做法是： 將包裝 Style 對應到 X 軸、星星數 Stars 對應到 Y 軸，然後使用長條 geom_bar 顯示數值 依照每個國家重複步驟一 而 ggplot2 的實作為： ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) + facet_wrap ( ~ Country ) 實際上在上面的程式碼裡頭，我們多操作了額外兩層： 統計層（Statistics）：專門負責匯總資料 小平面層（Facets）：依照選定的變數分別畫圖，如上述的步驟二 首先 ggplot2 的 geom_bar 預設只需要 x 視覺變數，因為匯總資料的統計層會把 x 依照不同的值分別計數（也就是各個包裝的數量），然後讓 geom_bar 顯示。但我們並不希望 geom_bar 使用這個數值，因此使用 geom_bar 裡頭的 stat = \"identity\" 是告訴統計層不要分別計數，而是使用我們給定的星星數 y 。 而 facet_wrap( ~ Country) 則是告訴小平面層依照 Country 這個變數重複畫 ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) 注意所有的圖的 x, y 軸都是一致的，方便我們做比較。 結語 資料視覺化需要統計知識以及設計美感，涵蓋範圍非常廣大。這篇雖然打了落落長，但真的只有碰到皮毛（淚）。資料視覺化感覺都可以打個系列文了。但最後再次重申資料視覺化的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 總之先確認你的觀眾與目的，選好你想要觀察的變數，選擇適當的視覺變數做可視化吧！ References DataCamp - Data Visualization with ggplot2 (Part 1) r-statistics.co - ggplot2 tutorial Safari - Data Visualization in R With ggplot2","tags":"Miscellaneous","url":"https://leemeng.tw/data-visualization-from-matplotlib-to-ggplot2.html"},{"title":"利用 Kinesis 處理串流資料並建立資料湖","text":"所謂的 資料湖 (data lake) 指的是一企業裡頭所有形式的資料的集合。這些資料包含原始資料 (raw data)，以及經過轉換的衍生資料 (derived data)。 資料湖的核心概念是將所有可用的資料全部整合在一個邏輯上相近的地方以供企業自由結合並做各式各樣的運用。資料湖可以用很多方式建立，這裏我們主要介紹如何利用 Amazon Kinesis 將串流資料 (streaming data) 載入資料湖。 概觀 資料湖概念上可以說是企業的所有資料的最終目的地。現在假設我們打算以 Amazon S3 中作為我們的資料湖，問題就變成：要如何將串流資料穩定地傳到 S3。這部分我們將透過 Amazon Kinesis 來達成。 Kinesis 本質上是跟 Apache Kafka 類似的 message broker ，將訊息依照 message producers 產生的順序傳遞給 message consumers。實際上資料的流動會如下圖所示： Simple Dataflow : 將 streaming data 透過 Kinesis 保存在 S3 上圖有幾點值得說明： 作為一個簡易的 demo，這邊我們的串流資料產生者 (streaming data producer) 是一個簡易 python script Streams 指的是 Amazon Kinesis Data Streams 。在 Kinesis 架構裡頭，一個 data stream 通常代表一個主題 (topic)， 跟這個主題相關的 producers 會把資料傳入該 stream 以讓該主題的 consumers 之後能接受訊息。 Firehose 指的是 Amazon Kinesis Data Firehose ，是專門把接受到的串流資料寫入 AWS 上的資料存放區（如 S3、Redshift、ElasticSearch）以供後續分析的服務。 建構流程 要完成上述的資料傳輸 pipeline，我們會 follow 以下步驟： 建立一個 Kinesis data stream 建立一個 Firehose delivery stream 用 Python 傳串流資料 確認 S3 上的資料 在每個步驟裡頭會稍微澄清一些概念。 建立一個 Kinesis data stream 現在假設有一個名為 naive-app 的應用程式，我們想要把使用者在上面做的操作紀錄下來。這時候我們可以建立一個新的 Kinesis Data Stream 來接受 app 的 streaming data。這邊指的 streaming data 是使用者存取應用程式時產生的 access log。 Scalability 這邊最重要的是 Number of shards 的設定。Kinesis 將接收到的資料以 log 的方式儲存在硬碟上，而為了提高 scalability，Kinesis 利用 Partitioning 的概念將 log 切割成多個部分並分配到不同的 shards 上，再將這些 shards 分別存在不同機器上面以提高 read/write capacity。因此我們可以理解一個 Kinesis Stream (Topic) 的資料吞吐量 (throughput) 直接受到 shard 的數目影響： shard 數目越多，同時能處理 read/write 的機器越多，資料吞吐量越高。 How to scale 理想上是一開始就掌握該 Stream/Topic 需要的資料吞吐量，進而決定最佳的 Number of shards ，但有時候事與願違。事後想要改變 shard 數目時需要透過 AWS Streams API 做 Resharding。Resharding 實際上就是在改變 shard 數目：增加 shard 會讓已存在的 shard 再度被切割；減少 shard 則會合併已存在的 shard。 在這邊我們就只直接使用一個 shard for demo。 Availability 另外值得一提的是 Kinesis 為了避免資料損失，會在三個不同的 availability zones 進行資料的 replication。因為這個額外的 overhead 可能使得在同樣設定下， Kinesis 比 Kafka 慢 的情況。因為是 log-based message broker，資料會被暫時存在硬碟上，預設保留 24 小時，而最多可以付費提升到維持 7 天以用來 replay data。 建立一個 Firehose delivery stream 有了接受 naive-app 串流資料的 Kinesis stream 以後，我們要建立一個 Firehose delivery stream 來接收 Kinesis stream 的資料。 Firehouse delivery stream 簡單來說是一個將串流資料存到 AWS 資料存放區的服務（如 S3、Redshift、ElasticSearch）。因此除了 Kinesis stream 的串流資料 以外，當然也可以接其他的串流資料： CloudWatch 的 log AWS IoT 使用者自定義的串流資料 在這篇裡頭我們的串流資料是 Kinesis stream，因此 Source 選擇 Kinesis stream 並填入我們剛剛建立的 stream 名稱： naive-app-access-log 。 值得一提的是 Firehose delivery stream 會 auto-scale，並不像 Kinesis stream 要手動調整 shard 數目。不過當然傳越多花越多。 如上張圖所示，實際上 Firehose 還允許我們在 delivery stream 接受到串流資料以後把原始資料傳到指定的 Lambda function 做進一步的轉換。 但因為我們想要資料湖儲存原始的串流資料，這邊我們省略這步驟。 Configuration 實際上 Firehose 不會一接收到資料就進行資料轉移。我們可以設定 Buffer size 以及 Buffer interval 讓 Firehose 在達到其中一個條件的時候把接收到的訊息統整起來一次做資料的轉移 (batch processing)。這邊為了能讓 Firehose 盡快把收到的資料轉移到 S3，設定 Buffer interval 為 60 秒。 選擇 delivery stream 目的地 在設定好 Firehose delivery stream 的串流資料來源（e.g., Kinesis stream）以及基本設定以後，我們要決定串流資料的目的地。這邊基本上很直覺， Destination 選擇 Amazon S3 以及想要放資料的 bucket 即可。 比較需要注意的是我們可以指定此 Firehose delivery stream 在放資料進入 bucket 時要為檔案加什麼前綴。 假設未來其他的串流資料我們也想要統一放在 me-data-lake 這個 bucket 裡頭。為了方便管理，我們可以為每個 delivery stream 設定一個識別用的 Prefix。以 naive-app 來說，我們指定 Prefix 為 naive-app-access-log/ 。加上 Firehose 預設的 YYYY/MM/DD/HH/ ，該 stream 的每個檔案的路徑就會變成如下圖的 naive-app-access-log/YYYY/MM/DD/HH/file_name 。 加入 Prefix 後實際將串流資料存入 S3 時的檔案路徑 用 Python 傳串流資料 確保 Kinesis stream -> Firehose delivery stream -> S3 的資料流設定以後，我們可以寫一個簡單的 Python script 實際傳資料進 Kinesis stream 做測試。但首先先讓我們使用 AWS SDK for Python 實作一個寄訊息給 Kinesis stream 的 function write_to_stream ： import boto3 import json def write_to_stream ( event_id , event , region_name , stream_name ): \"\"\"Write streaming event to specified Kinesis Stream within specified region. Parameters ---------- event_id: str The unique identifer for the event which will be needed in partitioning. event: dict The actual payload including all the details of the event. region_name: str AWS region identifier, e.g., \"ap-northeast-1\". stream_name: str Kinesis Stream name to write. Returns ------- res: Response returned by `put_record` func defined in boto3.client('kinesis') \"\"\" client = boto3 . client ( 'kinesis' , region_name = region_name ) res = client . put_record ( StreamName = stream_name , Data = json . dumps ( event ) + ' \\n ' , PartitionKey = event_id ) return res write_to_stream 基本上是把一個 Python dict event 利用 json.dumps 轉成字串後傳到指定的 region 的 Kinesis stream 裡的函式。（完整的 Gist ） 這邊值得注意的是 Data=json.dumps(event) + '\\n' 裡頭的 '\\n' 。如果之後想要利用 AWS Glue 或者 Athena 來進一步分析此串流資料的話，推薦在代表一個 event 的字串後面加上換行符號以維持「一行一事件」的資料形式，方便 schema 的自動產生。 範例日誌檔案內容會像是這樣： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 另外值得一提的是因為 Kinesis 背後是使用 Hash partitioning 來分配資料到 shard，基本上 PartitionKey=event_id 裡頭的 event_id 只要每個訊息都是獨一無二的，就能保證資料能「平均地」分配到各個 shard 上。 有了此函式以後，我們可以實際傳一些訊息進 Kinesis stream： while True : event = { \"event_id\" : str ( random . randint ( 1 , 100000 )), \"event_type\" : random . choice ([ 'read_post' , 'write_post' , 'make_comments' ]), \"timestamp\" : calendar . timegm ( datetime . utcnow () . timetuple ()) } # send to Kinesis Stream event_id = event [ 'event_id' ] write_to_stream ( event_id , event , REGION_NAME , KINESIS_STREAM_NAME ) time . sleep ( 5 ) 假設我們的 naive-app 可以讓使用者讀文章、寫文章以及寫評論，則上面的程式碼是模擬使用者使用 naive-app 時產生的事件，並將該事件的內容傳到 Kinesis stream naive-app-access-log 。60 秒內幾筆產生的事件如下： {'event_id': '56262', 'event_type': 'write_post', 'timestamp': 1522740951} {'event_id': '35672', 'event_type': 'make_comments', 'timestamp': 1522740956} {'event_id': '71613', 'event_type': 'read_post', 'timestamp': 1522740962} {'event_id': '48160', 'event_type': 'make_comments', 'timestamp': 1522740967} {'event_id': '96093', 'event_type': 'write_post', 'timestamp': 1522740972} 確認 S3 上的資料 注意因為上面的 5 個事件在 $5 * 5 = 25$ 秒內就產生了。且因為我們前面設定 Firehose delivery stream 的 Buffer interval 為 60 秒，Firehose 會把以上的事件的訊息全部串接起來，放到一個檔案裡頭，而不是分成五個檔案： 而實際檔案的內容如下： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 結語 到這邊為止成功把（偽）串流資料透過 Kinesis 存到 S3 了！為了方便之後的應用，輸出的檔案的內容格式或許還可以再改進，但資料湖的其中一個想法是 Command Query Responsibility Segregation (CQRS) ，也就是在存放資料的時候就只專心丟資料，不去在意之後資料會被以什麼方式、schema 使用，可以保證之後實際應用資料時有最大的彈性。 另外在確保資料好好地儲存在資料湖以後，我們通常會實際針對串流資料再進行一些處理 / 分析像是： 放到 Elasticsearch 並用 Kibana 做 Visualization 觸發 Lambda function 做進一步處理 使用 Athena 做 ad-hoc 分析 ... 但這邊時間有限，之後有機會再來記錄資料湖之後的分析筆記。 References Youtube: Introduction to Amazon Kinesis Firehose sumologic - Kinesis Stream vs Firehose A Cloud Guru - difference betwwen Kinesis Streams and Kinesis Firehose Getting started with AWS Kinesis using Python opsclarity - Evaluating Message Brokers: Kafka vs. Kinesis vs. SQS","tags":"Miscellaneous","url":"https://leemeng.tw/use-kinesis-streams-and-firehose-to-build-a-data-lake.html"},{"title":"AWS Data Migration Service - 從 MongoDB 遷移到 Redshift","text":"同樣一份資料因應不同的使用案例，可能需要使用不同的存取方式。而針對這些不同的存取方式，我們通常需要選擇最適合的資料庫來最佳化使用者體驗。 這篇文章將簡單介紹如何使用 AWS Database Migration Service (以下簡稱 AWS DMS )來快速地達到我們的目標：將 MongoDB 資料遷移到 Redshift 上。 使用案例 舉例來說，一個電子商務網站的後端可以使用一個具有高度彈性的 NoSQL 資料庫如 MongoDB 來應對變化快速的使用者需求；而公司內部的資料科學家可以利用資料倉儲如 Redshift 來找出 business insight 。但這時候一個問題產生了：資料科學家用的資料倉儲 (例：Redshift) 的資料哪裡來？ 常見的方式是對 MongoDB 裡頭的資料定期做 ETL 以後將轉換過後的資料載入 Redshift 供分析需求。理論上在做 ETL 時要依照資料倉儲的 Data Model 重新設計 Tables (例： Star Schema )，但為了能在最短的時間將 MongoDB 上的資料轉到 Redshift 進行一些 Query，這篇文章將簡單介紹 AWS DMS 的運作方式，以及如何運用它來實際進行資料遷移所需要的步驟。 AWS DMS : 遷移（並轉換） AWS 上的資料庫 AWS DMS 基本介紹 DMS 基本上運作方式就是幫我們啟動一台 EC2 機器 (稱之為 replication instance) ，然後在上面跑 replication task(s) 。 一個 instance 上可以有多個 tasks 進行資料遷移。 instance 則分別透過 Source Endpoint / Target Endpoint 連結來源 / 目標資料庫。在後面我們會看到， endpoints 實際上就只是告訴 AWS DMS 的 replication instance 如何連結到實際的資料庫的設定罷了。在我們的例子裡頭，來源 / 目標資料庫分別對應到 MongoDB / Redshift 。 DMS 基本運作方式 : 資料遷移是由在 Replication Instance 上執行的 Replication Task 透過 endpoints 連結來源/目標資料庫完成的 基本遷移步驟 在假設來源 / 目標資料庫已經在運作的情況下，如同 AWS DMS 的 Get started, 一般會進行以下步驟來遷移資料： 建立 replication instance 確保 replication instance 能連結到來源 / 目標資料庫 定義 replication task Debugging：確保一切運作正常 以下針對每個步驟，我會紀錄一些需要注意的地方。 建立 replication instance 點擊 AWS DMS 介面的 Get started 選項會請我們建立新的 instance: 建立 Replication Instance : 注意 VPC /設定 這步驟基本上沒什麼問題， replication task 會佔用大量的 CPU 以及記憶體資源，理想上是依據需求選擇 Instance class ，不過第一次測試功能的話用預設的 t2.medium 即可。這邊值得注意的是 VPC 以及下面進階選項的 VPC Security Group(s) 設定。 如果來源 / 目標資料庫都可供公開存取的話，基本上不需要 VPC 。但一般來說我們都會有安全考量，也就是要求所有在 AWS 上的資源都要套用安全設定，則最簡單的架構是將來源資料庫、目標資料庫以及 Replication Instance 都放入同一個 VPC ，並利用 security group 設定來允許該 Instance 存取兩個資料庫。概念上此 VPC 的架構會如下 ( sg 為 Security Group 之縮寫 )： 來源 / 目標資料庫所在的 Security Group 要允許 Replication Instance 所在的 Security Group 存取 以上圖為例， Security Group sg_mongodb 以及 sg_redshift 的 Inbound Rule 要允許 sg_replicate 存取。而允許存取的 Port 則依照資料庫實際使用的 Port 設定即可 (例： MongoDB 慣用 27017； Redshift 則是 5439)。最後別忘了在建立 replication instance 的進階設定的 VPC Security Group(s) 選擇 sg_replicate 。 另外你可能已經注意到上圖的 S3 bucket 。就 replication tasks 的 log 來看， AWS DMS 在遷移資料的時候實際上會再細分為兩步驟： Replication Task 將來源資料庫的資料載出、轉換並暫存到 S3 Task 將存在 S3 的資料載入目標資料庫 雖然 ClodWatch 需要額外收費，但為了方便除錯，建議使用。在文章後面的 Debugging 我們會實際看一些例子。 確保 replication instance 能連結到來源 / 目標資料庫 上一步驟設定好以後， AWS DMS 會馬上幫我們建立一個新的 replication instance。在等待的同時我們可以開始設定資料庫的 endpoints。 設定來源 / 目標 enpoints : 在此步驟確保 Replication Instance 可以連到兩個資料庫可以減少除錯時間 這步驟基本上依照資料庫的不同，需要的輸入的項目可能不一樣。不過值得一提的是，在 建立 replication instance 的時候我們已經讓來源 / 目標資料庫以及 replication instance 都待在同個 VPC 裡頭。假設我們的 MongoDB 是運行在該 VPC 裡頭的某個 EC2 instance 之上，要允許在同個 VPC 的 replication instance 存取該 EC2 instance，我們要在 Server name 選項輸入運行 MongoDB 的 EC2 的 Private IP (上圖第一個紅框)。 MongoDB as Source Database 當 MongoDB 為來源資料庫時有一些值得注意的事情可以參考 官方文件 。以下會說明一些值得特別注意的地方。 Metadata mode Metadata mode 預設為 document （上圖第二個紅框），也就是把 MongoDB 裡頭的 json-formated 文件放到 Redshift 裡頭對應 Table 的一個 _doc 欄位。假設 MongoDB 裡有一個 users collection ，裡頭存了以下文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 將會被以下的格式載入 Redshift： _doc | --------------------------- {\"user\": \"leemeng\", \"fav .. 而這通常不是我們要的。將 metadate mode 設定為 table 模式能讓 AWS DMS 把文件裡頭的欄位扁平化後放入對應的欄位(column)： user | favorite | a.b | unnecessary_field ------------------------------------------------- leemeng | chocolate | For fun!| Don't include me! 注意到這邊有一個我們不需要遷移到 Redshift 的 unnecessary_field 。在後面的 Transformation Rules 我們會了解怎麼辦該欄位去除。 Numbers of documents to scan 而 Numbers of documents to scan 選項則讓我們決定要讓 AWS DMS 拿多少文件來決定要建立哪些欄位。如果要遷移的 MongoDB collection 的文件 schema 很常被更動（常有新鍵值）的話，建議可以讓 AWS DMS 掃描多一點文件來建立足夠的欄位。 Redshift as Target Database 如果按照 AWS DMS 的 Get started 一步一步走的話基本上沒有問題。要注意的是 Redshift 要有允許 DMS 存取的 AMI Rule，否則會出錯。 定義 replication task 在確定 replication instance 可以連線到兩個資料庫後，可以開始建立我們的 replication task： 這邊我們可以看到有三種資料遷移方式 (圖中的 Migration type)： 遷移目前 MongoDB 的資料 同上，但在遷移後之後繼續同步 MongoDB & Redshift (前提是 MongoDB 要以 Replica Set 模式執行) 只把 Task 啟動後 MongoDB 的資料變動遷移到 Redshift 這邊選擇自己的想要的遷移方式即可。接著我們要告訴 AWS DMS 想要進行遷移的 MongoDB Collections 以及在想要做的簡單轉換。兩者分別透過 Selection Rules 還有 Transformation Rules 定義。 Selection Rules Selection Rules 的用途是告訴 AWS DMS 該遷移以及不要遷移的 (MongoDB) collections 。我們可以定義一個 general rule 讓一個 task 處理某個 db 的所有 collections ；也能讓一個 task 只負責一個 collection 的遷移。後者的設定比較花時間但是彈性比較高，可以依照不同 collection 特性決定遷移的方式。 下圖是定義一個 rule 告訴 AWS DMS 遷移所有在 MongoDB 的 Collections。另外如果想要排除哪個 collection 的話就新增一個 rule 並在 Action 選擇 Exclude 。基本上想要加幾個 Selection Rules 都可以。而 Exclude Rules 的效果是在所有 Include Rules 後套用。 Selection Rules : 選擇要遷移到目標資料庫的 Tables / Collections Transformation Rules 這邊所謂的轉換並不是對欄位的實際值 (value) 進行轉換，而是針對 Table / Column 層級做 排除（不遷移該 Table / Column） 幫 Table / Column 更名、大小寫轉換或是名稱加上 prefix / postfix 這種操作。下圖是將 users collection 裡頭不需要遷移的鍵值 uncessary_field 從 Redshift 排除的 rules: 透過這個 Transformation rule，我們上面 users collection 的範例文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 就會被轉成： user | favorite | a.b ------------------------------ leemeng | chocolate | For fun! 注意 uncessary_field 不會被存到 Redshift 裡頭。 Debugging 當建立並執行一個新的 replication task 後，我們可以從 Load State 看到每個 Table 載入的狀況。 Load State 有幾種可能的值： Before Loading Full Load Table completed Table error 當出現 Table error 時，我們可以先看 log 瞭解情況： 2018-03-28T01:23:30 [TARGET_LOAD ]E: RetCode: SQL_ERROR SqlState: XX000 NativeError: 30 Message: [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState XX000] ERROR: Load into table 'users' failed. Check 'stl_load_errors' system table for details. [1022502] (ar_odbc_stmt.c:4406) 依照不同的錯誤、不同的目標資料庫，實際的 log 內容會有所不同。以我們目標資料庫 = Redshift 的情況下，上面的 log 告訴我們 replication task 在載入 users Table 時出錯，詳情可以參考 Redshift 的 stl_load_error Table ( 官方文件 )： SELECT * FROM pg_catalog . stl_load_errors ORDER BY starttime DESC LIMIT 1 ; 查看 Redshift 裡頭 stl_load_error Table 來除錯 就這個錯誤例子來看， err_reason 的內容告訴我們有個 memo 欄位 ( colname ) 的值太長導致沒辦法載入 Redshift。這時候可以把正在運行的 replication task 暫停，用前面提到的 Transformation Rules 來去除該欄位。而基本上其他錯誤也能用類似的方式解決。 到這邊為止大致上應該可以順利把 MongoDB 的資料載入 Redshift 了。之後想到什麼再補充。","tags":"Miscellaneous","url":"https://leemeng.tw/replicate-data-from-mongodb-to-redshift-using-aws-data-migration-service.html"},{"title":"Designing Data-Intensive Applications (1) - 序言","text":"最近在拜讀 Martin Kleppmann 的 Designing Data-Intensive Applications ， 覺得受益匪淺，且我也相信透過 Feynman Technique 將學到的東西用最淺顯易懂的方式表達能幫助自己內化這些知識，遂嘗試把閱讀後的心得記錄在此。 另外在提到書內內容時都會盡量使用英文原文，不另做名詞的翻譯，以方便對照書內內容。 何謂 data-intensive applications 所謂的 data-intensive applications 如同名稱所示，專注在如何有效率地處理、儲存 密集資料 。通常一個這樣的系統的後端要用多種方式處理資料，而不是只用一個資料庫就結束了。（雖然對 end users 來說可能看起來像這樣） 舉個簡單例子，一個電子商務網頁的後端除了做為 OLTP 的 NoSQL 資料庫 (e.g., MongoDB) 以外，可能還有： 一個專門存放網頁快取的資料庫 (e.g. Redis) 給資料科學家分析用的資料倉儲 (e.g., Redshift) 處理 streaming events 的 messaging queue (e.g., Kafka) 定期將 NoSQL 資料庫的資料做 ETL 存到 資料倉儲的批次處理 (e.g., Hive jobs) 光是要把以上所列的資料庫 / 分散式系統 / 資料流 以有系統的方式組合起來就需要大量經驗，更遑論還要達到以下三個要求了： 可靠性 (reliable): 像是 zero-down time, 很短的回應時間 etc 規模性 (scalable): 即使之後資料量增加，系統也能很好地運作 維護性 (maintainable): 容易改善、新增功能的系統設計 Image Credit : 如何了解各個 data system 的優缺點並予以組合 儘管我們不可能熟悉所有資料庫以及分散式系統的細節，了解他們背後設計的核心理念、演算法以及大致上的運作方式能讓我們了解每個 data system 的特性以及優缺點，依照不同的使用案例選擇最適合的 data system 並予以組合。 何謂資料密集 書中所指的「密集」資料有以下所列的特徵（一個以上）： 大量資料 資料的（格式、 schema etc）變動速度很快 資料有複雜結構 針對「資料有複雜結構」以及「資料變動很快」這點，最為人所知的 solution 就是 NoSQL 等允許彈性 schema 的資料庫的崛起； 而針對「資料量很大」這點，則端看使用案例有各式各樣的資料庫、分散式系統。舉幾個例子： 能有效儲存大量資料的 Google BigTable 以欄 (column) 為單位儲存以壓縮大量資料的資料倉儲 Redshift Amazon 的 Single-leader Replication - DynamoDB 專門處理 realtime streaming data 的 Kafka, RabbitMQ etc. 如同前述，以上提到的系統依照它們想要解決的問題的特性，背後都會有一些假設以及 trade-off 。了解這些背後的原理可以讓我們了解哪些工具在什麼時候最 powerful 。 這本書主要分成三部分來闡述，抓到大方向會比較容易閱讀： 針對單一機器上的資料，有哪些常用的資料儲存/處理方法 類似前一部份，闡述針對分散式系統的資料儲存/處理方法 資料密集型應用：如何將多個 data systems 組合起來 一句話總結 在資料密集的時代，我們的最終目標在於如何將各式各樣的 data systems 以有系統的方式「組合」起來，以建立一個可靠、具規模性以及維護性的系統。","tags":"Miscellaneous","url":"https://leemeng.tw/designing-data-intensive-applications-1-preface.html"},{"title":"Google Data Studio 基礎","text":"Google Data Studio 是 Google 推出的一個 Dashboard / Reporting 的服務，讓我們可以利用多種 連結器 將儲存在如 Google Analytics、 Google 試算表及 Google BigQuery 等特定資料來源的資料做出漂亮的 visualization ，用資料講故事而不用自己設計 UI。公司內部雖然有自己的 dashboards 不過想說多試一些方案沒有壞處，而且現在 Data Studio 還是 Beta 版本，雖然介面是中文，說明文件還只有英文，想說把學到的一些技巧以及使用心得記錄下來。 將 Google 試算表的資料可視化 為了快速展示 Data Studio 的功能，我們將使用 政府資料開放平臺 上由交通部觀光局提供的 105年來台旅客性別統計 資料。將 CSV 檔案下載下來，稍微簡化格式後上傳到 Google 試算表以當作報表的資料來源。下圖是簡化後的資料： 資料來源 : 2016年來台旅客性別統計 每一列代表某地區 / 國家的訪台人數以及男女比 條件欄位應用 條件欄位 讓我們可以針對試算表裡頭每一列做 IF ELSE 判斷，依照判斷結果給予不同的值。現在假設我們想知道有多少國家的男性遊客過半數，可以使用簡易的評量表來計算： 訪台男性遊客過半國家佔全部國家的比例 我們發現高達八成的國家（有些是區域）的訪台男性遊客較女性為多。我們可以調查其他國家的訪客性別比，看是不是只有台灣有此現象。要產生分母的「國家數」很直覺，我們只要新增一個欄位並計算有幾個國家即可： 新增一個名為「國家數」的欄位 但要計算分子的「男性遊客過半國家數」就稍微 tricky 了。我們想做的是，針對每一國家（每一列），只有在該國訪台男性遊客百分比過半（超過 50%)的時候才會被納入結果。而 Data Studio 的 條件欄位 就是專門針對這種情況設計的。 使用 CASE 語法對每一列做 IF-ELSE 判斷 上面的公式用白話來說就是： 針對每一列的國家，看它的「男性百分比」欄位的值有沒有大於50。有的話值為1，否則為0。在針對每列做完條件判斷以後再把所有 1 加起來，就等於符合條件的國家數。 篩選器（filter）應用 根據上個分析，我們知道女性遊客過半的國家只佔 20%。假設我們想確切知道是哪些國家的女性遊客過半，可以從女性百分比最高的國家開始列出男女比： 訪台女性遊客過半國家 我們發現女性遊客過半的都是亞洲國家，或許我們可以簡單解釋成這些國家與台灣的距離短，適合女性遊客拜訪。而為了讓圖表易讀，上面這張組合圖額外建立一個篩選器來過濾掉男性遊客比女性多的國家： 新增一個篩選器以過濾男性遊客比例較高的國家 註：一般的長條圖可以直接透過設定限制長條圖數目 維度 VS 指標 在 Data Studio 裡頭，了解 維度跟指標的差異 很重要。 以我們現在的資料集為例，每一列就是一筆紀錄（record），每一行則是一個欄位。每個欄位則是維度或指標。 指標（Metric，底下藍色） 數值型欄位，有經過「匯總」，負責 quantify 資料 如「國家數」、「總人數」 維度（Dimension，底下綠色） 分類型欄位，負責 qualify 資料 如「國家」、「居住地」 fx 則代表是額外利用公式建立的欄位 像我們前面定義的「男性遊客過半國家數」欄位因為有經過 SUM 公式匯總成為一個數值，因此為一個指標（藍）。而如果我們透過 CASE 語法新定義一個「男性過半」欄位如下： 此欄位沒有經過匯總因此被視為維度，在上一張圖被標為綠色。因此一句話總結維度跟指標的功能就是： 維度負責「描述」資料； 指標則負責「衡量」資料。 資料透視表 (Pivot Table) 資料透視表很適合拿來看在不同條件下某個指標的表現。下圖是一個依照 居住地 國家 兩個維度計算「男性人數」指標的資料透視表： 依照 官方文件 有幾點值得注意： 資料透視表最多處理 50,000 筆資料，為了避免 scan 資料太花時間，可以額外建立一些篩選器 subset 資料 列維度跟欄維度最多可以分別設定 2 個維度（上例列欄各設定 1 個維度） 限制 可能因為還處在 beta 版本，在這篇文章寫的時候（2018/03）試用了一陣子發現 Data Studio 也有一些使用案例沒有辦法做到，像是： 篩選器（filter）只能設定像是「欄位 C 大於 X」這種條件，而不能做「當欄位 C1 > 欄位 C2」這種欄位間的比較。 同上，條件欄位也只能設定像是「欄位 C 大於某固定值 X」的條件 資料透視表包含的資料稍多 (> 2000筆)就開始變慢 .. 實戰演練 這篇文章用的報表連結在 此 ，可以自己試試不同 visualization。有任何 feedback 也歡迎聯絡。","tags":"Miscellaneous","url":"https://leemeng.tw/google-data-studio-basics.html"},{"title":"Pelican 實戰手冊(主題篇)","text":"有些人可能已經注意到這個部落格是用 Pelican 所寫成並且 host 在 Github 上。這篇主要紀錄如何使用 Jinja2 自訂主題。 Pelican 是一個用 Python 寫的靜態網頁生成器, 可以幫我們把 reStructedText, Markdown file 甚至 Jupyer notebook 轉成靜態的 HTML 檔案。 靜態網頁的好處就是我們不需要一個 web server 或者是資料庫來管理內容, 可以把 HTML 檔案 host 在想要的地方，比方說 Github Pages 。用 Pelican 官網一句來介紹的話就是： Pelican is a static site generator, written in Python, that requires no database or server-side logic. - Pelican Blog Google 一下你會發現除了 Pelican 以外還有很多其他像是 Jekyll, Hexo 等 靜態網頁生成器 。 之所以會選擇 Pelican 是因為以下幾點： Pelican 是用 Python 寫的，讓 Python 開發者（我）很容易客製化 可以把 jupyter notebook 轉成 HTML ，這對每天寫一堆 notebooks 的資料科學家很友善 主題是用強大的 Jinja2 模組引擎建立，可以用前人寫好的 主題 或是自己寫 templates，自由度很高，也是本篇重點。 如果你的需求類似而且想要自己架一個部落格，可以現在就跳入 Pelican Quickstart ，有問題再回來看這篇。 Jinja2 是 Python 知名的模組引擎 (templating engine)，可以有系統地產生 HTML，很常出現在 Flask 或是 Django Apps 裡頭。以下介紹在建立 Pelican blog 時常用到的功能。 再利用 HTML 區塊 比方說我們可以建立一個汎用的 template base.html 來定義整個部落格共用的資訊，像是 header 裡頭要 import 的 css / favicon 等等： <!DOCTYPE html> < html lang = \"en\" > < head > {% block head %} < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> {% endblock head %} </ head > < body > {% block content %} < p > 部落格內容 </ p > {% endblock content %} </ body > 注意到上面的 {% block head %} jinja2 語法。會在多個 HTML 檔案重複使用的部分我們可以用 {% block BLOCKNAME %} 以及 {% endblock BLOCKNAME %} 包起來，然後在獨立顯示一篇文章的 article.html 裡頭我們可以定義： { % extends \"base.html\" % } { % block head % } {{ super () }} < title > 文章標題 </ title > { % endblock head % } < body > { % block content % } < p > 文章內容 </ p > { % endblock content % } </ body > 上面的 code 基本上是告訴 jinja2 article.html 要繼承 base.html 的所有內容，而在 head block 除了用 {{ super() }} 繼承 base.html 的內容以外，在下面再追加新的內容。而 content block 則是完全取代。 因此最後 article.html 會被渲染成： <!DOCTYPE html> < html lang = \"en\" > < head > < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> < title > 文章標題 </ title > </ head > < body > < p > 文章內容 </ p > </ body > 為當前文章取得前/後一篇文章連結 Pagination 範例: 顯示前後文章連結 依照主題不同，有些主題可能文章頁面裡頭並沒有提供前一篇/後一篇文章的連結。要像上圖為每一篇文章取得前後文章的連結，可以在 article.html 裡存取 articles Variable 並使用 jinja2 namespace 來取得前後文章( namespace 要在 jinja 2.10+ 以後才能使用) { # get prev- and next-article for pagination #} { % set ns = namespace ( found = false , prev = None , next = None ) % } { % for a in articles % } { # 要使用 break 要安裝 extension, 最佳化效率可省略 #} { %- if ns . found % }{ % break % }{ % endif % } { # 假設文章標題不會重複, unique #} { % if a . title == article . title % } { % set ns . found = true % } { % set ns . prev = loop . previtem % } { % set ns . next = loop . nextitem % } { % endif % } { % endfor % } 上面的 code 會 iterate 所有文章，當遇到當前文章的時候利用 loop.previtem 以及 loop.nextitem 把前後文章記下來。 jinja2 預設是無法在 loop 裡頭改變變數的值 ，但使用 namespace 即可。 接著就能利用剛剛取得的前後 article 物件來渲染前後連結： {# 方便起見的 assignment %} {% set prev_article = ns.prev %} {% set next_article = ns.next %} {% if prev_article %} < div > < a href = \"prev_article.url\" rel = \"prev\" > < span > Previous Post </ span > {{ prev_article.title }} </ a > </ div > {% endif %} {% if next_article %} < div > < a href = \"next_article.url\" rel = \"next\" > < span > Next Post </ span > {{ next_article.title }} </ a > </ div > {% endif %} 傳參數給子 template 有時候多個 templates 會使用類似的 HTML，像是當首頁 index.html 以及部落格 blog.html 都用相同格式渲染最新幾篇文章時，我們可以定義一個 article_entries.html 如下： { # 簡化版 #} { % for article in articles % } < article class = \"col-block\" > < a href = \"{{ SITEURL }}/{{ article.url }}\" > {{ article . title }} </ a > < p > {{ article . summary }} </ p > </ article > { % endfor % } 注意這時候如果直接在 index.html 使用 { % include 'article_entries.html' % } 是會出現 錯誤 的。理由是被 include 的 article_entries.html 看不到定義在 index.html 的 articles 變數。 解決方法是在 index.html 裡透過 {% with %} 語法定義一個 scope： { # 選擇前五篇文章來渲染 #} { % set articles_to_show = articles_page . object_list [ 5 ] % } { # 定義 scope #} { % with articles = articles_to_show % } { % include 'article_entries.html' % } { % endwith % } 使用 with 的好處是可以把子 template article_entries.html 當作 function 來使用，我們可以依照母 template 的需要，傳進想要渲染的文章即可。 Reference Jinja2 Extension","tags":"Miscellaneous","url":"https://leemeng.tw/build-a-pelican-powered-blog-like-a-pro.html"},{"title":"BeautifulSoup 筆記","text":"Beautifulsoup 是一個可以幫助我們 parse HTML 的 lib, 這篇主要紀錄使用 beautifulsoup 時常用的指令。 安裝 pip install beautifulsoup4 下載一個網頁並爬出特定內容 這邊假設我們想要把維基百科上的 「國家區域代碼」 的表格爬下來，並轉成一個 Pandas 的 Dataframe： 取得某個頁面的 HTML 字串 import urllib from bs4 import BeautifulSoup import pandas as pd html = urllib . request . urlopen ( \"https://zh.wikipedia.org/zh-tw/ISO_3166-1\" ) . read () soup = BeautifulSoup ( html , 'html.parser' ) 利用 class 從該 HTML 裡取得特定表格 table = soup . find ( 'table' , { 'class' : 'wikitable sortable' }) 產生欄位名稱 columns = [ th . text . replace ( ' \\n ' , '' ) for th in table . find ( 'tr' ) . find_all ( 'th' )] columns ['英文短名稱', '二位代碼', '三位代碼', '數字代碼', 'ISO 3166-2', '中文名稱', '獨立主權'] 產生每個國家的對應資料 trs = table . find_all ( 'tr' )[ 1 :] rows = list () for tr in trs : rows . append ([ td . text . replace ( ' \\n ' , '' ) . replace ( ' \\xa0 ' , '' ) for td in tr . find_all ( 'td' )]) rows [: 5 ] [['Afghanistan', 'AF', 'AFG', '004', 'ISO 3166-2:AF', '阿富汗', '是'], ['Åland Islands', 'AX', 'ALA', '248', 'ISO 3166-2:AX', '奧蘭', '否'], ['Albania', 'AL', 'ALB', '008', 'ISO 3166-2:AL', '阿爾巴尼亞', '是'], ['Algeria', 'DZ', 'DZA', '012', 'ISO 3166-2:DZ', '阿爾及利亞', '是'], ['American Samoa', 'AS', 'ASM', '016', 'ISO 3166-2:AS', '美屬薩摩亞', '否']] 產生 Dataframe df = pd . DataFrame ( data = rows , columns = columns ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 英文短名稱 二位代碼 三位代碼 數字代碼 ISO 3166-2 中文名稱 獨立主權 0 Afghanistan AF AFG 004 ISO 3166-2:AF 阿富汗 是 1 Åland Islands AX ALA 248 ISO 3166-2:AX 奧蘭 否 2 Albania AL ALB 008 ISO 3166-2:AL 阿爾巴尼亞 是 3 Algeria DZ DZA 012 ISO 3166-2:DZ 阿爾及利亞 是 4 American Samoa AS ASM 016 ISO 3166-2:AS 美屬薩摩亞 否 找出特定 HTML 物件 假設我們有一個字串代表一個表格： html = \"\"\"<div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div>\"\"\" 渲染成 HTML: x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 實際 HTML 架構： < div > < table border = \"1\" class = \"dataframe\" > < thead > < tr style = \"text-align: right;\" > < th ></ th > < th > x </ th > < th > y </ th > </ tr > </ thead > < tbody > < tr > < th > 0 </ th > < td > -2.863752 </ td > < td > -1.066424 </ td > </ tr > < tr > < th > 1 </ th > < td > -0.779238 </ td > < td > 0.862169 </ td > </ tr > </ tbody > </ table > </ div > 利用 BeautifulSoup 物件 parse HTML: from bs4 import BeautifulSoup soup = BeautifulSoup ( html , 'html.parser' ) soup <div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> 找到第一個符合條件的 table 標籤 table = soup . find ( 'table' , { 'class' : 'dataframe' }) table <table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table> 設定新屬性 / class 因為這時候我們取出來的 table 物件是 reference 到 soup 裡頭對應的物件, 只要直接改變對應的 attr 就會直接反映結果到 soup 物件: table [ 'class' ] = table [ 'class' ] + [ 'table' , 'table-striped' , 'table-responsive' ] soup <div><table border=\"1\" class=\"dataframe table table-striped table-responsive\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> Iterate 標籤裡頭的子標籤 for c in table . children : print ( f ' {c.name} in {table.name} ' ) thead in table tbody in table 移除標籤 這邊假設我們要移除表格裡頭第一行的值 ( 第2個 tr 標籤 ), 可以對要移除的標籤物件使用 extract() func. x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 for i , tr in enumerate ( soup . findAll ( 'tr' )): if i == 1 : tr . extract () x y 1 -0.779238 0.862169 建立新標籤 假設我們想要建立一個新的 blockquote 標籤，並加入一些文字： text = 'I love BeautifulSoup!' blockquote = soup . new_tag ( 'blockquote' ) blockquote . append ( text ) blockquote <blockquote>I love BeautifulSoup!</blockquote>","tags":"Miscellaneous","url":"https://leemeng.tw/beautifulsoup-cheat-sheet.html"},{"title":"Seaborn 筆記","text":"這篇記錄我在使用 seaborn 做資料分析還有 visualization 時常用的 code. 一般慣例會把 seaborn 更名成 sns for reference. % matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt 基本設定 這邊值得注意的是要調整的參數要一次全部設定, 用好幾次 set() 的話只有最後一次的 set() 的結果會被保留 sns . set ( font = 'IPAPMincho' , font_scale = 1.8 ) Histogram data = np . random . randn ( 1000 ) data [: 10 ] array([-0.53267554, 0.03851161, -0.16072742, -0.70889663, 0.23085979, -1.61295347, -0.46508874, 0.60112507, 0.42017249, -0.73656917]) seaborn 是建立在 matplotlib 之上, 因此 matplotlib 也可以直接拿來跟 seaborn 產生的圖互動 plt . figure ( figsize = ( 10 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . title ( 'Defualt style with kde' ) sns . distplot ( data , kde = True ); plt . subplot ( 1 , 2 , 2 ) sns . set_style ( 'dark' ) plt . title ( 'Dark style without kde' ); sns . distplot ( data , kde = False ); Scatter plot df = pd . DataFrame ({ 'x' : np . random . randn ( 100 ), 'y' : np . random . randn ( 100 )}) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 2 0.016786 -0.016519 3 0.948504 0.298314 4 2.029428 1.211997 要使用 seaborn 初始設定就再呼叫一次 set() sns . set () 注意點： 一般用lmplot畫, 然後設定 fit_reg=False 就可以讓 regression line 消失. 有時候有沒有那條線影響圖很大 一樣先 x , 再 y for fit_reg in [ True , False ]: sns . lmplot ( 'x' , 'y' , data = df , fit_reg = fit_reg , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) title = 'Show regression line' if fit_reg else 'Without regression line' plt . title ( title ) 想要將兩個 lmplot 並排 render 可以參考這個 stackoverflow 答案 . Correlation matrix / Heatmap df = pd . DataFrame ({ 'x1' : np . random . randn ( 100 ), 'x2' : np . random . randn ( 100 ), 'x3' : np . random . randn ( 100 ) }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 0 1.269566 0.349083 -0.000743 1 -1.634587 0.072568 0.042596 2 -0.581238 -0.337935 -0.412084 3 -0.080881 -1.376481 1.361046 4 -0.609886 -1.061285 0.265788 這邊利用 pandas 本身的 corr() 計算 correlation matrix 然後使用 seaborn 做 vis. corr = df . astype ( float ) . corr () corr .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x1 1.000000 -0.034731 0.032407 x2 -0.034731 1.000000 -0.192169 x3 0.032407 -0.192169 1.000000 sns . set ( font_scale = 1.5 ) sns . heatmap ( corr , cmap = 'Blues' , annot = True , annot_kws = { \"size\" : 15 }, xticklabels = corr . columns . values , yticklabels = corr . columns . values ); References API References","tags":"Miscellaneous","url":"https://leemeng.tw/seaborn-cheat-sheet.html"},{"title":"SQLite 筆記","text":"Table of Contents Prettier output 調整每一個 column 寬度 在 sqlite3 shell 裡清空畫面 使用 SQL script 建立 tables 顯示目前的 tables 顯示 table schema 顯示 indexes 這篇主要紀錄使用 SQLite shell 下 SQL Query 的指令。基本上在 shell 裡頭都是用 dot-command, 使用 .help 可以顯示所有可用的指令. Prettier output 在 command-line program 裡頭使用的 response format .mode column .headers on Example output Code Name Price Manufacturer ---------- ---------- ---------- ------------ 7 CD drive 90 2 9 Toner cart 66 3 調整每一個 column 寬度 .width 5 18 15 缺點是不同的 tables, 不同的 columns 需要的寬度不同, 要自己調整 要重置設定: .width 0 在 sqlite3 shell 裡清空畫面 要看 OS 決定實際的 shell command .shell clear 除了 clear 以外, 其他 shell command都能使用, e.g., .shell cd 使用 SQL script 建立 tables 比方我們有一個 create_tables.sql 內容是： CREATE TABLE Departments ( Code INTEGER PRIMARY KEY, Name varchar(255) NOT NULL , Budget decimal NOT NULL ); INSERT INTO Departments(Code,Name,Budget) VALUES(14,'IT',65000); 我們可以用 .read dot-command 在 shell 跑該 script 建立 Department table: .read create_tables.sql 顯示目前的 tables .tables 顯示 table schema .schema <TABLE_NAME> 顯示 indexes .indexes 在 Table T 的 Column C 建立 index CREATE INDEX <INDEX_NAME> ON T(C); 砍掉 index DROP INDEX <INDEX_NAME>","tags":"Miscellaneous","url":"https://leemeng.tw/sqlite-note.html"},{"title":"Find Word Semantic by Using Word2vec in TensorFlow","text":"The goal of this assignment is to train a Word2Vec skip-gram model over Text8 data using Tensorflow. Word2vec is a kind of vector space model (VSM) in natural language processing (NLP) where the core assumption/intuition is that words that appear in similar 'context' share similar meaning and they should be near in the vector space. So what word2vec trying to do is to find a vector representation (embedding) for each word in our training corpus where words with similar meanings are near in the vector space. Figure 1 : words' representation in 2D vector space Unlike supervised learning, we don't have labels that tell us 'kitten' = 'cat'. So how do we train a model that will learn the relationship between these two words? Recap the assumption mentioned before, words with similar meaning tend to appear in similar context. Because 'kitten' and 'cat' appear in similar context, if we can train a model to predict the context of the target word 'cat' and 'kitten' respectively, model should learn a similar representation for both 'kitten' and 'cat' because they produce similar context. Figure 2 : Training data generated from target word and context As shown above, for each words like 'cat' in raw text, we will treat them as target word and the words surrounding it as context and construct the training instances (x, y) where x is target word and y is one of the word in context. And the definition of 'context' is decided by the parameter 'window size'. First, let's load the text data and build the training data in order to train our model. Libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. % matplotlib inline from __future__ import print_function import collections import os import math import random import zipfile import numpy as np import tensorflow as tf # from matplotlib import pylab # use pyplot instead import matplotlib.pyplot as plt from six.moves import range from six.moves.urllib.request import urlretrieve from sklearn.manifold import TSNE from tqdm import tnrange plt . style . use ( 'ggplot' ) Raw text data Download / load data Download the data from the source website if necessary. will store the zip file in the 'datasets' subdirectory. url = 'http://mattmahoney.net/dc/' def maybe_download ( filename , expected_bytes ): \"\"\"Download a file into 'datasets' sub-directory if not present, and make sure it's the right size. \"\"\" rel_path = 'datasets/ {} ' . format ( filename ) # if file in not found, download it if not os . path . exists ( rel_path ): filename , _ = urlretrieve ( url + filename , rel_path ) statinfo = os . stat ( rel_path ) if statinfo . st_size == expected_bytes : print ( 'Found and verified {} . size: {} ' . format ( rel_path , statinfo . st_size )) else : print ( statinfo . st_size ) raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) return rel_path filename = maybe_download ( 'text8.zip' , 31344016 ) Found and verified datasets/text8.zip. size: 31344016 Turn data into words Read the data into a string. def read_data ( filename ): \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\" with zipfile . ZipFile ( filename ) as f : data = tf . compat . as_str ( f . read ( f . namelist ()[ 0 ])) . split () return data words = read_data ( filename ) print ( 'Data size %d ' % len ( words )) Data size 17005207 Look into text corpus The 'data size' above mean how many words we have in the data. That is, there are about 17 millions words! Let's show some parts of the text data to make some sense of it. Some phrases which include word 'cat' with window size = '2'. cats = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'cat' or word == 'cats' ] print ( ' \\n ' . join ( cats [: 5 ])) del cats the cartoon cat garfield would amount of cats that roam politicians autodidacts cat lovers firearm and activists cat lovers epistemologists force australia cat six two Some phrases which include word 'kitten' with window size = '2'. kitten = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'kitten' ] print ( ' \\n ' . join ( kitten [: 5 ])) del kitten put the kitten nermal in s sex kitten in the of tom kitten one nine as a kitten rudolph grey called a kitten which is Create training data In order to let TensorFlow make use of the text corpus, we have to transform the text corpus into sequence of numbers. The way to achieve this to build a dictionary which map every word to a unique number and use that dictionary to transform the corpus into number-based data. Figure 3 : Build dictionary and turn text into numbers Notice that some rare words may appear very few times in the entire text corpus. We may want to exclude these terms to keep our dictionary in a reasonable size. In order to do this, we will build the dictionary and view these terms as UNK tokens. UNK means unknown word that doesn't exist in the vocabulary set and the default number of a UNK in dictionary is 0 as shown above. Decide dictionary size Depend on the size of the vocabulary, we will construct a dictionary for top vocabulary_size - 1 common words. For example, if the vocabulary_size = 50000 , we will first count the frequencies of every word appeared in the text corpus and put the most common 49,999 terms into our vocabulary and make the rest of words as UNK token. ( thus the 50,000 th term in the vocabulary). vocabulary_size = 50000 Build dictionary and transform text corpus into sequence of numbers def build_dataset ( words ): \"\"\" Build training data for word2vec from a string including sequences of words divided by spaces. Parameters: ----------- words: a string with every word devided by spaces Returns: -------- dictionary: a dict with word as key and a unique number(index) as their value. dictionary[word] = idx reverse_dictionary: a dict with index as key and the corresponding word as value. reverse_dictionary[idx] = word counts: a list contain tuples (word, frequency) sorted descendingly by frequency while use ('UNK', unk_count) as first tuple. data: a list contain indices of the original words in the parameters 'words'. \"\"\" # count term frequencies and choose the most frequent # terms of vocabulary_size count = [[ 'UNK' , - 1 ]] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 )) dictionary = dict () # index term by their frequency. while UKN is indexed as 0, # the term with most frequencies is indexed as 1, the term with 2th frequencies # is indexed as 2, ... for word , _ in count : dictionary [ word ] = len ( dictionary ) # turn the text corpus into a sequence of number where each number is the # index of the original term in 'dictionary' dict and mark those UNK's number # as 0 which indicate that they're unknown words data = list () unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data . append ( index ) # update UNK's count in corpus count [ 0 ][ 1 ] = unk_count # create reverse dict to enable lookup the original word by their index reverse_dictionary = dict ( zip ( dictionary . values (), dictionary . keys ())) return data , count , dictionary , reverse_dictionary data , count , dictionary , reverse_dictionary = build_dataset ( words ) print ( 'Most common words (+UNK) in text corpus: \\n {} ' . format ( count [: 5 ])) Most common words (+UNK) in text corpus: [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)] See transformed text corpus sample_idx = 1000 print ( '\" {} \" \\n\\n was transformed into \\n\\n \" {} \"' \\ . format ( ' ' . join ( words [ sample_idx : sample_idx + 10 ]), ' ' . join ([ str ( i ) for i in data [ sample_idx : sample_idx + 10 ]]))) del words # Hint to reduce memory. \"american individualist anarchism benjamin tucker in one eight two five\" was transformed into \"64 10276 5234 3248 9615 5 4 13 10 16\" Function to generate a training batch for the skip-gram model. As usual, we will use mini-batch GD to update our model's parameters. Other than batch_size, we also have to decide the range of context surrounding target word (skip_window) and how many training instances are we going to create from a single (target, context) pair. Figure 4 : Build mini-batches by different num_skips # global variable to randomize mini-batch data_index = 0 def generate_batch ( batch_size , num_skips , skip_window ): \"\"\" Generate a mini-batch containing (target_word, context) pairs of `batch_size`. Parameters: ----------- batch_size: mini_batch's size, typically 16 <= batch_size <= 512 num_skips: how many times to reuse an input/target word to generate a label. skip_window: how many words to consider left and right. Returns: -------- batch: a list of target words labels: a list of context words corresponding to target words in batch \"\"\" global data_index assert batch_size % num_skips == 0 assert num_skips <= 2 * skip_window batch = np . ndarray ( shape = ( batch_size ), dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ), dtype = np . int32 ) # initialize first (target, context) sequence # = [ skip_window target skip_window ] span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ): buffer . append ( data [ data_index ]) data_index = ( data_index + 1 ) % len ( data ) # for every target word, for i in range ( batch_size // num_skips ): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] # generate #num_skips of training instances for j in range ( num_skips ): # randomly choose a context word that hasn't been chosen yet # exclude target word by default while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] # shift to next (target, context) sequence buffer . append ( data [ data_index ]) # randomize the start location of every mini-batch # by adding one offset data_index = ( data_index + 1 ) % len ( data ) return batch , labels print ( 'data: \" {} \"' . format ( ' ' . join ([ reverse_dictionary [ di ] for di in data [: 8 ]]))) for num_skips , skip_window in [( 2 , 1 ), ( 4 , 2 )]: data_index = 0 batch , labels = generate_batch ( batch_size = 8 , num_skips = num_skips , skip_window = skip_window ) print ( ' \\n (target, context) with num_skips = %d and skip_window = %d :' % ( num_skips , skip_window )) print ( ' {} ' . format ( ' \\n ' . join ( [ str (( reverse_dictionary [ t ], reverse_dictionary [ c ])) \\ for t , c in zip ( batch , labels . reshape ( 8 ))]))) data: \"anarchism originated as a term of abuse first\" (target, context) with num_skips = 2 and skip_window = 1: ('originated', 'anarchism') ('originated', 'as') ('as', 'originated') ('as', 'a') ('a', 'term') ('a', 'as') ('term', 'of') ('term', 'a') (target, context) with num_skips = 4 and skip_window = 2: ('as', 'term') ('as', 'anarchism') ('as', 'a') ('as', 'originated') ('a', 'as') ('a', 'originated') ('a', 'term') ('a', 'of') Word2Vec skip-gram model. Figure 5 : Word2vec model Computation graph batch_size = 128 embedding_size = 128 # Dimension of the embedding vector. skip_window = 1 # How many words to consider left and right. num_skips = 2 # How many times to reuse an input to generate a label. # We pick a random validation set to sample nearest neighbors. here we limit the # validation samples to the words that have a low numeric ID, which by # construction are also the most frequent. valid_size = 16 # Random set of words to evaluate similarity on. valid_window = 100 # Only pick dev samples in the head of the distribution. valid_examples = np . array ( random . sample ( range ( valid_window ), valid_size )) num_sampled = 64 # Number of negative examples to sample. graph = tf . Graph () with graph . as_default (), tf . device ( '/cpu:0' ): # Input data. train_dataset = tf . placeholder ( tf . int32 , shape = [ batch_size ]) train_labels = tf . placeholder ( tf . int32 , shape = [ batch_size , 1 ]) valid_dataset = tf . constant ( valid_examples , dtype = tf . int32 ) # Variables. embeddings = tf . Variable ( tf . random_uniform ([ vocabulary_size , embedding_size ], - 1.0 , 1.0 )) softmax_weights = tf . Variable ( tf . truncated_normal ( [ vocabulary_size , embedding_size ], stddev = 1.0 / math . sqrt ( embedding_size ))) softmax_biases = tf . Variable ( tf . zeros ([ vocabulary_size ])) # Model. # Look up embeddings for inputs. embed = tf . nn . embedding_lookup ( embeddings , train_dataset ) # Compute the softmax loss, using a sample of the negative labels each time. # this is how we speed up training phase loss = tf . reduce_mean ( tf . nn . sampled_softmax_loss ( weights = softmax_weights , biases = softmax_biases , inputs = embed , labels = train_labels , num_sampled = num_sampled , num_classes = vocabulary_size )) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # This is because the embeddings are defined as a variable quantity and the # optimizer's `minimize` method will by default modify all variable quantities # that contribute to the tensor it is passed. # See docs on `tf.train.Optimizer.minimize()` for more details. optimizer = tf . train . AdagradOptimizer ( 1.0 ) . minimize ( loss ) # Compute the similarity between minibatch examples and all embeddings. # We use the cosine distance: norm = tf . sqrt ( tf . reduce_sum ( tf . square ( embeddings ), 1 , keep_dims = True )) normalized_embeddings = embeddings / norm valid_embeddings = tf . nn . embedding_lookup ( normalized_embeddings , valid_dataset ) similarity = tf . matmul ( valid_embeddings , tf . transpose ( normalized_embeddings )) Train the model num_steps = 100001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( 'Initialized' ) average_loss = 0 for step in tnrange ( num_steps ): batch_data , batch_labels = generate_batch ( batch_size , num_skips , skip_window ) feed_dict = { train_dataset : batch_data , train_labels : batch_labels } _ , l = session . run ([ optimizer , loss ], feed_dict = feed_dict ) average_loss += l if step % 2000 == 0 : if step > 0 : average_loss = average_loss / 2000 # The average loss is an estimate of the loss over the last 2000 batches. print ( 'Average loss at step %d : %f ' % ( step , average_loss )) average_loss = 0 # note that this is expensive (~20% slowdown if computed every 500 steps) if step % 10000 == 0 : sim = similarity . eval () for i in range ( valid_size ): valid_word = reverse_dictionary [ valid_examples [ i ]] top_k = 8 # number of nearest neighbors nearest = ( - sim [ i , :]) . argsort ()[ 1 : top_k + 1 ] log = 'Nearest to %s :' % valid_word for k in range ( top_k ): close_word = reverse_dictionary [ nearest [ k ]] log = ' %s %s ,' % ( log , close_word ) print ( log ) final_embeddings = normalized_embeddings . eval () Initialized var element = $('#ba961300-e349-4f59-aef1-a77ffd794967'); {\"model_id\": \"479bac87e8864793a2657306231c3b2a\", \"version_major\": 2, \"version_minor\": 0} Average loss at step 0: 7.942154 Nearest to up: refit, airmen, unexplored, scharnhorst, histones, envelopes, wanna, wick, Nearest to many: herbivorous, kazimierz, surgeries, juliette, merovingian, christadelphians, experimentation, strauss, Nearest to people: chicken, zulu, glaucus, temporarily, groundbreaking, mapuche, varnish, vinod, Nearest to some: platelets, mauritania, anzus, soaemias, plankton, orcs, cegep, danzig, Nearest to was: blotter, continuously, gulls, lineages, turbines, cardano, honky, gcb, Nearest to and: haller, potion, rickshaw, fares, soldier, mariam, sponsor, irs, Nearest to in: wrench, atwood, boys, fermat, uhf, midrash, hallucinogens, deflate, Nearest to be: infertility, olaf, faramir, dxf, latino, clem, aia, cation, Nearest to all: stripe, abbreviation, nationalised, maoi, hermann, three, lara, jay, Nearest to than: antiderivatives, deduce, brainiac, wry, propel, requested, selangor, transposed, Nearest to system: rmi, devine, elite, deism, transgressions, bows, primer, undoubtedly, Nearest to were: ange, oh, widest, fidel, kristallnacht, predicates, coprocessor, surnames, Nearest to see: astounding, equilateral, concubines, syllables, lackluster, transoxiana, kasparov, smooth, Nearest to that: figurative, cluster, wendell, horch, cards, prinz, ellesmere, maximilian, Nearest to s: endosperm, saberhagen, pipelined, recantation, calcium, infrastructural, manchuria, spears, Nearest to will: suppressive, rotations, predefined, dessau, confederation, hovered, radiohead, apicomplexa, Average loss at step 2000: 4.364343 Average loss at step 4000: 3.857699 Average loss at step 6000: 3.785119 Average loss at step 8000: 3.686496 Average loss at step 10000: 3.616513 Nearest to up: scharnhorst, refit, arabs, concubines, zeeland, airmen, calibers, diem, Nearest to many: some, kazimierz, herbivorous, timber, poorly, revitalize, psychoanalyst, studies, Nearest to people: ach, la, semester, several, glaucus, gediminas, mechanism, yupik, Nearest to some: many, overbearing, firmer, oracles, dissonant, limes, soaemias, cpu, Nearest to was: is, has, had, were, by, been, be, are, Nearest to and: or, s, but, scriptores, who, of, in, alans, Nearest to in: on, at, of, from, with, between, by, during, Nearest to be: have, was, is, do, receiving, latino, subclass, chisel, Nearest to all: marketed, maoi, abbreviation, stripe, protectors, nationalised, lara, chadic, Nearest to than: haer, deduce, caucus, omniglot, casings, propel, selangor, softer, Nearest to system: rmi, devine, elite, primer, hcl, try, deism, bows, Nearest to were: are, was, tiu, have, adware, ragged, popularizer, macrobiotic, Nearest to see: william, concubines, astounding, transoxiana, but, bullough, pigeon, artistic, Nearest to that: which, ellesmere, she, it, pectoral, also, who, breaches, Nearest to s: and, vu, his, the, was, carlist, tanoana, chulainn, Nearest to will: may, would, jews, barbarism, predefined, could, confederation, receiving, Average loss at step 12000: 3.606183 Average loss at step 14000: 3.572916 Average loss at step 16000: 3.410224 Average loss at step 18000: 3.453891 Average loss at step 20000: 3.539739 Nearest to up: unvoiced, scharnhorst, zeeland, him, refit, arabs, cesium, wick, Nearest to many: some, these, several, other, kazimierz, studies, all, psychoanalyst, Nearest to people: languages, gediminas, countries, glaucus, eplf, several, those, temporarily, Nearest to some: many, these, their, olav, overbearing, his, its, most, Nearest to was: is, has, were, had, became, be, chisel, been, Nearest to and: or, but, at, which, from, in, however, for, Nearest to in: at, on, during, from, for, by, with, and, Nearest to be: have, been, was, were, campaigner, by, receiving, combination, Nearest to all: these, marketed, maoi, marshals, many, lara, escalation, agate, Nearest to than: or, deduce, much, trough, haer, brainiac, tug, spalding, Nearest to system: hcl, rmi, elite, devine, oleg, bows, henotheism, primer, Nearest to were: are, was, had, tiu, have, be, by, is, Nearest to see: atzma, diaconate, transoxiana, syllables, showcasing, lackluster, bullough, rabbinical, Nearest to that: which, but, breaches, because, it, ietf, this, ellesmere, Nearest to s: pu, purr, tanoana, forum, traveller, predicated, plasmodium, integrator, Nearest to will: would, may, can, could, should, to, cannot, geographic, Average loss at step 22000: 3.502537 Average loss at step 24000: 3.485267 Average loss at step 26000: 3.483680 Average loss at step 28000: 3.478223 Average loss at step 30000: 3.503296 Nearest to up: him, begin, scharnhorst, them, unvoiced, refit, zeeland, calibers, Nearest to many: some, several, these, their, its, all, various, the, Nearest to people: countries, those, languages, dhea, temporarily, ach, vlsi, eplf, Nearest to some: many, these, several, the, their, olav, limes, this, Nearest to was: is, were, had, has, became, been, when, by, Nearest to and: or, who, in, but, from, of, rehearsal, tsunamis, Nearest to in: during, at, from, of, on, since, between, and, Nearest to be: have, is, been, are, were, mustelids, aldiss, impurity, Nearest to all: these, lara, some, many, inhabiting, several, maoi, marketed, Nearest to than: much, or, no, deduce, trough, spalding, tug, haer, Nearest to system: hcl, group, devine, francisco, oleg, rmi, raccoons, master, Nearest to were: are, was, have, had, is, tiu, dowager, been, Nearest to see: diaconate, include, atzma, showcasing, slider, transoxiana, rent, syllables, Nearest to that: which, this, however, but, what, where, kilometre, if, Nearest to s: his, forum, isbn, her, ancestral, insulated, sucking, and, Nearest to will: can, would, could, may, should, must, cannot, to, Average loss at step 32000: 3.500015 Average loss at step 34000: 3.495233 Average loss at step 36000: 3.458392 Average loss at step 38000: 3.301554 Average loss at step 40000: 3.431673 Nearest to up: out, him, them, unvoiced, back, arabs, down, begin, Nearest to many: some, several, these, various, those, their, certain, such, Nearest to people: languages, countries, those, quintessential, baum, serif, dhea, areas, Nearest to some: many, these, any, several, olav, their, those, most, Nearest to was: is, had, were, became, has, severing, being, been, Nearest to and: or, but, while, cynical, subsystem, where, mus, in, Nearest to in: from, of, during, on, for, and, bubbled, at, Nearest to be: been, have, were, are, is, continue, was, shotguns, Nearest to all: lara, these, both, two, many, travellers, lobster, citizen, Nearest to than: or, much, no, spalding, deduce, even, brainiac, significance, Nearest to system: systems, code, crusading, group, bows, smuts, hcl, completions, Nearest to were: are, have, was, tiu, had, be, been, being, Nearest to see: include, pigeon, syllables, algardi, diaconate, theroux, slider, lada, Nearest to that: which, however, what, this, because, where, it, oro, Nearest to s: his, forum, levine, fl, fender, isbn, tyr, empowerment, Nearest to will: would, can, could, may, should, must, might, cannot, Average loss at step 42000: 3.433565 Average loss at step 44000: 3.451384 Average loss at step 46000: 3.450792 Average loss at step 48000: 3.354962 Average loss at step 50000: 3.388338 Nearest to up: out, them, down, him, disproven, off, back, unvoiced, Nearest to many: some, several, these, various, those, most, such, other, Nearest to people: countries, languages, roots, vlsi, men, gediminas, those, scholars, Nearest to some: many, these, several, olav, most, their, any, the, Nearest to was: is, has, were, became, had, been, being, be, Nearest to and: but, or, in, while, of, from, whose, eurasia, Nearest to in: from, during, of, since, on, and, by, within, Nearest to be: have, been, were, was, is, become, being, are, Nearest to all: both, these, many, lara, writs, every, citizen, diffuses, Nearest to than: or, much, even, bam, deduce, while, spalding, significance, Nearest to system: systems, crusading, bows, code, hcl, renderings, smuts, group, Nearest to were: are, was, have, tiu, be, had, those, been, Nearest to see: include, pigeon, diaconate, algardi, theroux, bloomington, showcasing, pwnage, Nearest to that: which, however, what, often, uv, this, where, honoring, Nearest to s: whose, isbn, romanian, his, cleaner, adheres, fender, predicated, Nearest to will: would, could, can, may, must, should, shall, might, Average loss at step 52000: 3.437301 Average loss at step 54000: 3.427909 Average loss at step 56000: 3.438053 Average loss at step 58000: 3.394049 Average loss at step 60000: 3.394188 Nearest to up: out, them, down, him, off, back, disproven, replace, Nearest to many: some, several, these, various, all, most, such, other, Nearest to people: those, countries, men, scholars, roots, gediminas, others, vlsi, Nearest to some: many, several, these, olav, most, any, each, this, Nearest to was: is, had, became, has, were, been, be, although, Nearest to and: or, but, including, than, owning, shutting, with, sagan, Nearest to in: during, within, including, of, at, since, throughout, anew, Nearest to be: been, have, are, is, was, were, refer, become, Nearest to all: many, these, both, rediscovery, some, timers, menial, various, Nearest to than: or, much, but, spalding, and, deduce, no, far, Nearest to system: systems, code, crusading, group, hcl, software, depth, smuts, Nearest to were: are, was, had, have, tiu, those, haired, be, Nearest to see: include, diaconate, but, can, bloomington, pwnage, according, algardi, Nearest to that: which, however, this, what, where, it, often, ellesmere, Nearest to s: whose, infrastructural, isbn, tanoana, arbenz, flybys, vladimir, geographic, Nearest to will: would, can, could, may, must, should, might, cannot, Average loss at step 62000: 3.243753 Average loss at step 64000: 3.259946 Average loss at step 66000: 3.404854 Average loss at step 68000: 3.393782 Average loss at step 70000: 3.359670 Nearest to up: out, them, down, off, back, him, disproven, arabs, Nearest to many: some, several, these, various, all, most, olav, numerous, Nearest to people: men, those, countries, scholars, children, peoples, languages, historians, Nearest to some: many, several, these, olav, various, all, those, any, Nearest to was: is, were, has, had, became, severing, been, when, Nearest to and: or, while, but, which, like, stockade, than, unclassified, Nearest to in: within, during, on, throughout, through, from, for, at, Nearest to be: been, were, is, have, being, become, are, fully, Nearest to all: many, some, both, every, any, montag, various, these, Nearest to than: or, no, spalding, much, while, but, and, resign, Nearest to system: systems, crusading, bred, depth, group, hcl, smuts, code, Nearest to were: are, was, have, tiu, had, be, those, been, Nearest to see: include, diaconate, orators, list, algardi, according, but, fatality, Nearest to that: which, however, what, this, but, where, because, also, Nearest to s: pu, infrastructural, isbn, whose, chulainn, codifying, sequestered, purr, Nearest to will: would, could, may, can, should, must, might, shall, Average loss at step 72000: 3.372420 Average loss at step 74000: 3.350309 Average loss at step 76000: 3.322349 Average loss at step 78000: 3.355263 Average loss at step 80000: 3.377625 Nearest to up: out, off, down, them, him, disproven, back, arabs, Nearest to many: several, some, various, these, most, numerous, those, both, Nearest to people: men, children, countries, those, words, members, peoples, scholars, Nearest to some: many, several, various, these, olav, most, both, any, Nearest to was: is, were, has, had, became, been, being, when, Nearest to and: or, but, including, than, while, globalsecurity, rehearsal, lances, Nearest to in: during, within, on, until, at, since, after, through, Nearest to be: been, have, become, refer, being, were, is, proceed, Nearest to all: both, every, various, any, many, some, these, aguilera, Nearest to than: or, much, but, while, and, spalding, resign, even, Nearest to system: systems, crusading, smuts, software, code, familias, nonpartisan, game, Nearest to were: are, was, had, have, those, tiu, being, be, Nearest to see: include, according, diaconate, but, orators, includes, list, eth, Nearest to that: which, however, where, ellesmere, markham, what, this, thus, Nearest to s: pu, isbn, whose, chulainn, mondeo, infrastructural, electorates, adheres, Nearest to will: would, could, can, may, should, must, might, cannot, Average loss at step 82000: 3.407648 Average loss at step 84000: 3.411925 Average loss at step 86000: 3.389199 Average loss at step 88000: 3.359499 Average loss at step 90000: 3.365950 Nearest to up: out, off, down, them, back, him, disproven, arabs, Nearest to many: some, several, various, these, all, numerous, most, those, Nearest to people: children, men, women, persons, religions, god, screenwriters, countries, Nearest to some: many, several, these, all, any, various, those, most, Nearest to was: is, became, had, were, has, been, being, be, Nearest to and: or, but, while, emory, consistory, including, however, who, Nearest to in: within, during, around, of, throughout, under, between, near, Nearest to be: have, been, become, is, produce, was, refer, simulate, Nearest to all: many, both, some, every, various, several, each, any, Nearest to than: or, much, spalding, no, even, resign, showered, considerably, Nearest to system: systems, crusading, process, program, group, mee, unit, brockovich, Nearest to were: are, was, had, have, while, tiu, those, been, Nearest to see: list, diaconate, include, includes, references, refer, external, kliper, Nearest to that: which, however, what, but, markham, autos, where, ellesmere, Nearest to s: whose, isbn, infrastructural, his, pu, tyr, vladimir, mondeo, Nearest to will: would, could, can, may, must, should, might, cannot, Average loss at step 92000: 3.398944 Average loss at step 94000: 3.258866 Average loss at step 96000: 3.356221 Average loss at step 98000: 3.242518 Average loss at step 100000: 3.359612 Nearest to up: out, off, down, him, back, them, arabs, begin, Nearest to many: several, some, these, various, numerous, those, all, few, Nearest to people: persons, men, children, someone, women, countries, players, scholars, Nearest to some: many, several, these, any, various, olav, all, certain, Nearest to was: is, became, had, were, has, although, when, been, Nearest to and: or, but, like, while, including, when, than, who, Nearest to in: within, during, throughout, of, at, from, on, with, Nearest to be: been, have, is, refer, become, are, were, produce, Nearest to all: many, various, any, every, these, some, both, several, Nearest to than: or, spalding, much, and, even, showered, while, omniglot, Nearest to system: systems, crusading, process, program, software, familias, hcl, stewardship, Nearest to were: are, was, have, tiu, these, those, had, be, Nearest to see: diaconate, includes, references, list, include, links, refer, external, Nearest to that: which, however, what, this, lodges, ellesmere, who, dicke, Nearest to s: whose, isbn, his, tyr, ancestral, infrastructural, pu, starring, Nearest to will: would, could, must, can, should, may, might, cannot, Transform embedding into 2D using t-SNE final_embeddings . shape (50000, 128) %% time num_points = 400 tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact') two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :]) Visualize result def plot ( embeddings , labels ): assert embeddings . shape [ 0 ] >= len ( labels ), 'More labels than embeddings' plt . figure ( figsize = ( 15 , 15 )) # in inches for i , label in enumerate ( labels ): x , y = embeddings [ i , :] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ), xytext = ( 5 , 2 ), textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) plt . show () words = [ reverse_dictionary [ i ] for i in range ( 1 , num_points + 1 )] plot ( two_d_embeddings , words ) Todo CBOW An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset. References Original jupyter notebook from the Udacity MOOC course: Deep learning by Google . TensorFlow word2vec tutorial http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/","tags":"Deep Learning","url":"https://leemeng.tw/find-word-semantic-by-using-word2vec-in-tensorflow.html"},{"title":"Simple Convolutional Neural Network using TensorFlow","text":"The goal here is to practice building convolutional neural networks to classify notMNIST characters using TensorFlow. As image size become bigger and bigger, it become unpractical to train fully-connected NN because there will be just too many parameters and thus the model will overfit very soon. And CNN solve this problem by weight sharing. We will start by building a CNN with two convolutional layers connected by a fully connected layer and then try also pooling layer and other thing to improve the model performance. Original jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import numpy as np import seaborn as sns import tensorflow as tf import matplotlib.pyplot as plt from six.moves import cPickle as pickle from six.moves import range from tqdm import tnrange import time # beautify graph plt . style . use ( 'ggplot' ) Load notMNIST dataset pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data Reformat into a TensorFlow-friendly shape: convolutions need the image data formatted as a cube of shape (width, height, #channels) labels as float 1-hot encodings. image_size = 28 num_labels = 10 num_channels = 1 # grayscale def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size , image_size , num_channels )) . astype ( np . float32 ) labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28, 1) (200000, 10) Validation set (10000, 28, 28, 1) (10000, 10) Test set (10000, 28, 28, 1) (10000, 10) def accuracy ( predictions , labels ): return ( 100.0 * np . sum ( np . argmax ( predictions , 1 ) == np . argmax ( labels , 1 )) / predictions . shape [ 0 ]) Helper for training visualization Let's define a function that make better visualization of our training progress. The function will draw mini-batch loss and training/validation accuracy dynamically. # dynamic showing loss and accuracy when training % matplotlib notebook def plt_dynamic ( x , y , ax , xlim = None , ylim = None , xlabel = 'X' , ylabel = 'Y' , colors = [ 'b' ], sleep_sec = 0 , figsize = None ): import time if figsize : fig . set_size_inches ( figsize [ 0 ], figsize [ 1 ], forward = True ) ax . set_xlabel ( xlabel ); ax . set_ylabel ( ylabel ) for color in colors : ax . plot ( x , y , color ) if xlim : ax . set_xlim ( xlim [ 0 ], xlim [ 1 ]) if ylim : ax . set_ylim ( ylim [ 0 ], ylim [ 1 ]) fig . canvas . draw () time . sleep ( sleep_sec ) NN with 2 convolutional layers Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes. Computation graph Although this assignment already provide good Tensorflow code to build convoluational networks, I found that I can't imagine what NN I was going to build by reading the code. So I tried to draw what we're going to build and explain some parameters used in code by comments. The convolutional network we're going to build: Figure 1 : CNN with 1 fully connected layer Something worth mentioning: We set both convoluational layers' output depth = 16. We use filters/patches of shape (5 * 5) to find features in local area of a image. The new width and height of the convoluational layer will be half of that in the previous layer because we use stride = 2 and SAME padding to 'slide' our patches. Thus 28 -> 14 -> 7. Notice that ReLU layers applied after convoluational layers are omitted for simplicity. The activations in C2 fully connected to the FC layer. For each neuron on the FC layer, there are $7 * 7 * 16 = 784$ weights ($785$ for bias ), so there are $785 * 64 = 50240$ parameters in the FC layer. For more details about CNN, I recommend CS231n . batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # When defining weights for a convoluational layer, use the notation # [filter_size, filter_size, input_depth, output_depth] layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) # in this CNN, two convoluational layers happen to have the same depth. # if we want, we can adjust them to be different like depth1, depth2 layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # because we use stride = 2 and SAME padding, our new shape of first feature map C1 # will be (image_size // 2, image_size //2). and because we use 2 convolutional layers, # the shape of second feature map C2 will be (image_size // 2 // 2, image_size // 2 // 2) # = (image_size // 4, image_size // 4). and because we have depth == 16, # the total neurons on C2 will be image_size // 4 * image_size // 4 * depth layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # this is where we set stride = 2 for both width and height and also SAME padding # the third parameters in tf.nn.conv2d is to set stride for every dimension # specified in the first parameter data's shape conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) conv = tf . nn . conv2d ( hidden , layer2_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) shape = hidden . get_shape () . as_list () # turn the C2 3D cube back to 2D matrix by shape (#data_points, #neurons) reshape = tf . reshape ( hidden , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model and visualize the result The best thing of the visualization is that it's rendered in a real-time manner. num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#f8e4fe66-51c6-4b2f-8598-5da6af724d2e'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#be3a1944-4def-4c52-8056-97ba28d82327'); {\"model_id\": \"255b2330df764c069bf0fa8b72e282b3\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 4.085.batch acc: 0.0%, Valid acc: 10.0%. Minibatch loss at step 100: 0.861.batch acc: 62.5%, Valid acc: 74.5%. Minibatch loss at step 200: 0.425.batch acc: 87.5%, Valid acc: 79.1%. Minibatch loss at step 300: 0.934.batch acc: 62.5%, Valid acc: 79.8%. Minibatch loss at step 400: 0.861.batch acc: 68.8%, Valid acc: 79.6%. Minibatch loss at step 500: 0.204.batch acc: 87.5%, Valid acc: 80.6%. Minibatch loss at step 600: 0.741.batch acc: 75.0%, Valid acc: 82.0%. Minibatch loss at step 700: 0.591.batch acc: 87.5%, Valid acc: 82.5%. Minibatch loss at step 800: 1.171.batch acc: 68.8%, Valid acc: 81.8%. Minibatch loss at step 900: 0.171.batch acc: 100.0%, Valid acc: 83.2%. Minibatch loss at step 1000: 0.487.batch acc: 93.8%, Valid acc: 82.6%. Test accuracy: 89.4% As shown above, mini-batch loss dropped rapidly at first 200 iterations, training and validation accuracy also improve quickly (both achieved about 80%). After 200 iterations, validation performance become stable but still improved about 5%. The test accuracy is about 89%. Problem 1 - Use pooling layers to reduce dimensionality The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation ( nn.max_pool() ) of stride 2 and kernel size 2. The reason why we're going to use pooling layer is that we can reduce spatial size thus parameters to reduce the chance of overfitting. And the advantage of pooling layer is that it require no new parameters. Let's see how much performance we can gain by using max pooling. Figure 2 : Max Pooling Build model with pooling layers Actually, what we will do is just to add pooling layers right after ReLU layers and let the convoluational layer use stride 1. In intuition, we let the convoluational layers look more 'closely' into the images, but also try to limit the number of activation and extract the important parts by pooling layers. batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#614d2696-7ccf-49cf-9b3c-7b7a0f3b271e'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#77050ab6-b4a4-4e2f-8873-0a33ed1ec188'); {\"model_id\": \"ae81313ad6ad4b428ce63444b7f0ae73\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.485.batch acc: 18.8%, Valid acc: 13.0%. Minibatch loss at step 100: 0.886.batch acc: 75.0%, Valid acc: 73.2%. Minibatch loss at step 200: 0.417.batch acc: 87.5%, Valid acc: 79.7%. Minibatch loss at step 300: 0.759.batch acc: 81.2%, Valid acc: 80.9%. Minibatch loss at step 400: 0.835.batch acc: 75.0%, Valid acc: 80.5%. Minibatch loss at step 500: 0.366.batch acc: 87.5%, Valid acc: 82.0%. Minibatch loss at step 600: 0.630.batch acc: 75.0%, Valid acc: 83.3%. Minibatch loss at step 700: 0.605.batch acc: 81.2%, Valid acc: 83.2%. Minibatch loss at step 800: 1.031.batch acc: 68.8%, Valid acc: 83.5%. Minibatch loss at step 900: 0.240.batch acc: 93.8%, Valid acc: 84.6%. Minibatch loss at step 1000: 0.500.batch acc: 87.5%, Valid acc: 83.9%. Test accuracy: 90.7% There is some performance gain in my current iteration between model w/o pooling layer ( about 1.3% ). And it seems that after several iterations, the validation set performance is slightly better with the pooling layers. But it took about 1 minute and 30 seconds to train CNN with pooling layers, and only 30 seconds to train the one without pooling layers. I think it depends on whether you're willing to gain a little more performance by using more time to train the model. Problem 2 Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay. I will just try to add a convoluational layer here and train a little longer to see how the performance changed. Computation graph batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # CONV -> FC layer4_weights = tf . Variable ( tf . truncated_normal ( [( image_size // 8 + 1 ) * ( image_size // 8 + 1 ) * depth , num_hidden ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) # FC -> output layer5_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer5_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # [CONV -> RELU -> POOL] * 3 conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer3_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer3_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer4_weights ) + layer4_biases ) return tf . matmul ( hidden , layer5_weights ) + layer5_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 10001 step_interval = 500 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#703f4296-8c80-477d-b62f-b4d871e2bb87'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#c7821d25-a350-48f9-8d1b-1509e2f1be13'); {\"model_id\": \"b240065c4ffa4ca29432614b189f2e04\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 3.563.batch acc: 6.2%, Valid acc: 10.0%. Minibatch loss at step 1000: 0.502.batch acc: 87.5%, Valid acc: 84.9%. Minibatch loss at step 2000: 0.414.batch acc: 87.5%, Valid acc: 86.4%. Minibatch loss at step 3000: 0.164.batch acc: 93.8%, Valid acc: 87.0%. Minibatch loss at step 4000: 0.545.batch acc: 75.0%, Valid acc: 88.1%. Minibatch loss at step 5000: 0.898.batch acc: 75.0%, Valid acc: 88.4%. Minibatch loss at step 6000: 0.493.batch acc: 81.2%, Valid acc: 86.4%. Minibatch loss at step 7000: 0.613.batch acc: 81.2%, Valid acc: 89.3%. Minibatch loss at step 8000: 0.089.batch acc: 100.0%, Valid acc: 89.2%. Minibatch loss at step 9000: 0.280.batch acc: 93.8%, Valid acc: 89.6%. Minibatch loss at step 10000: 0.437.batch acc: 87.5%, Valid acc: 89.4%. Test accuracy: 94.6% By adding a new convoluational layer and train 10x steps, our model's performance can even boost to almost 95%! (though it take about 5 minutes to train on my pc) and I think there are still many things we can tune to make the model better, but I will stop here to move on to sequence model!","tags":"Deep Learning","url":"https://leemeng.tw/simple-convolutional-neural-network-using-tensorflow.html"},{"title":"Regularization for Multi-layer Neural Networks in Tensorflow","text":"The goal of this assignment is to explore regularization techniques. The original notebook can be found here Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function from tqdm import tnrange import numpy as np import tensorflow as tf from six.moves import cPickle as pickle Load NotMNIST dataset First reload the data we generated in 1_notmnist.ipynb . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) X_train = save [ 'train_dataset' ] Y_train = save [ 'train_labels' ] X_valid = save [ 'valid_dataset' ] Y_valid = save [ 'valid_labels' ] X_test = save [ 'test_dataset' ] Y_test = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , Y_test . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat dataset Reformat into a shape that's more adapted to the models we're going to train: data as a flat matrix, labels as float 1-hot encodings. As I did in previous notebook, this reformat operation will be different from the operation suggested by the original notebook . image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T return dataset , labels X_train , Y_train = reformat ( X_train , Y_train ) X_valid , Y_valid = reformat ( X_valid , Y_valid ) X_test , Y_test = reformat ( X_test , Y_test ) print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , X_test . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (784, 10000) Using Accuracy as Default Metric Because as we explored before, there exist no unbalanced problem in the dataset, so accuracy alone will be sufficient for evaluating performance of our model on the classification task. def accuracy ( predictions , labels ): return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) 3-layer NN as base model In order to test the effect with/without regularization, we will use a little more complex neural network with 2 hidden layers as our base model. And we will be using ReLU as our activation function. Hyper parameters # hyper parameters learning_rate = 1e-2 lamba = 1e-3 keep_prob = 0.5 batch_size = 128 num_steps = 501 n0 = image_size * image_size # input size n1 = 1024 # first hidden layer n2 = 512 # second hidden layer n3 = 256 # third hidden layer n4 = num_labels # output size Build model # build a model which let us able to choose different optimzation mechnism def model ( lamba = 0 , learning_rate = learning_rate , keep_prob = 1 , learning_decay = False , batch_size = batch_size , num_steps = num_steps , n1 = n1 , n2 = n2 , n3 = n3 ): print ( \"\"\" Train 3-layer NN with following settings: Regularization lambda: {} Learning rate: {} learning_decay: {} keep_prob: {} Batch_size: {} Number of steps: {} n1, n2, n3: {}, {}, {}\"\"\" . format ( lamba , learning_rate , learning_decay , keep_prob , batch_size , num_steps , n1 , n2 , n3 )) # construct computation graph graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training X = tf . placeholder ( tf . float32 , shape = ( n0 , batch_size )) Y = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) global_step = tf . Variable ( 0 ) # use all valid/test set tf_X_valid = tf . constant ( X_valid ) tf_X_test = tf . constant ( X_test ) # initialize weights, biases # notice that we have two hidden # layers so we now have W1, b1, W2, b2, W3, b3 W1 = tf . Variable ( tf . truncated_normal ([ n1 , n0 ], stddev = np . sqrt ( 2.0 / n0 ))) W2 = tf . Variable ( tf . truncated_normal ([ n2 , n1 ], stddev = np . sqrt ( 2.0 / n1 ))) W3 = tf . Variable ( tf . truncated_normal ([ n3 , n2 ], stddev = np . sqrt ( 2.0 / n2 ))) W4 = tf . Variable ( tf . truncated_normal ([ n4 , n3 ], stddev = np . sqrt ( 2.0 / n3 ))) b1 = tf . Variable ( tf . zeros ([ n1 , 1 ])) b2 = tf . Variable ( tf . zeros ([ n2 , 1 ])) b3 = tf . Variable ( tf . zeros ([ n3 , 1 ])) b4 = tf . Variable ( tf . zeros ([ n4 , 1 ])) # training computation Z1 = tf . matmul ( W1 , X ) + b1 A1 = tf . nn . relu ( Z1 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z1 ), keep_prob ) Z2 = tf . matmul ( W2 , A1 ) + b2 A2 = tf . nn . relu ( Z2 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z2 ), keep_prob ) Z3 = tf . matmul ( W3 , A2 ) + b3 A3 = tf . nn . relu ( Z3 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z3 ), keep_prob ) Z4 = tf . matmul ( W4 , A3 ) + b4 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( Y ), logits = tf . transpose ( Z4 ))) if lamba : loss += lamba * \\ ( tf . nn . l2_loss ( W1 ) + tf . nn . l2_loss ( W2 ) + tf . nn . l2_loss ( W3 ) + tf . nn . l2_loss ( W4 )) # optimizer if learning_decay : learning_rate = tf . train . exponential_decay ( 0.5 , global_step , 5000 , 0.80 , staircase = True ) optimizer = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss , global_step = global_step ) else : optimizer = ( tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss )) # valid / test prediction Y_pred = tf . nn . softmax ( Z4 , dim = 0 ) Y_vaild_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_valid ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) Y_test_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_test ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) # define training with tf . Session ( graph = graph ) as sess : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # generate randomized mini-batches from training data offset = ( step * batch_size ) % ( Y_train . shape [ 1 ] - batch_size ) batch_X = X_train [:, offset :( offset + batch_size )] batch_Y = Y_train [:, offset :( offset + batch_size )] # train model _ , l , batch_Y_pred = sess . run ( [ optimizer , loss , Y_pred ], feed_dict = { X : batch_X , Y : batch_Y }) if ( step % 200 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( batch_Y_pred , batch_Y ), accuracy ( Y_vaild_pred . eval (), Y_valid ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( Y_test_pred . eval (), Y_test ))) Train model without regularization model ( learning_rate = 0.5 , num_steps = 1601 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.5 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 1601 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#ee539fed-5f8f-4802-9400-7c29749f4c07'); {\"model_id\": \"95a1e075513f4d02a8579c4fcd5b8509\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.374. batch acc: 14.1%, Valid acc: 28.4%. Minibatch loss at step 200: 0.600. batch acc: 82.0%, Valid acc: 84.9%. Minibatch loss at step 400: 0.429. batch acc: 89.8%, Valid acc: 85.8%. Minibatch loss at step 600: 0.372. batch acc: 87.5%, Valid acc: 85.7%. Minibatch loss at step 800: 0.454. batch acc: 89.1%, Valid acc: 87.7%. Minibatch loss at step 1000: 0.374. batch acc: 87.5%, Valid acc: 88.1%. Minibatch loss at step 1200: 0.251. batch acc: 91.4%, Valid acc: 88.8%. Minibatch loss at step 1400: 0.397. batch acc: 89.8%, Valid acc: 89.0%. Minibatch loss at step 1600: 0.470. batch acc: 82.0%, Valid acc: 88.9%. Test acc: 94.2% L2 regularization Introduce and tune L2 regularization for the models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t) . The right amount of regularization should improve your validation / test accuracy. # for lamda in [1 / 10 ** i for i in list(np.arange(1, 4))]: # model(lamba=lamda) model ( lamba = 0.1 , learning_rate = 0.01 ) Train 3-layer NN with following settings: Regularization lambda: 0.1 Optimizer: sgd Learning rate: 0.01 Batch_size: 128 Number of steps: 501 n1, n2: 512, 256 Initialized var element = $('#7b1de211-c65b-4064-b4f9-db0ddf213dfa'); {\"model_id\": \"6b6986da296646b9aac2266326edcf21\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 22969.777. batch acc: 9.4%, Valid acc: 19.3%. Minibatch loss at step 200: 13876.185. batch acc: 74.2%, Valid acc: 75.2%. Minibatch loss at step 400: 9266.566. batch acc: 78.1%, Valid acc: 74.3%. Test acc: 81.4% Case of overfitting Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens? model ( num_steps = 10 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#4dc2802f-92e4-492a-a90d-4cf35a856408'); {\"model_id\": \"312b63e26f364cec95540b452b4ec95c\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.442. batch acc: 8.6%, Valid acc: 11.4%. Test acc: 20.7% Dropout Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training. What happens to our extreme overfitting case? model ( num_steps = 10 , keep_prob = 0.5 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 0.5 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#cef5f9e5-d650-4003-b0f0-657243421264'); {\"model_id\": \"f5a5be47b2a14f488d9d302df5d5988d\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.784. batch acc: 7.0%, Valid acc: 10.0%. Test acc: 17.3% Boost performance by using Multi-layer NN Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1% . One avenue you can explore is to add multiple layers. Another one is to use learning rate decay: global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step, ...) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) model ( learning_decay = True , num_steps = 1501 , lamba = 0 , keep_prob = 1 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: True keep_prob: 1 Batch_size: 128 Number of steps: 1501 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#6ba46be4-d9b1-418b-b3b9-e29faf52a653'); {\"model_id\": \"ee2b75cf5f6542f99a17c5ad02bc49fd\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.395. batch acc: 12.5%, Valid acc: 37.0%. Minibatch loss at step 200: 0.589. batch acc: 82.0%, Valid acc: 84.7%. Minibatch loss at step 400: 0.409. batch acc: 89.1%, Valid acc: 86.2%. Minibatch loss at step 600: 0.396. batch acc: 88.3%, Valid acc: 86.5%. Minibatch loss at step 800: 0.435. batch acc: 88.3%, Valid acc: 87.6%. Minibatch loss at step 1000: 0.407. batch acc: 85.2%, Valid acc: 88.5%. Minibatch loss at step 1200: 0.262. batch acc: 91.4%, Valid acc: 88.9%. Minibatch loss at step 1400: 0.411. batch acc: 87.5%, Valid acc: 88.8%. Test acc: 94.3%","tags":"Deep Learning","url":"https://leemeng.tw/regularization-for-multi-layer-neural-networks-in-tensorflow.html"},{"title":"Using TensorFlow to Train a Shallow NN with Stochastic Gradient Descent","text":"The goal here is to progressively train deeper and more accurate models using TensorFlow. We will first load the notMNIST dataset which we have done data cleaning. For the classification problem, we will first train two logistic regression models use simple gradient descent, stochastic gradient descent (SGD) respectively for optimization to see the difference between these optimizers. Finally, train a Neural Network with one-hidden layer using ReLU activation units to see whether we can boost our model's performance further. Previously in 1_notmnist.ipynb , we created a pickle with formatted datasets for training, development and testing on the notMNIST dataset . This post is modified from the jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import os import numpy as np import tensorflow as tf from six.moves import cPickle as pickle from six.moves import range Load notMNIST dataset This time we will use the dataset which has been normalized and randomized before to omit the data preprocessing step. Tips: Release memory after loading big-size dataset using del . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) print ( 'Dataset size: {:.1f} MB' . format ( os . stat ( pickle_file ) . st_size / 2 ** 20 )) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Dataset size: 658.8 MB Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data for easier training Reformat both pixels(features) and labels that's more adapted to the models we're going to train: features(pixels) as a flat matrix with shape = (#total pixels, #instances) Figure 1 : Flattened features labels as float 1-hot encodings with shape = (#type of labels, #instances) Figure 2 : Flattened labels Tips: Notice that we use different shape of matrix with the original TensorFlow example nookbook because I think it's easier to understand how matrix multiplication work by imagining each training/test instance as a column vector. But in response to this change, we have to modify several code in order to make it works! Transpose logits and labels when calling tf.nn.softmax_cross_entropy_with_logits Set dim = 0 when using tf.nn.softmax Set axis = 0 when using np.argmax to compute accuracy One-hot encode labels by compare the label with the 0-9 array and transform True/False array as float array use astype(np.float32) image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T # key point1 return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (10, 10000) Logistic regression with gradient descent For logistic regression, we use the formula $WX + b = Y'$ to do the computation. W is of shape (10, 784), X is of shape (784, m) and Y' is of shape (10, m) where $m$ is the number of training instances/images. After compute the probabilities of 10 classes stored in Y', we will use built-in tf.nn.softmax_cross_entropy_with_logits to compute cross-entropy between Y' and Y(train_labels) as cost. We will first instruct Tensorflow how to do all the computation and make it run the optimization several times. Build the Tensorflow computation graph We're first going to train a multinomial logistic regression using simple gradient descent. TensorFlow works like this: First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below: with graph.as_default(): ... Then you can run the operations on this graph as many times as you want by calling session.run() , providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below: with tf.Session(graph=graph) as session: ... Let's load all the data into TensorFlow and build the computation graph corresponding to our training: # With gradient descent training, even this much data is prohibitive. # Subset the training data for faster turnaround. train_subset = 10000 graph = tf . Graph () # when we want to create multiple graphs in the same script, # use this to encapsulate each graph and run session right after graph definition with graph . as_default (): # Input data. # Load the training, validation and test data into constants that are # attached to the graph. tf_train_dataset = tf . constant ( train_dataset [:, : train_subset ]) tf_train_labels = tf . constant ( train_labels [:, : train_subset ]) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # These are the parameters that we are going to be training. The weight # matrix will be initialized using random values following a (truncated) # normal distribution. The biases get initialized to zero. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. # We multiply the inputs with the weight matrix, and add biases. We compute # the softmax and cross-entropy (it's one operation in TensorFlow, because # it's very common, and it can be optimized). We take the average of this # cross-entropy across all training examples: that's our loss. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. # We are going to find the minimum of this loss using gradient descent. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. # These are not part of training, but merely here so that we can report # accuracy figures as we train. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Tips: As we saw before, logits = tf.matmul(weights, tf_train_dataset) + biases is equivalent to the logistic regression formula $Y' = WX + b$ Transpose y_hat and y to fit in softmax_cross_entropy_with_logits Gradient descent by iterating computation graph Now we can tell TensorFlow to run this computation and iterate. Here we will use tqdm library to help us easily visualize the progress and the time used in the iterations. Tips: Use np.argmax(predictions, axis=0) to transfrom one-hot encoded labels back to singe number for every data points. Use .eval() to get the predictions for test/validation set from tqdm import tnrange num_steps = 801 def accuracy ( predictions , labels ): \"\"\"For every (logit/Z, y) pair, get the (predicted label, label) and count the occurence where predicted label == label and divide by the total number of data points. \"\"\" return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) # Calculate the correct predictions # correct_prediction = tf.equal(tf.argmax(predictions), tf.argmax(labels)) # # Calculate accuracy on the test set # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) return accruacy with tf . Session ( graph = graph ) as session : # This is a one-time operation which ensures the parameters get initialized as # we described in the graph: random weights for the matrix, zeros for the # biases. tf . global_variables_initializer () . run () print ( 'Initialized' ) for step in tnrange ( num_steps ): # Run the computations. We tell .run() that we want to run the optimizer, # and get the loss value and the training predictions returned as numpy # arrays. _ , l , predictions = session . run ([ optimizer , loss , train_prediction ]) if ( step % 100 == 0 ): print ( 'Cost at step {} : {:.3f} . Training acc: {:.1f} %, Validation acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , train_labels [:, : train_subset ]), accuracy ( valid_prediction . eval (), valid_labels ), \">\" )) # Calling .eval() on valid_prediction is basically like calling run(), but # just to get that one numpy array. Note that it recomputes all its graph # dependencies. print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#b60808c6-176f-4330-b67d-e28cce33ba43'); {\"model_id\": \"a776469c0f394bab94bc900cf03f469d\", \"version_major\": 2, \"version_minor\": 0} Cost at step 0: 20.057. Training acc: 6.4%, Validation acc: 10.0%. Cost at step 100: 2.326. Training acc: 70.9%, Validation acc: 70.7%. Cost at step 200: 1.868. Training acc: 73.9%, Validation acc: 73.4%. Cost at step 300: 1.611. Training acc: 75.4%, Validation acc: 74.5%. Cost at step 400: 1.436. Training acc: 76.4%, Validation acc: 74.8%. Cost at step 500: 1.306. Training acc: 77.1%, Validation acc: 75.1%. Cost at step 600: 1.207. Training acc: 77.8%, Validation acc: 75.5%. Cost at step 700: 1.127. Training acc: 78.5%, Validation acc: 75.6%. Cost at step 800: 1.062. Training acc: 79.2%, Validation acc: 75.9%. Test acc: 82.8% Logistic regression with SGD Or more precisely, mini-batch approach. From the result above, we can see it cost about 20 seconds (on my computer) to iterate 10,000 training instances by simple gradient descent. Let's now switch to stochastic gradient descent training instead, which is much faster. The graph will be similar, except that instead of holding all the training data into a constant node, we create a Placeholder node which will be fed actual data at every call of session.run() . Tips: The difference between SGD and gradient descent is that the former don't use whole training set to compute gradient descent, instead just use a 'mini-batch' of it and assume the corresponding gradient descent is the way to optimize. So we will keep using GradientDescentOptimizer but with a different loss computed from a smaller sub-training set. Figure 3 : SGD vs Gradient Descent Build computation graph batch_size = 128 graph = tf . Graph () with graph . as_default (): # Input data. For the training data, we use a placeholder that will be fed # at run time with a training minibatch. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Iterate using SGD num_steps = 3001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # Pick an offset within the training data, which has been randomized. # Note: we could use better randomization across epochs. offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) # Generate a minibatch. batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] # Prepare a dictionary telling the session where to feed the minibatch. # The key of the dictionary is the placeholder node of the graph to be fed, # and the value is the numpy array to feed to it. feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#1124985b-ce80-45d3-bdd2-a0b5f317734e'); {\"model_id\": \"2d2a36e34f9843b5a38a9fe105281093\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 20.939. batch acc: 6.2%, Valid acc: 9.7%. Minibatch loss at step 500: 2.546. batch acc: 70.3%, Valid acc: 75.1%. Minibatch loss at step 1000: 1.520. batch acc: 74.2%, Valid acc: 76.3%. Minibatch loss at step 1500: 1.441. batch acc: 76.6%, Valid acc: 77.8%. Minibatch loss at step 2000: 1.135. batch acc: 79.7%, Valid acc: 77.1%. Minibatch loss at step 2500: 1.225. batch acc: 72.7%, Valid acc: 78.8%. Minibatch loss at step 3000: 0.932. batch acc: 76.6%, Valid acc: 79.4%. Test acc: 86.9% It took only about 3 seconds in my computer to finish the optimization using SGD (which took gradient descent about 20 seconds) and got a even slightly better result. The key of SGD is take randomized samples / mini-batches and feed that into the model every iteration (thus the feed_dict term). 2-layer NN with ReLU units Instead all just linear combination of features, we want to introduce non-linearlity in our logistic regression. By turning the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes, we should be able to improve validation / test accuracy. A 2-layer NN (1-hidden layer NN) look like this: Figure 4 : 1 hidden-layer NN A ReLU activation unit look like this: Figure 5 : ReLU Build compuation graph In this part, use the notation $X$ in replace of dataset . The weights and biases of the hidden layer are denoted as $W1$ and $b1$, and the weights and biases of the output layer are denoted as $W2$ and $b2$. Thus the pre-activation output(logits) of output layer is computed as $ logits = W2 * ReLU(W1 * X + b1) + b2 $ batch_size = 128 num_hidden_unit = 1024 graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) # use all valid/test set tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # initialize weights, biases # notice that we have a new hidden layer so we now have W1, b1, W2, b2 W1 = tf . Variable ( tf . truncated_normal ([ num_hidden_unit , image_size * image_size ])) b1 = tf . Variable ( tf . zeros ([ num_hidden_unit , 1 ])) W2 = tf . Variable ( tf . truncated_normal ([ num_labels , num_hidden_unit ])) b2 = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # training computation logits = tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_train_dataset ) + b1 )) + b2 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # optimizer optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # valid / test prediction - y_hat train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_valid_dataset ) + b1 )) + b2 , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_test_dataset ) + b1 )) + b2 , dim = 0 ) Run the iterations num_steps = 3001 with tf . Session ( graph = graph ) as session : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) # take steps to optimize for step in tnrange ( num_steps ): # generate randomized mini-batches offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#955122f0-c581-460c-a334-d83be0c82999'); {\"model_id\": \"f379ee5b4ed04eb18e5ee8d6b1e69a17\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 409.203. batch acc: 4.7%, Valid acc: 30.5%. Minibatch loss at step 500: 12.319. batch acc: 75.8%, Valid acc: 80.7%. Minibatch loss at step 1000: 12.638. batch acc: 74.2%, Valid acc: 80.8%. Minibatch loss at step 1500: 7.635. batch acc: 77.3%, Valid acc: 81.2%. Minibatch loss at step 2000: 7.322. batch acc: 80.5%, Valid acc: 81.4%. Minibatch loss at step 2500: 10.451. batch acc: 76.6%, Valid acc: 80.1%. Minibatch loss at step 3000: 3.914. batch acc: 83.6%, Valid acc: 82.7%. Test acc: 88.7% Summary Because we use a more complex model(1 hidden-layer NN), it take a little longer to train, but we're able to gain more performance from logistic regression even with the same hyper-parameter settings (learning rate = 0.5, batch_size=128). Better performance may be gained by tuning hyper parameters of the 2 layer NN. Also notice that by using mini-batch / SGD, we can save lots of time training models and even get a better result.","tags":"Deep Learning","url":"https://leemeng.tw/using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent.html"},{"title":"Simple Image Recognition using NotMNIST dataset","text":"Today we're going to do some simple image recogintion using NotMNIST dataset. But before creating model for prediction, it's more important to explore, clean and normalize our dataset in order to make the learning go smoother when we actually build predictive models. I motified the notebook from Udacity's online Deep learning course and the objective of this assignment is to learn about simple data curation practices , and familiarize you with some of the data we'll be reusing later. This notebook uses the notMNIST dataset to be used with python experiments. This dataset is designed to look like the classic MNIST dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST. This notebook is mainly foucsing on data preprocessing rather than building models. Workflow Download / load raw notMNIST dataset Drop unreadable images and save the remaining images Combine all images and divide it into testing/validation/test set Shuffle / Randomize the dataset Remove duplicate images appear both in train/test or train/validation set Build simple model for image recognition using different size of training data After finishing this notebook, we learn Use matplotlib to read images, transform them to ndarray and render. Identify whether there exist unbalanced problem for the labels of classification . Understand why it's important to have both valid and test set. Identify the importance of randomizing data for better efficieny when training sequentially. Identify duplicate images between training/test set. Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import matplotlib.pyplot as plt import numpy as np import os import sys import tarfile from IPython.display import display from PIL import Image from scipy import ndimage from sklearn.linear_model import LogisticRegression from six.moves.urllib.request import urlretrieve from six.moves import cPickle as pickle # Config the matplotlib backend as plotting inline in IPython % matplotlib inline Dataset Download compressed dataset if the dataset is not available yet First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labeled examples. Given these sizes, it should be possible to train models quickly on any machine. url = 'https://commondatastorage.googleapis.com/books1000/' last_percent_reported = None data_root = './datasets' # Change me to store data elsewhere def download_progress_hook ( count , blockSize , totalSize ): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int ( count * blockSize * 100 / totalSize ) if last_percent_reported != percent : if percent % 5 == 0 : sys . stdout . write ( \" %s%% \" % percent ) sys . stdout . flush () else : sys . stdout . write ( \".\" ) sys . stdout . flush () last_percent_reported = percent def maybe_download ( filename , expected_bytes , force = False ): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os . path . join ( data_root , filename ) if force or not os . path . exists ( dest_filename ): print ( 'Attempting to download:' , filename ) filename , _ = urlretrieve ( url + filename , dest_filename , reporthook = download_progress_hook ) print ( ' \\n Download Complete!' ) statinfo = os . stat ( dest_filename ) if statinfo . st_size == expected_bytes : print ( 'Found and verified' , dest_filename ) else : raise Exception ( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?' ) return dest_filename train_filename = maybe_download ( 'notMNIST_large.tar.gz' , 247336696 ) test_filename = maybe_download ( 'notMNIST_small.tar.gz' , 8458043 ) Found and verified ./datasets/notMNIST_large.tar.gz Found and verified ./datasets/notMNIST_small.tar.gz Extract the dataset into folders by characters Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labeled A through J. num_classes = 10 np . random . seed ( 133 ) def maybe_extract ( filename , force = False ): root = os . path . splitext ( os . path . splitext ( filename )[ 0 ])[ 0 ] # remove .tar.gz if os . path . isdir ( root ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping extraction of %s .' % ( root , filename )) else : print ( 'Extracting data for %s . This may take a while. Please wait.' % root ) tar = tarfile . open ( filename ) sys . stdout . flush () tar . extractall ( data_root ) tar . close () data_folders = [ os . path . join ( root , d ) for d in sorted ( os . listdir ( root )) if os . path . isdir ( os . path . join ( root , d ))] if len ( data_folders ) != num_classes : raise Exception ( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes , len ( data_folders ))) print ( data_folders ) return data_folders train_folders = maybe_extract ( train_filename ) test_folders = maybe_extract ( test_filename ) ./datasets/notMNIST_large already present - Skipping extraction of ./datasets/notMNIST_large.tar.gz. ['./datasets/notMNIST_large/A', './datasets/notMNIST_large/B', './datasets/notMNIST_large/C', './datasets/notMNIST_large/D', './datasets/notMNIST_large/E', './datasets/notMNIST_large/F', './datasets/notMNIST_large/G', './datasets/notMNIST_large/H', './datasets/notMNIST_large/I', './datasets/notMNIST_large/J'] ./datasets/notMNIST_small already present - Skipping extraction of ./datasets/notMNIST_small.tar.gz. ['./datasets/notMNIST_small/A', './datasets/notMNIST_small/B', './datasets/notMNIST_small/C', './datasets/notMNIST_small/D', './datasets/notMNIST_small/E', './datasets/notMNIST_small/F', './datasets/notMNIST_small/G', './datasets/notMNIST_small/H', './datasets/notMNIST_small/I', './datasets/notMNIST_small/J'] Problem 1 - Sample some images in dataset and render them Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display. For each character folder in dataset, randomly choose one picture in it and render them horizontally Keypoints: use os.listdir() to get list of file in a folder use mpl.image.imread to read in image as ndarray, and use plt.imshow to render ndarray as images ! ls ./datasets/notMNIST_small/ A B.pickle D E.pickle G H.pickle J A.pickle C D.pickle F G.pickle I J.pickle B C.pickle E F.pickle H I.pickle characters = 'abcdefghij' . upper () # sub folders to choose images from image_per_folder = 4 # number of images to show for each folder BASE_PATH = './datasets/notMNIST_small/' list_of_images = [] for _ in range ( image_per_folder ): for char in characters : char_folder = BASE_PATH + char + '/' images = os . listdir ( char_folder ) image_file_name = images [ np . random . randint ( len ( images ))] list_of_images . append ( char_folder + image_file_name ) def showImagesHorizontally ( list_of_files ): from matplotlib.pyplot import figure , imshow , axis from matplotlib.image import imread number_of_files = len ( list_of_files ) num_char = len ( characters ) for row in range ( int ( number_of_files / num_char )): fig = figure ( figsize = ( 15 , 5 )) for i in range ( num_char ): a = fig . add_subplot ( 1 , num_char , i + 1 ) image = imread ( list_of_files [ row * num_char + i ]) imshow ( image , cmap = 'gray' ) axis ( 'off' ) showImagesHorizontally ( list_of_images ) Data curation drop unreadable images normalization pickle the normalized data by characters / folders Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size. We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. A few images might not be readable, we'll just skip them. Debugging In case of following error occur: ImportError : Could not import the Python Imaging Library ( PIL ) required to load image files . Follow these steps: pip install pillow replace from IPython.display import display , Image to from IPython.display import display from PIL import Image Keypoints: Normalize image using image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth where pixel_depth == 255 . References https://stackoverflow.com/questions/41124353/importerror-could-not-import-the-python-imaging-library-pil-required-to-load image_size = 28 # Pixel width and height. pixel_depth = 255.0 # Number of levels per pixel. def load_letter ( folder , min_num_images ): \"\"\"Load the data for a single letter label.\"\"\" image_files = os . listdir ( folder ) dataset = np . ndarray ( shape = ( len ( image_files ), image_size , image_size ), dtype = np . float32 ) print ( folder ) num_images = 0 for image in image_files : image_file = os . path . join ( folder , image ) try : image_data = ( ndimage . imread ( image_file ) . astype ( float ) - pixel_depth / 2 ) / pixel_depth if image_data . shape != ( image_size , image_size ): raise Exception ( 'Unexpected image shape: %s ' % str ( image_data . shape )) dataset [ num_images , :, :] = image_data num_images = num_images + 1 except IOError as e : print ( 'Could not read:' , image_file , ':' , e , '- it \\' s ok, skipping.' ) dataset = dataset [ 0 : num_images , :, :] if num_images < min_num_images : raise Exception ( 'Many fewer images than expected: %d < %d ' % ( num_images , min_num_images )) print ( 'Full dataset tensor:' , dataset . shape ) print ( 'Mean:' , np . mean ( dataset )) print ( 'Standard deviation:' , np . std ( dataset )) return dataset def maybe_pickle ( data_folders , min_num_images_per_class , force = False ): dataset_names = [] for folder in data_folders : set_filename = folder + '.pickle' dataset_names . append ( set_filename ) if os . path . exists ( set_filename ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping pickling.' % set_filename ) else : print ( 'Pickling %s .' % set_filename ) dataset = load_letter ( folder , min_num_images_per_class ) try : with open ( set_filename , 'wb' ) as f : pickle . dump ( dataset , f , pickle . HIGHEST_PROTOCOL ) except Exception as e : print ( 'Unable to save data to' , set_filename , ':' , e ) return dataset_names train_datasets = maybe_pickle ( train_folders , 45000 ) test_datasets = maybe_pickle ( test_folders , 1800 ) ./datasets/notMNIST_large/A.pickle already present - Skipping pickling. ./datasets/notMNIST_large/B.pickle already present - Skipping pickling. ./datasets/notMNIST_large/C.pickle already present - Skipping pickling. ./datasets/notMNIST_large/D.pickle already present - Skipping pickling. ./datasets/notMNIST_large/E.pickle already present - Skipping pickling. ./datasets/notMNIST_large/F.pickle already present - Skipping pickling. ./datasets/notMNIST_large/G.pickle already present - Skipping pickling. ./datasets/notMNIST_large/H.pickle already present - Skipping pickling. ./datasets/notMNIST_large/I.pickle already present - Skipping pickling. ./datasets/notMNIST_large/J.pickle already present - Skipping pickling. ./datasets/notMNIST_small/A.pickle already present - Skipping pickling. ./datasets/notMNIST_small/B.pickle already present - Skipping pickling. ./datasets/notMNIST_small/C.pickle already present - Skipping pickling. ./datasets/notMNIST_small/D.pickle already present - Skipping pickling. ./datasets/notMNIST_small/E.pickle already present - Skipping pickling. ./datasets/notMNIST_small/F.pickle already present - Skipping pickling. ./datasets/notMNIST_small/G.pickle already present - Skipping pickling. ./datasets/notMNIST_small/H.pickle already present - Skipping pickling. ./datasets/notMNIST_small/I.pickle already present - Skipping pickling. ./datasets/notMNIST_small/J.pickle already present - Skipping pickling. Problem 2 Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot. Load test data and show normalized images for each pickled dataset notMNIST_small only in order to prevent memory-insufficient problem when load all train data Keypoints: use pickle.load(open(file_path, 'rb')) to load a pickle create a figure obj and use fig.add_subplot(1, len(images), i+1) to add subplot for each image render ndarray represent images using matplotlob.pyplot.imshow(image) images = [] for file_path in test_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) images . append ( data [ 0 , :, :]) from matplotlib.pyplot import figure , imshow , axis fig = figure ( figsize = ( 15 , 5 )) for i , image in enumerate ( images ): a = fig . add_subplot ( 1 , len ( images ), i + 1 ) imshow ( image , cmap = 'gray' ) axis ( 'off' ) Number of samples in ./datasets/notMNIST_small/A.pickle: 1872 Number of samples in ./datasets/notMNIST_small/B.pickle: 1873 Number of samples in ./datasets/notMNIST_small/C.pickle: 1873 Number of samples in ./datasets/notMNIST_small/D.pickle: 1873 Number of samples in ./datasets/notMNIST_small/E.pickle: 1873 Number of samples in ./datasets/notMNIST_small/F.pickle: 1872 Number of samples in ./datasets/notMNIST_small/G.pickle: 1872 Number of samples in ./datasets/notMNIST_small/H.pickle: 1872 Number of samples in ./datasets/notMNIST_small/I.pickle: 1872 Number of samples in ./datasets/notMNIST_small/J.pickle: 1872 Problem 3 We expect the data to be balanced across classes. By problem 2 above, we already see the data is balanced in test set, verify data is balanced in train data as well for file_path in train_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) Number of samples in ./datasets/notMNIST_large/A.pickle: 52909 Number of samples in ./datasets/notMNIST_large/B.pickle: 52911 Number of samples in ./datasets/notMNIST_large/C.pickle: 52912 Number of samples in ./datasets/notMNIST_large/D.pickle: 52911 Number of samples in ./datasets/notMNIST_large/E.pickle: 52912 Number of samples in ./datasets/notMNIST_large/F.pickle: 52912 Number of samples in ./datasets/notMNIST_large/G.pickle: 52912 Number of samples in ./datasets/notMNIST_large/H.pickle: 52912 Number of samples in ./datasets/notMNIST_large/I.pickle: 52912 Number of samples in ./datasets/notMNIST_large/J.pickle: 52911 Merge seperate character dataset together (for all training/test/valid) Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune train_size as needed. The labels will be stored into a separate array of integers 0 through 9. Also create a validation dataset for hyperparameter tuning. Keypoints: Why we need vaild/test set? Our goal is to make sure the model we train can generalize to brand-new data. If we only divide the whole dataset into training/test set and try to get the best model by tuning parameters on test set, chances are that we actully give the model 'hints' on training set by our eyes. Because eventually, the best model will use the parameters incorporating our knowledge about training dataset, and when the very brand-new data comes, our model can't actually generalize to it and make wrong predictions. So we have to use vaild/dev set to tune our model, and only test the performance on test set to simulate model's 'real-world' performance after deploying it. def make_arrays ( nb_rows , img_size ): if nb_rows : dataset = np . ndarray (( nb_rows , img_size , img_size ), dtype = np . float32 ) labels = np . ndarray ( nb_rows , dtype = np . int32 ) else : dataset , labels = None , None return dataset , labels def merge_datasets ( pickle_files , train_size , valid_size = 0 ): num_classes = len ( pickle_files ) valid_dataset , valid_labels = make_arrays ( valid_size , image_size ) train_dataset , train_labels = make_arrays ( train_size , image_size ) vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v , start_t = 0 , 0 end_v , end_t = vsize_per_class , tsize_per_class end_l = vsize_per_class + tsize_per_class for label , pickle_file in enumerate ( pickle_files ): try : with open ( pickle_file , 'rb' ) as f : letter_set = pickle . load ( f ) # let's shuffle the letters to have random validation and training set np . random . shuffle ( letter_set ) if valid_dataset is not None : valid_letter = letter_set [: vsize_per_class , :, :] valid_dataset [ start_v : end_v , :, :] = valid_letter valid_labels [ start_v : end_v ] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set [ vsize_per_class : end_l , :, :] train_dataset [ start_t : end_t , :, :] = train_letter train_labels [ start_t : end_t ] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e : print ( 'Unable to process data from' , pickle_file , ':' , e ) raise return valid_dataset , valid_labels , train_dataset , train_labels train_size = 200000 valid_size = 10000 test_size = 10000 valid_dataset , valid_labels , train_dataset , train_labels = merge_datasets ( train_datasets , train_size , valid_size ) _ , _ , test_dataset , test_labels = merge_datasets ( test_datasets , test_size ) print ( 'Training:' , train_dataset . shape , train_labels . shape ) print ( 'Validation:' , valid_dataset . shape , valid_labels . shape ) print ( 'Testing:' , test_dataset . shape , test_labels . shape ) Training: (200000, 28, 28) (200000,) Validation: (10000, 28, 28) (10000,) Testing: (10000, 28, 28) (10000,) Randomize data Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match. The purpose of randomizing data is to perserve the assumption that we get the data randomly as predict phase. We don't want to train models sequantially on training instances like AAA ... BBB ... CCC. Keypoints: train set and test set should apply the same shuffle shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] Although we do only one randomization here, we may do it multiple times when we are going to use the same dataset many times. def randomize ( dataset , labels ): permutation = np . random . permutation ( labels . shape [ 0 ]) shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] return shuffled_dataset , shuffled_labels train_dataset , train_labels = randomize ( train_dataset , train_labels ) test_dataset , test_labels = randomize ( test_dataset , test_labels ) valid_dataset , valid_labels = randomize ( valid_dataset , valid_labels ) Problem 4 - Sanity check after shuffling dataset Convince yourself that the data is still good after shuffling! Randomly sample data instances to make sure X is corresponding to y for both training and test set after shuffling. Keypoints: label is start from 0 to 9 for A to J lookup_labels = { k : v for ( k , v ) in zip ( np . arange ( 10 ), 'ABCDEFGHIJ' )} def sanity_check ( X , y , s = None ): print ( s ) m1 , m2 = X . shape [ 0 ], y . shape [ 0 ] assert ( m1 == m2 ) # randomly choose 10 images to check label indices = np . random . randint ( 0 , m1 , size = 10 ) fig = plt . figure ( figsize = ( 15 , 5 )) for i , idx in enumerate ( indices ): fig . add_subplot ( 1 , len ( indices ), i + 1 ) plt . imshow ( X [ idx , :, :], cmap = 'gray' ) plt . title ( lookup_labels [ y [ idx ]]) plt . axis ( 'off' ) sanity_check ( train_dataset , train_labels , 'Training dataset:' ) Training dataset: sanity_check ( test_dataset , test_labels , 'Test dataset:' ) Test dataset: Serialize dataset for later usage Everything looks good, finally, let's save the data for later reuse: pickle_file = os . path . join ( data_root , 'notMNIST.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : valid_dataset , 'valid_labels' : valid_labels , 'test_dataset' : test_dataset , 'test_labels' : test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 690800503 Problem 5 - Remove overlapping samples in test/valid set By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples. Optional questions: What about near duplicates between datasets? (images that are almost identical) Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments. Find duplicate images in test set I used Cosine Similarity to measure whether image A in training set is identical to those image B in valid/test set. As formula suggested, I reshape every image in training set into column vectors, and do the same for images in valid/test set and compute similarity between the vectors. Although we can compute cosine similarity for every (train image vector, test image vector) explicitly, it's better to use vectorization to speed up computation since that we have 200,000 training images and 10,000 valid/test images. (Although it still take about 10 minutes to run in my computer) Keypoints: Vectorize both training/test images and compute cosine similarity using cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y)) . The output matrix cosine_sim will be shape (m1, m2) where m1 is the number of training images and m2 the number of test images. cosine_matrix(i, j) mean the cosine similarity between training image #i and test image #j. %% time def get_duplicate_data(source_dataset, target_dataset, threshold=1, num_duplicate_to_show=0): X = source_dataset.reshape(source_dataset.shape[0], -1) Y = target_dataset.reshape(target_dataset.shape[0], -1) assert(X.shape[1] == Y.shape[1]) dim = X.shape[1] cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y)) assert(cosine_sim.shape == (X.shape[0], Y.shape[0])) # for each image in training set, find corresponding duplicate in test/valid set dup_target_indices = [] show_duplicate_counter = 0 for source_idx in range(cosine_sim.shape[0]): dup_indices = list(np.where(cosine_sim[source_idx, :] >= threshold)[0]) # render duplicate images when is available. may omit if visual output is not required if dup_indices and num_duplicate_to_show and (show_duplicate_counter < num_duplicate_to_show): # show only non-redudent duplicate images for i in dup_indices: if i in dup_target_indices: dup_indices.remove(i) if not dup_indices: continue if len(dup_indices) == 1: fig = plt.figure(figsize=(3, 15)) fig.add_subplot(1, len(dup_indices) + 1, 1) plt.imshow(source_dataset[source_idx, :, :], cmap='gray') plt.title('Source: ' + str(source_idx)) plt.axis('off') for i, target_idx in enumerate(dup_indices): fig.add_subplot(1, len(dup_indices) + 1, i + 2) plt.imshow(target_dataset[target_idx, :, :], cmap='gray') plt.title('Target: ' + str(target_idx)) plt.axis('off') show_duplicate_counter += 1 dup_target_indices.extend(dup_indices) return list(set(dup_target_indices)) dup_indices_test = get_duplicate_data(train_dataset, test_dataset, num_duplicate_to_show=5) print('Number of duplicates in test dataset: {}'.format(len(dup_indices_test))) Number of duplicates in test dataset: 1768 CPU times: user 5min 11s, sys: 4min 17s, total: 9min 29s Wall time: 7min 40s Duplicate images in validation set %% time dup_indices_valid = get_duplicate_data(train_dataset, valid_dataset, num_duplicate_to_show=5) print('Number of duplicates in validation dataset: {}'.format(len(dup_indices_valid))) Number of duplicates in validation dataset: 1507 CPU times: user 7min 52s, sys: 5min 7s, total: 12min 59s Wall time: 8min 57s Serialize sanitized dataset for later model performance comparison Remove duplicates in test/valid set Save dataset Keypoints: Do the same operation to both X and y print ( \"Number of duplicate images in test set: {} \" . format ( len ( dup_indices_test ))) print ( \"Number of duplicate images in valid set: {} \" . format ( len ( dup_indices_valid ))) Number of duplicate images in test set: 1768 Number of duplicate images in valid set: 1507 non_duplicate_indices = [ i for i in range ( test_dataset . shape [ 0 ]) if not i in dup_indices_test ] sanitized_test_dataset = test_dataset [ non_duplicate_indices , :, :] sanitized_test_labels = test_labels [ non_duplicate_indices ] non_duplicate_indices = [ i for i in range ( valid_dataset . shape [ 0 ]) if not i in dup_indices_valid ] sanitized_valid_dataset = valid_dataset [ non_duplicate_indices , :, :] sanitized_valid_labels = valid_labels [ non_duplicate_indices ] pickle_file = os . path . join ( data_root , 'notMNIST_sanitized.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : sanitized_valid_dataset , 'valid_labels' : sanitized_valid_labels , 'test_dataset' : sanitized_test_dataset , 'test_labels' : sanitized_test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 680517003 Problem 6 - Build naive classifier using logistic regression Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it. Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model. Optional question: train an off-the-shelf model on all the data! from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score plt . style . use ( 'ggplot' ) np . random . seed ( 42 ) train_sizes = [ 100 , 1000 , 50000 , 100000 , 200000 ] # train models using different size of training set test_scores , test_scores_sanitized = [[] for _ in range ( 2 )] for train_size in train_sizes : # random choose #train_size of training instances indices = np . random . randint ( 0 , train_dataset . shape [ 0 ], train_size ) # reshape images to (train_size, dim * dim) for easier processing X = train_dataset [ indices , :, :] \\ . reshape ( - 1 , train_dataset . shape [ 1 ] * train_dataset . shape [ 2 ]) y = train_labels [ indices ] # train model clf = ( LogisticRegression ( random_state = 10 , solver = 'lbfgs' , multi_class = 'multinomial' ) . fit ( X , y )) # test on original test set and the sanitized one y_pred = clf . predict ( test_dataset . reshape ( test_dataset . shape [ 0 ], - 1 )) y_pred_sanitized = clf . predict ( sanitized_test_dataset . reshape ( sanitized_test_dataset . shape [ 0 ], - 1 )) test_score = accuracy_score ( y_pred , test_labels ) test_score_sanitized = accuracy_score ( y_pred_sanitized , sanitized_test_labels ) test_scores . append ( test_score ) test_scores_sanitized . append ( test_score_sanitized ) # print(classification_report(test_labels, y_pred)) # print(accuracy_score(test_labels, y_pred)) plt . figure ( figsize = ( 7 , 7 )) plt . xlabel ( 'Training size' , fontsize = 20 ) plt . ylabel ( 'Accuracy' , fontsize = 20 ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) for x , y in zip ( train_sizes , test_scores ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) for x , y in zip ( train_sizes , test_scores_sanitized ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) plt . scatter ( train_sizes , test_scores , label = 'Test score' , color = 'blue' ); plt . scatter ( train_sizes , test_scores_sanitized , label = 'Test score(Sanitized)' , color = 'red' ); plt . legend ( loc = 4 ) plt . title ( 'Test set Accuracy vs Training size' , fontsize = 25 ); As show above, there are seveal things worth mentioning: Our model became better on classifiying labels in test set by using more data in training phase. By training a simple logistic regression model on all available training data, we can expect to get about 90% accuracy. There is a performance gap (2~3% on accuracy) between test set with duplicate images and the one without. Depends on our application, choose the one which best fit our use case. Notice that we just want to have a off-the-shelf model quickly, so we don't even tune hyper-parameters using validation set. We may be able to further improve model's predictive performance by tuning hyper-parameters or using Neural Network in another notebook.","tags":"Machine learning","url":"https://leemeng.tw/simple-image-recognition-using-notmnist-dataset.html"},{"title":"Purpose of this blog","text":"第一篇文章做一點 blog 的簡介，打算把自己在學 data science 還有 machine learning 過程中寫的筆記還有在 MOOC 上課的 code (主要是 jupyter notebook) 記錄下來方便自己以後搜尋。 雖然目前為止我都將 jupyter notebook render 成 HTML 然後存到 Evernote 搜尋, 但是如果該 notebook 一直更新的話就變得很不實際.. </br> 這次利用 python 的 pelican 將 jupyter notebook 轉成靜態網頁, 讓我可以不斷更新 notebooks 而且也希望 code 可以幫助到其他也在做 data science / machine learning 的人當作一些參考。 blog 的走向目前看來應該會是中英夾雜 .. This is the first post of my blog. </br> I decided to record my learning path toward data scientist / machine learning engineer and make it easier for my future self to review what I have learnt. This blog will include codes from the courses on MOOC like Coursera, Udacity and also some pet projects. It would also be wonderful if these code and thought can help someone who want to become a data scientist.","tags":"Miscellaneous","url":"https://leemeng.tw/purpose-of-this-blog.html"}]}