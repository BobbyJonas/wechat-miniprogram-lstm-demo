{"pages":[{"title":"資料科學文摘 Vol.1 AutoML、Airflow 及 DAU","text":"作為「資料科學文摘」系列文的第一篇，在開始介紹一些優質的文章之前，請讓我說明稍微一下為何會有這系列文章的誕生。先說結論：我希望透過分享一些優質文章的重點摘要，讓更多人能更快地掌握資料科學領域的知識，找出自己有興趣的領域專研，並激盪出更多的討論。 在開始閱讀之前 或許所有職業皆如此，但個人認為資料科學家是前幾不「佛系」的一個職業，你需要擁有非常多知識來讓工作更為順利： 特定程式語言如 Python、SQL 及 R 的使用方法 統計分析方法 建構資料管道（Data Pipeline） 訓練並部署機器學習模型 業界趨勢 ... 當然，你也可以選擇當個佛系資料科學家（如果你知道有哪間企業在應徵，站內信 500 P）。 不過身為一個（非佛系）資料科學家，我常需要閱讀大量的相關文章、新知，並且嘗試模仿文章內出現的演算法、分析手法，加以實踐並應用在自己的工作上。 愛爾蘭詩人 奧斯卡．王爾德 曾說過一句 名言 ： You are what you read. - Oscar Wilde 以資料科學為例，你讀越多相關文章，你就越接近一個資料科學家。不管要精通什麼能力，最快的方式都是透過「模仿」專家怎麼做的。透過閱讀大量的相關文章並從它們學習及模仿，我們可以更快地，且有效率地成為一個稱職的資料科學家。 以往我在閱讀完不錯的文章以後，都會在 Evernote 裡頭寫重點摘要以供自己之後做參考、連結不同領域的知識。在回顧的時候節省了自己大量的時間。有鑑於現在越來越多人對資料科學有興趣，透過分享自己的文摘，希望能讓沒有什麼時間的人也能快速地了解新知，並進一步閱讀自己有興趣的文章。 前言說得夠多了，讓我們來看看這週的文摘吧！ 本週分享：機器學習、資料工程及 App 分析 這週想分享 5 篇文章的文摘，大致上可分為三個主題。這週因為想一次分享 Rachel Thomas 在 fast.ai 談 AutoML 的三篇系列文章，機器學習的文章比例會佔得比較重。 機器學習 What do machine learning practitioners actually do An Opinionated Introduction to AutoML and Neural Architecture Search Google's AutoML: Cutting Through the Hype 資料工程 Airflow: a workflow management platform App 業界分析 DAU/MAU is an important metric to measure engagement, but here's where it fails 除了 AutoML 的系列文以外，閱讀順序不限：） What do machine learning practitioners actually do 這篇介紹機器學習工程師平常在做些什麼，在理解這點以後，我們才知道中間有什麼地方可以自動化，以讓機器學習專案更有效率。 一個完整的機器學習專案通常會包含這些步驟： 理解企業脈絡 清理＆準備資料 訓練模型 實際部署 事後監控模型表現 針對每個步驟，文內都有進一步的項目細分以及解釋，推薦閱讀。儘管一個機器學習工程師不需要自己做所有步驟，了解它們會讓專案更為順利。 就算是專業的研究者，訓練一個深度學習的模型也不是一件非常簡單的事情。而這是 AutoML 以及其子領域，神經結構搜索（neural architecture search）嘗試要解決的。 Google 甚至號稱「只要我們有現在的一百倍計算能力，就可以取代所有機器學習人才」。 An Opinionated Introduction to AutoML and Neural Architecture Search 神經結構搜索或者 AutoML 領域可以幫助我們在「訓練模型」這個步驟的時候，訓練並選擇出最好的超參數（Hyperparameters）。 但如同上篇文所述，這通常只是機器學習專案的其中一小部分，資料科學家或機器學習相關人才並不會因此被全部取代且失業。 現在 AutoML 是非常計算密集（Computation-intensive）的：拿大量的 GPU 計算能力換取研究員的時間。但沒有大量計算能力的人，等 Google 等大公司把最佳化的架構推出來再使用或許是一個比較實際的方案。 DARTS 也是 CMU 與 DeepMind 在嘗試解決「神經結構搜索」這個問題時提出的一個架構，不過他們的假設是所有可行的模型之間是「連續的」，因此可以用常見的「梯度學習」的方式找出最佳模型。這個概念使得他們所需要的計算資源大量減少，值得關注。 Google's AutoML: Cutting Through the Hype 作者認為 Google 在推廣 AutoML 的主張：「我們需要更多計算能力來做神經架構搜尋」值得懷疑，因為就算我們能自動化搜尋出最好的神經模型架構，如何用這些模型解決真正的企業問題、如何實際部署並持續改善機器學習應用等課題，都需要人動腦筋來解決，而這部分還無法自動化。 另外畢竟不是所有做機器學習的人都需要、且有（計算）能力使用神經架構搜尋來訓練自己的模型。但我們可以透過轉換學習（Transfer Learning）來使用已訓練過的模型（pre-trained model）來解決類似問題。與其想著自己也要做最夯的神經架構搜尋，不如多多善用如 Dropout、Batch Normalization 以及 ReLU Linear Unit 來強化模型的預測能力。 不過 Google Colab Notebook 是不錯的免費計算資源，可以善加利用。 Airflow: a workflow management platform 資料科學家在進行各式各樣的分析前，首先需要做的事情通常是蒐集、整理並匯總各式各樣的資料來源以供分析。舉幾個例子： 建立數據倉儲（Data Warehousing） 做 A/B 測試的效果分析 Sessionization：了解使用者在一個 session 裡頭的探訪的網頁、點擊的廣告等活動 為了做這些分析，資料團隊需要建立可靠的資料管道及 ETL，來確保有資料可供分析以及保證資料的品質。 Airflow 是一個由 Airbnb 開發，以 Python 實作的工作流管理系統（Workflow Management System, WMS）。 Airflow 被設計來幫助資料科學家們專注在建構資料管道的邏輯，而不是擔心如果資料管道中間出了什麼差錯時該怎麼維護、重新啟動工作流。（Airflow 有會自己重試失敗工作、當失敗時通知工程師等方便功能）。 Airflow 現在已經進入 Apache 孵化器 ，前景可期。其作者 Maxime Beauchemin 在這篇用淺顯易懂的方式解說 Airflow，值得一看。手癢的朋友可以參考 Quickstart 以及 Tutorial 。 SmartNews 也有在使用 Airflow，之後也會寫一篇 Airflow 的新手指南，敬請密切關注：） DAU/MAU is an important metric to measure engagement, but here's where it fails 多年前由 Facebook 開始使用，DAU / MAU 是一個 App 產業常用的指標，用來衡量使用者利用自家 App 的程度。 DAU：每天活躍人數（Daily Active Users） MAU：每月活躍人數（Monthly Active Users） DAU / MAU 則是這兩者的比例。可以想像當此比例越高，代表在每月活躍的使用者人數（MAU）中，每天活躍的人數（DAU）越高，可以說明使用者的黏著度越高。 但這篇重點在於說明不同服務、產品因為本身性質的不同，並不都適合用這個指標。像是 Airbnb 這種公司，有些使用者每年可能只使用一次（活躍次數一年才一次），但一次的消費金額很驚人。以 DAU 的角度來看這種顧客的話價值不高，但使用者的生涯價值（Life Time Value）卻很高。 雖然業界很常使用，不盲目使用 DAU / MAU 這個指標，而是依照自己的產品種類，選擇最能代表使用者價值的指標，並將其最大化才是上策。 結語 呼！以上就是這週的文摘內容了！儘管我們在這篇文摘裡頭只包含了 5 篇文章， 3 篇還是系列文，你應該也能感受到不同領域的知識在腦海中互相激盪吧！ 在整理這幾篇文章的重點時我學到不少，希望你也一樣。之後會定期更新，可以隨時回來看看有沒有新文章。如果懶得每天打卡，但希望在新文摘出來的時候馬上收到通知的話，可以點擊下面的訂閱。如果你在閱讀完後有其他感想，也歡迎跟我分享：） Stay tuned and happy data science!","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/data-science-digest-volume-1.html"},{"title":"資料科學家 L 的奇幻旅程(1)：新人不得不問的 2 個問題","text":"身為一個資料科學家，我平常會寫些相關領域的文章，像是 揭開資料科學的神秘面紗 為何資料科學家需要學習 SQL 淺談資料視覺化以及 ggplot2 實踐 。 它們都獲得不錯的迴響，我也得到不少很棒的回饋。 不過如果只是介紹特定跟資料科學（Data Science，以下簡稱 DS）相關的工具或概念的話，我們可能會陷入「見樹不見林」的窘境：知道很多 DS 的知識，但卻不曉得這些知識是如何實際被運用在解決人們或是企業的問題。實際上我相信大多數企業的資料科學家在做的事情，並不像很多線上課程那麼單純；有時候你需要結合多種領域的知識，如資料工程、分析手法以及領域知識（Domain Knowledge）來解決一個商業問題。 為了讓有志成為資料科學家，或是單純想要了解的讀者們能理解 DS 是如何實際被（企業）應用，以及讓自己多一點反思的機會，趁著最近開始在 SmartNews 的新工作，我打算開始（不定期地）紀錄自己平常的工作內容以及一些經驗分享（當然，在不洩漏隱私資訊的前提下）。 我相信透過寫作，能讓更多人了解資料科學並幫助自己釐清重要概念 這系列文章將以類似說故事（奇幻旅程，喔耶！）的手法，闡述我在 SmartNews 遇到的一些挑戰，以及作為一名資料科學家（Data Scientist），我如何利用手邊各式各樣的工具以及手法來解決這些問題。透過問題導向（Problem-oriented）的方式，我希望能讓更多人理解 DS 是如何實際被應用在企業之中，進而思考自己該如何預先準備，減少進入這個領域的障礙。（歡迎分享你的想法！） 在後面幾篇文章，我們將有機會深入探討一些分析手法、如何建置預測模型，以及建置可靠的資料流（Dataflow）。但在那之前可別忘了：「巧婦難為無米之炊」。我們才剛剛開始資料科學家的工作，就連筆電裡頭也是什麼軟體都還沒被安裝呢！ 因此在大展身手之前，在這篇文章我們將討論作為一個資料科學家，如何在開始第一個分析專案的同時，「有效率」地熟悉新環境。後面你將會發現，這個初始步驟看似瑣碎，卻能讓之後的工作進行地更為順利。 熟悉環境 = 安裝軟體？ 讓我們想像一下剛從 IT 管理部門手上拿到新筆電的情境。 通常拿到公司配的新電腦以後，一個資料科學家會思考的幾個問題是： 「我要在新電腦上面裝什麼軟體？」 「公司的資料科學家們用什麼軟體？」 「我要怎麼存取公司的資料？」 面對一片空白的環境，我們的腦袋可不能也是一片空白 這些問體的確很重要也很實際（practical），也是我當初能馬上想到的問題。但後面我們會看到，該安裝什麼軟體、該怎麼存取資料庫都是「熟悉環境」裏頭最簡單的部分。為什麼？ 因為通常 manager 會準備好一個清單告訴你該裝什麼，只要照著做就好了。這個清單當然會依照公司內部使用的技術而有所差異，但在大 Google 搜尋時代之下，要在自己的筆電安裝任何東西（應該）都不是太困難的事情。 就算公司沒有給你清單，沒問題！事實上，我也不過就安裝了以下軟體： Python & Anaconda R 語言 & RStudio Jupyter Notebook Docker iTerm2 PyCharm SourceTree ... 當然隨著專案的增加，你可能還會需要其他工具，但基本上沒有想像中的那麼多。有了開發/分析工具以後， IT 管理部門也會跟你說明該如何透過加密的方式，存取一些重要的資料庫以及伺服器。等到這些都搞定以後，理論上我們已經可以準備寫落落長的 SQL 查詢來結合多個資料庫的表格，並使用各種酷炫的 Python packages 進行分析了！ 不過在進行分析的同時，有一些問題值得我們花幾天慢慢地思考。這篇我想特別強調 2 個： 公司內有什麼 關鍵績效指標（KPI） ？ 這些 KPI 是怎麼被產生並顯示在儀表板（Dashboard）上的？ 你可能覺得這些事情看起來並不直接跟資料分析相關，但接下來你會看到，為何在進入公司早期就理解它們，對一個資料科學家來說很重要。首先讓我們看看第一個問題。 公司內有什麼關鍵績效指標？ 為什麼了解公司內有什麼關鍵績效指標（Key Performance Indicator, 後簡稱 KPI）很重要？ 因為這些 KPI 代表著一企業或團隊衡量成功的方式，同時也決定了資料科學家們將要努力的方向。沒有這些 KPI，我們將不能評估我們是不是走在對的路上，也不知道前進的速度。講得浮誇點，一個資料科學家能提供的最大價值就是： 分析數據、從中找出洞見讓企業做出更好的決策，以在最短的時間內最大化 KPI 以這樣的角度來看，KPI 的概念就跟機器學習中的 目標函數（Objective Function） 的概念相同，差別只在於我們是用電腦去最佳化目標函數；用人腦去最佳化企業的 KPI。 資料科學家的工作說穿了，就是如何利用資料以及 DS 的力量，來最大化儀表板上的 KPI 因為 SmartNews 是一個新聞 APP，讓我們舉些 手機 APP 產業中常被使用的 KPI 為例： 安裝次數（#Installs） 每人平均使用時間（Session Time） 瀏覽頁面數（#Page Views） 每天活躍人數（#Daily Active Users） 每月活躍人數（#Monthly Active Users） 重度使用者人數（#Heavy Users） 每日廣告營收（Ad Revenue Per Day） 儘管相同產業可能用類似的 KPI，每家公司給的實際定義（Definition）可能有所出入。不同公司之間的定義有差異是正常的，但該定義合不合理就是另外一回事了。 就跟我們訓練一個機器學習模型的時候會注意目標函數的定義是否合理一樣，在了解有什麼 KPI 以後，我們也應該積極地去詢問相關人員，了解這些 KPI 的定義是否合理。像是： 怎樣的行為可以算是完成安裝？是使用者第一次打開 APP 的瞬間算安裝，還是完成新手教學的時候呢？ 何謂活躍？使用者要做什麼操作才算活躍？打開 APP，更改設定就關掉也算活躍嗎？ 何謂重度使用者？過去一個月使用超過 7 天的人算嗎？ 把握 onboarding 的機會，詢問所有你能質疑的問題 如果 KPI 的定義不合理，糟一點的結果就是你的努力方向對了， KPI 卻沒有上升；更糟的結果則是你往錯的方向最佳化：錯誤的 KPI 提升了，你則沾沾自喜。儘管定義合適的 KPI 需要大量的領域知識，在剛開工的時候，你仍應該對現有的 KPI 做出適當的質疑，嘗試理解它們的合理性。 現在假設 KPI 的定義沒有明顯問題，不管什麼公司都會希望能將這些 KPI 即時地顯示在儀表板上以方便監控自己的營運狀況。但如果一個 APP 的 下載次數超過 2500 萬 ，每天產生上億筆使用者存取紀錄的話，幾個衍生出來的問題就是：KPI 該怎麼從這些原始資料產生出來的？如何保證中間沒有出錯？我們能信賴這些計算出來的值嗎？ 讓我們在下小節討論這個問題。 儀表板上的 KPI 是怎麼產生的？ 實作方式會依照公司有所不同，但讓我們以 SmartNews 為例。 我們的儀表板是使用 CHARTIO ，但基本上 CHARTIO 這種儀表板服務也只是一個 Web UI，它並不會自動幫我們把使用者的存取紀錄轉成 KPI。為了理解每天人們使用 APP 的情況，我們必須自己將所有網路伺服器（Web Servers）上的使用者存取紀錄（Log Data）做一系列的處理以後，轉變成儀表板上的 KPI。 而一個使用者的使用行為大致上會經過以下幾個步驟轉變成 KPI： 使用者打開 SmartNews App，手機客戶端向網路伺服器做出請求（Request） 網路伺服器回傳結果，並將該請求紀錄存在自己的硬碟上 fluentd 搜集所有伺服器上的請求資料，將它們存到 Amazon S3 上 工作流管理系統 Airflow 進行 Batch 處理，定期將被存到 S3 上的使用者存取紀錄轉成 Apache Hive SQL 表格（Tables） CHARTIO 透過分散式 SQL 查詢引擎 Presto 對該表格作查詢，顯示 KPI 從左到右來表示這個流程的話會如下圖： SmartNews 的資料平台 ：將大量原始日誌資料轉成儀表板上有用的 KPI（當然不只用在顯示 KPI） 乍看之下，你可能會想： 「這看起來跟資料科學完全沒相關啊！」 「我只要能存取關聯式資料庫（Relational Database）裡頭的表格不就好了？」 沒錯，嚴格來說這是一個資料工程（Data Engineering）的問題。但正如我們在 資料科學家為何需要了解資料工程 一文裡頭提到的，身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 事實上，了解儀表板上的 KPI 是怎麼產生的，有以下幾個優點： 理解工程師的痛點。能事先以他們的角度思考建立新表格所需的成本的話，他們會更願意幫你建立 通常新的分析會需要新的 ETL，而你可以利用跟產生 KPI 一樣的 ETL 來產生自己的資料管道（Data Pipeline） 確保資料品質。一旦使用的資料有瑕痴，做出來的分析也不會有意義。 一個資料科學家會去了解企業內的資料是怎麼流動的，確保資料的品質並建立自己需要的資料流 最後一點尤其重要。在 SmartNews 的例子裡頭，資料科學家實際上想要分析的是「APP 使用者的存取行為」，而跟使用者行為最直接相關的其實是那些被存在網路伺服器上的 log。只是因為該資料量太大，我們必須建立資料管道做前處理，從大量原始資料中萃取、匯總出我們「可能」有興趣的資料存入關聯式資料庫供之後分析。 以這種角度來看的話，資料彷彿是從網路伺服器（上游）經過一連串的河道（資料管道）流向資料庫（下游）。這也就暗示著兩個可能的風險： 資料在經過河道的時候被污染，資料品質下降 資料在經過河道的時候被限縮，有些有價值的資料沒辦法抵達下游 一個資料科學家如果只專注在下游的資料，就可能冒著以上的風險而不自知。這就是為什麼我們需要了解企業內的資料是如何流動的。 資料的流動當然不限於 KPI 的產生，但我認為用這個問題： 「儀表板上的 KPI 是怎麼產生的？」 來理解一個企業的資料流是一個很好的起始點。畢竟 KPI 是公司最重視的資訊，用來建構其的資料管道也會是最完善且重要的。 結語 在這篇文章裏頭，我們討論了一個資料科學家在進入新公司熟悉環境的時候，除了問該裝什麼工具以外，可以問的兩個問題： 公司內有什麼 KPI？ 儀表板上的 KPI 是怎麼產生的？ 表面上看來是兩個再簡單不過的問題，實際上第一個問題跟業界的領域知識（Domain Knowledge）息息相關；第二個問題則牽涉到大量的資料工程專業。而透過深刻地思考這兩個問題並詢問相關人員，一個資料科學家可以更全面的理解企業並掌握大局觀，做出最有影響力的分析。 當然，除了這兩個問題以外，你還需要問很多其他重要的問題如： 公司的資料文化如何？ 我在 Data Science 團隊裡頭的定位為何？ many more .. 但作為「資料科學家 L 的奇幻旅程」系列文的第一篇文章，為了避免累贅，我把這些問題留給你們（可以留言跟我說你覺得還有什麼問題重要！） 最後的 Bonus 問題：為何是資料科學家「L」？","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/journey-of-data-scientist-L-part-1-two-must-ask-questions-when-on-board.html"},{"title":"從彼此學習 - 淺談機器學習以及人類學習","text":"說到近年最熱門的機器學習（Machine Learning）或者人工智慧（Artificial Intelligence），因為知識背景以及觀點的不同，幾乎每個人都有不一樣的見解。雖然我們有千百種定義、無數的專業術語，這篇文章希望用直觀的方式以及具體的例子，讓讀者能夠在跳入一大堆 ML 的教學文章以及線上課程之前，能以一個更高層次且人性化的角度理解機器學習，並進而思考要如何開啟自己的機器學習旅程。 不僅如此，你將發現機器學習並不是冷冰冰的科學，隨處可見人類的巧思；就算不是資料科學家，你也能從『機器學習』獲得啟發，將一些概念用在改善『自己的學習』。 讓我們開始吧！ 目錄 何謂機器學習 機器學習實例：智慧咖啡機 如何讓機器學得更好 如何改善我們的學習 結語 何謂機器學習 多虧了媒體的大量宣傳，我們現在都知道 機器學習 被應用在各個領域。一些常見的例子包含： 自然語言處理，如 Google 翻譯、iPhone 的 Siri 語音辨識 推薦系統，如 Amazon 的 『買了這個商品的人同時也購買了 ...』功能 垃圾郵件自動判定，如同我們在 《直觀理解貝氏定理及其應用》 一文中談到的 電腦視覺，如 Facebook 的人臉辨識 、Youtube 的影片推薦，影像分類 〈這張照片是貓還是狗？〉 等 例子不勝枚舉。有那麼多應用機器學習的例子，不禁讓人思考，究竟什麼是『機器學習』？ 依照目前機器學習的應用，一個大致上的定義是： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 有了這個定義，讓我們再看一下上面提到的幾個例子： 自然語言處理：將得到的英文字串〈輸入〉，轉成中文文字〈輸出〉 推薦系統：將使用者過去的購買記錄〈輸入〉，轉成使用者可能想要購買的商品列表〈輸出〉 垃圾郵件判斷：將郵件內文〈輸入〉，轉成該郵件為垃圾信的機率〈輸出〉 電腦視覺：將一個 400 x 400 像素的圖片，轉成多個標籤的機率〈輸出〉 Google Machine Learning Practica : Google 教你做影像分類，利用機器學習，將充滿著像素的圖片轉換成一個個標籤 嗯嗯，我想這定義還算合理。 眼尖的讀者會發現，這邊的例子說明了上述定義的一半：將輸入 X 轉換成輸出 y。為了進一步解釋後半段『衡量成功的方式』，下面讓我們以一個虛構的咖啡機舉例。 機器學習實例：智慧咖啡機 假設你是個咖啡愛好者，家裡有好幾台高檔的咖啡機，但每次泡出來的咖啡都不合你胃口。 經過無數的失敗，忍無可忍，你最後決定向月巴克公司買台『智慧』咖啡機。該咖啡機宣稱可以了解你的個人需求，泡出世界上最符合你胃口的咖啡。 拆開咖啡機包裝，你興奮地把咖啡豆、砂糖以及牛奶加到該咖啡機裡頭。幾分鐘過後，號稱世界上最好喝的咖啡完成了！ 外觀看起來不錯，你滿懷期待地啜了一口。 『太甜了吧！砂糖太多了，什麼鳥機器！』 你怒吼著，幾乎馬上萌生退貨的想法。這時候咖啡機感應到你的抱怨，用很委屈的聲調說： 『目前調配咖啡的方式為原廠設定。經過統計分析，要得到一個正常台灣人的最佳評分，平均一杯咖啡裡頭的咖啡豆顆數、砂糖匙數以及牛奶的小杯數的比例應該要是 10 比 2 比 3。』 你只覺得莫名其妙，心想這什麼神奇的比例。而且咖啡機剛剛是在拐彎抹角地說我不正常嗎？ 這時候咖啡機又說話了： 『為了做出最符合您口味的咖啡，滿分 100 的情況下，請按鈕輸入你認為此杯咖啡值幾分。另外請告訴我是哪邊出了問題，如糖份比例太高還是牛奶太多，以讓我能記住您的喜好。』 你翻了個白眼，喝杯咖啡還要教機器怎麼調配？哪裡智慧了？ 但為了喝到最符合自己喜好的咖啡，你決定給咖啡機一個機會，好好地調教它。針對眼前這杯咖啡，你把自己的回饋〈評分、調配比例的建議：砂糖太多〉老老實實地輸入進去。 於是乎就這樣，不知不覺中你已經與月巴克咖啡機一起踏上了調配世界上最好喝咖啡的學習之旅。 為了實際了解咖啡機怎麼學習，你翻開咖啡機使用手冊，看到以下內容： 《月巴克智慧咖啡機說明指南》 基本假設；使用者評分 = 使用者滿意程度 預測使用者給咖啡的評分 y' = w1 * 咖啡豆顆數 + w2 * 砂糖匙數 + w3 * 牛奶小杯數 + 基本分 b 目標：找出一組調配比重 w1, w2, w3, b ，使得咖啡機預測的評分 y' 越接近實際的使用者評分 y 越好 咖啡製作：使用上述比重調配咖啡，使得預測評分 y' 接近 100 各原料調配比重〈原廠設定〉： w1 = 10, w2 = 2, w3 = 3, b = 60 ... 你恍然大悟，原來月巴克公司為了讓咖啡機最大化你給咖啡的評分，在咖啡機裡頭建構了一個簡單的 線性回歸〈Linear Regression〉 模型。 在這模型裡頭，使用者針對一杯咖啡的評分 y 會受到多個原料的量的影響。每個原料量的影響程度則透過個別的 w 來描述。理想上，如果咖啡機可以找出一組比重〈weights〉 w ，使得咖啡機『預測』出來的評分 y' 跟『實際』使用者給的評分 y 非常相近的話，咖啡機就可以利用該模型來合理地選擇咖啡豆、砂糖以及牛奶的量，調配出一杯預期能獲得你最高評分的咖啡。 那咖啡機要如何實際『學習』呢？ 或者換句話說，咖啡機要怎麼樣知道它現在用的參數〈 w1 、 b 等〉夠不夠好呢？如果不夠好的話，要怎麼修正呢？ 在一開始完全沒有任何使用者回饋的時候，咖啡機可以很合理地使用原廠設定來計算使用者評分 y' 。等到你輸入了一些評分 y 以後，將所有從你得到的評分 y 跟咖啡機自己預測的 y' 做比較，看咖啡機做的預測評分跟你的給分差了多少，據此修正原料的比重 w 以及 b 。 y' 跟 y 的差異讓我們暫時稱作 diff_y 。 修正以後，一般來說我們會得到新的比重 w' 以及 b' 。當咖啡機使用 w' 以及 b' 產生新的預測評分 y'' ，其跟你的實際給分 y 也會有一個差距，我們則將其稱作為 diff_y' 當使用新的參數〈 w1 、 b 等〉產生的 diff_y' 比原來的 diff_y 來小的時候，我們就能很開心地表示：『這咖啡機幹得真不錯！學到了點東西，能更準確地找出我的喜好！』。 而在每次獲得你回饋的時候重複上述步驟，咖啡機不斷地修正它用來預估你給咖啡分數的參數，讓預測出來的值 y' 跟你過去所有評分 y 之間的差異都更小。雖然我們這邊不會細講，但在線性回歸裡頭，一個常被用來計算預測值 y' 跟實際值 y 差異的方式是 最小平方法〈Least Squares〉 ： diff_y = 針對每次使用者的評分 y，機器利用當下的參數產生相對應的 y' 以後，用兩者計算 (y' - y) 的平方並加總它們 你可以看到，當 diff_y 越小，代表咖啡機越能準確地依據目前的原料量，來預測你會給咖啡的評分。 咖啡機學得很快。經過幾個怒吼以及失望的夜晚，透過你給的回饋，它現在做出的咖啡已經能很穩定地讓你給出 90 分以上的評價。 透過詢問咖啡機，你現在知道，為了獲得你的高評價，咖啡機學到了以下的模型： 你給咖啡的評分 = 咖啡豆顆數 * 13 + 砂糖匙數 * 1.2 + 牛奶小杯數 * 1.5 + 基本分 40 分 這跟一開始為了滿足所有人的原廠設定相比，還差真不少： 一般使用者評分 = 咖啡豆顆數 * 10 + 砂糖匙數 * 2 + 牛奶小杯數 * 3 + 基本分 60 分 依照你過去的回〈ㄊㄧㄠˊ〉饋〈ㄐㄧㄠˋ〉，咖啡機發現跟一般人相比，咖啡豆量對你來說，是一杯咖啡好不好喝的重要指標〈13 vs 10〉，砂糖跟牛奶量則反而顯得沒那麼重要。而從基本分來看，咖啡機甚至學到你對咖啡的要求程度比一般人要來得嚴格〈40 vs 60〉，實實在在地說明機器了解你是個專業的咖啡愛好者。 現在再讓我們看一次前面定義的機器學習： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 經過上面的咖啡機例子，我們能清楚地歸納出以下幾點： 咖啡機是在進行機器學習，學習如何用一連串運算，將原物料的量〈咖啡豆顆數等〉 X 轉換成使用者評分 y 機器學習裡所謂的一系列運算，在咖啡機的例子裡是進行線性回歸，即 y = w * x + b 咖啡機衡量成功的方式是計算『預測評分 y' 跟實際評分 y 之間的差異大小』，此差異越小，代表學得越好 衡量成功的方法很重要，因為咖啡機可以知道『努力/學習的方向』 咖啡機透過反覆地修正參數，進而最小化上述差異，成功地『學習』 機器學習是學習一組最符合目標的『參數』〈如基本分的 40 、咖啡豆顆數的 13 〉 我們可以總結說，咖啡機在你給的回饋以及監督之下，想辦法從三種原料〈咖啡豆、砂糖、牛奶〉中，『學習』出一個最棒的調配比例，以做出一杯能得到你最高評價的咖啡。在機器學習領域裡頭，這實際上被稱作 監督式學習〈Supervised Learning〉 。 太棒了，你跟月巴克咖啡機從此過著幸福美滿的日子‧ 如何讓機器學得更好 如果你閱讀完上面例子，開始思考以下問題： 『除了原物料的量以外，或許還可以搜集其他類型的資料，像是咖啡機主人的性別、年齡甚至泡咖啡的時間，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『與其預測使用者評分，能不能建立新的模型，直接預測使用者喜好？』 『如果咖啡機得到更多我的回饋資料，是不是會更準？』 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我得說聲恭喜，你已經擁有機器學習的思維且準備好進入機器學習的殿堂了。 但在你摩拳擦掌，準備進入殿堂時，有些人可能會跟你說，近年因為機器學習在各領域發展神速，且機器能使用的訓練資料〈Training Data〉也越來越多， 強人工智慧〈Strong AI〉 很快就會出現。不久的未來，我們甚至也不用自己設計演算法以及模型，A.I.會自動幫我們全部做好。也就是：機器會自己讓機器學得更好。 當強人工智慧出現以後，或許人類就不再被需要了。因為機器會自己讓機器學得更好。 真的嗎？沒有人能真正的預測未來，所以我們無從知曉。 但至少在接下來幾年，要讓機器學習或者人工智慧再繼續進步，『人類的思考』是不可或缺的重要因素。主要體現在兩個地方： 機器並沒有意識判斷『為什麼』以及何謂『對的方向』 機器的世界觀是人類教的 機器並沒有意識判斷『為什麼』以及何謂『正確』 電腦因為有著強大的記憶以及運算能力，在很多任務上面都已經可以超越人類的表現。 ILSVRC 歷屆的深度學習模型 : 近年電腦視覺〈Computer Vision〉領域發展快速，機器學習在影像分類〈Image Classification〉的表現已經超越人眼。 但能達到這樣的成果的前提，都是因為有人類在設計模型、監督機器學習。 目前機器學習或是 A.I. 的應用其背後的模型，當你去看裡頭一行行的程式碼的時候，裡頭並不會定義『為什麼』要做這些任務。實際上，在機器學習的過程中，機器並沒有意識到為什麼要做這些任務；而如果沒有人類的介入的話，機器也不會自己定義什麼樣的結果叫做『成功』或『正確』，而也就不知道該往什麼方向學習。 該讓機器學習什麼 怎麼定義『正確/成功』，讓機器遵從並往該方向改善 這兩件事情只有依靠人類來做決定。而其決定將大大地影響機器學習的成果以及品質。機器不會跟你說： 『我覺得把影片裡面的貓咪識別出來，比識別出交通號誌燈來得重要。』 『喔... 我覺得我們學的方向怪怪的，讓我們往這個方向學習如何？』 一個定義出錯的目標函式〈Objective Function〉將永遠無法讓機器學出我們想要的結果。 針對這點，我們應該： 找出值得解決的問題，下定我們的目標並明確定義何謂『正確』，以讓機器往該方向學習。 機器的世界觀是人類教的 第二點應該也不難理解。一個機器的世界觀基本上取決於兩點： 人類指定使用的模型〈Model〉 餵給它的資料〈Data〉 如同前面咖啡機的線性回歸，我們透過一個簡單的線性模型，教會咖啡機看世界。在咖啡機所認知的世界裡頭，使用者的評分就只會受到三種原物料量的影響：咖啡豆、砂糖及牛奶。這是一個非常簡單的世界，方便我們理解機器學習，但在真實世界上基本上不會成功運作，你需要考慮更多因素。 如同我們前面有提到，你可能會思考以下問題： 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『除了原物料的量以外，或許還可以搜集其他類型的資料，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我們怎麼看世界，將直接影響機器怎麼看世界。 事實上你已經在思考如何擴充機器的世界觀了。你可以使用各式各樣的模型、更多的資料以讓機器能用更全面的方式來理解這個世界。而這個新的世界觀只能由你來定義。 〈現在的〉機器不會突然跟你說： 『嗯... 我覺得我們應該考慮泡咖啡時有沒有下雨，因為這可能會嚴重地影響使用者心情，進而影響評分。』 『我只依照你的評分做最佳化，可能會有 過適 問題喔！』 這些問題都是我們必須自己發現並解決，不能只期待機器自動解決〈至少這幾年〉。在機器學習領域裡頭，最怕的不是模型完全不行，而是上述的 過適〈Overfitting〉 問題：機器所看到的資料本身太過侷限，導致其雖然只看到真實世界的一小部分，就誤以為那是全世界。換句話說，機器裡存在著強烈的偏見〈bias〉。前陣子常聽到的案例是 白人設計出來的臉部辨識模型對黑人有偏見 。 以我們咖啡機的例子來說，如果你家裡只有你一人，咖啡機只需要服務你一人即好；但如果你們是一個家庭，家裡的人都希望咖啡機能為它們弄出好喝的咖啡，則每個人都需要給予咖啡機回饋，以讓咖啡機了解每個人喜好。如果仍然只有你一個人給予咖啡機回饋，其他人不給分，則咖啡機會以為得到的評分來自所有人，誤以為只要最佳化這些評分，就能滿足所有人，其實不然。 為了讓機器看得更遠更全面，我們應該： 想辦法在機器學習的模型內融入更多我們的直觀想法〈intuition〉，並讓機器看到更全面的資料，以拓展機器的『世界觀』。 如何改善我們的學習 閱讀到此，相信你對機器學習已經有個高層次的理解了。 在對機器學習有個基本的了解以後，我們在前面章節提到為了讓機器學得更好，一個可行的方向是將我們的直觀想法、世界觀轉換成機器可以運算的模型或是目標函式，以讓機器能從聰明的我們身上學習。但換個角度思考，在我們教機器『學習』的時候，應該也能從機器『學習』到什麼才對。 事實上，很多我們應用在機器學習領域的想法，緊密地跟我們的個人生活息息相關。 舉個例子，在機器學習中，過適〈Overfitting〉是我們最想要避免的問題。我們不會希望機器只學到事物的表象，或者受到 outliers 的影響，而是希望機器學到更重要的模式〈Pattern〉、趨勢〈Trend〉。所以研究者們透過各種方式來讓機器不要過適： 輸入更多資料 用更簡單的模型 減輕 outliers 的比重 正規化〈Normalization〉 ... 而當機器成功地學到了事物的本質，就能精準地預測未來並且概括所有情況〈Generalize〉： 預測股票漲幅 預測誰最後會當上總統 預測詐騙交易 預測一張照片裡頭有什麼物件 畢竟，一個只看過貓跟狗照片的機器，不管未來看到什麼，就算是汽車或是人類，也只會將視它們為一定程度的貓或者是狗。 知名心理學家 馬斯洛 曾 說過 ： 如果你只有一個槌子，你可能就會把每個問題都視為釘子。 不管學習的是機器還是人類，學會概括〈Generalize〉並避免過適〈Overfitting〉是最重要的課題。 手中只有槌子的人，什麼問題都看起來像釘子。 同樣道理可以應用在人類的學習上。 當我們只注重在參加各式各樣的線上深度學習〈Deep Learning〉課程，而不去了解機器學習背後的原理就是一種過適；當我們掙扎著要用 Python 還是 R 畫漂亮的圖，而不去理解為何要這樣畫，才能讓觀眾更容易理解時也是一種過適。更不用說一個只了解 決策樹〈Decision Tree〉 的同學，看到什麼問題都會想要用決策樹來解的案例了。 學習表象比較簡單沒錯，但不能帶你走很遠。了解趨勢或者模式則讓你看到未來： 卓越的歷史學家忽視單一歷史事件，透過了解世界整體的歷史脈絡來預測未來 愛因斯坦觀察到世界的運作原理而推出有名的質能轉換公式 E = mc² 好的學習方式是理解事物背後的運作的趨勢、模式。為何我們要機器學習？為什麼深度學習會崛起？注重在詢問更多的『為什麼』以理解事物本質。 從一些已經被應用在機器學習的概念獲得啟發，我們可以重新思考並改善我們人類自己的學習。 結語 我們在這篇文章前半段以一個虛構的智慧咖啡機為例，深入探討機器學習的一些基本但十分的重要概念以及運作方式。 在掌握機器學習的基本概念以後，我們討論了如何以『人』為本，融入我們人類的智慧以讓機器更聰明地學習、了解這個世界。接著，我們用了一點篇幅，討論了看似不相關的『人類學習』以及『機器學習』之間一個共同且最重要的核心目標：『學習如何去概括〈Generalize〉事物並避免過適〈Overfitting〉』。 現在機器學習〈尤其是深度學習〉跟其他學術領域如統計、電腦科學相比，是一個相對新的領域，大家都還在摸索階段。但正如當年新興的程式設計已經普遍被重視，甚至加入國高中教育一般，我想再過幾年，等機器學習更為成熟後，人們也會開始呼籲將『機器學習』領域的知識納入課綱，成為我們下一代的基本素養之一。 未來教育模式的改變： 或許『機器學習』會如同『程式設計』素養一般，成為下一代必備的基本知識素養之一 或許那就是本篇所提到的『從機器學習中學習』。 但在那時代到達之前，讓我們開心〈機器〉學習吧！","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/some-thought-on-learning-from-machine-learning.html"},{"title":"從經驗中學習 - 直觀理解貝氏定理及其應用","text":"貝氏定理（Bayes' theorem） 是機率論中，一個概念簡單卻非常強大的定理。有了機率論的存在，人們才能理性且合理地評估未知事物發生的可能性（例：今天的下雨機率有多少？我中樂透的可能性有多高？），並透過貝氏定理搭配經驗法則來不斷地改善目前的認知，協助我們做更好的決策。 英國數學家 哈羅德·傑弗里斯 甚至 說過 ： 貝氏定理之於機率論，就像是畢氏定理之於幾何學。 因為其簡單且強大的特性，被廣泛應用在醫療診斷以及機器學習等領域。網路並不缺貝氏定理的教學文章，但多數以 機率公式 出發，不夠直觀（至少以我個人來說），就算理解了也不易內化成自己的知識。 貝氏定理公式 : 喔喔喔感覺到數學的力量了嗎？ 因此這篇將利用生活上我們（或 人工智慧 ）常需要考慮的事情當作引子，如： 今天的下雨機率是多少？ 這封 email 是垃圾郵件的可能性有多高？ 醫生說我得癌症了，這可靠度有多高？（好吧，或許這沒那麼常發生） 來直觀地了解貝氏定理是怎麼被應用在各式各樣的地方。我們甚至可以效仿貝氏定理的精神，讓自己能更理性地評估未知並從經驗中學習。 廢話不多說，讓我們開始吧！ 今天會下雨嗎？ 在實際說明貝氏定理的公式把你嚇跑之前，讓我們先做個簡單的假想實驗來說明貝氏定理的精神。 假設大雄一早準備出門跟靜香見面，正在考慮要不要帶傘出門。 起床的時候他想： 「這地區不太會下雨，不需要帶傘吧！」 往窗外一看，大雄眉頭一皺，發現烏雲密佈。 「痾有烏雲，感覺下雨機率上升了，但好懶得帶傘 .. 先吃完早餐再說吧。」 走到廚房，發現餐桌上一大堆螞蟻在開趴。 「依據老媽的智慧，螞蟻往屋內跑代表 下雨機率又提升了 。真的不得不帶傘了嗎 .. 不不不！我不要帶好麻煩！」 想著想著，這時候靜香打電話過來了： 「胖虎說他也要去喔！」「蛤你說什麼！？」 胖虎是有名的雨男，每次跟他出遊都會下雨。依照這個經驗以及前面看到的幾個現象，最後大雄放棄掙扎，帶著雨傘出門了。 在上面的例子中，大雄觀察到三個現象： 烏雲密佈 螞蟻開趴 胖虎出沒 依據他過往的經驗，這些現象都會使得降雨的機率提升，讓他逐漸改變剛起床的時候「今天不太會下雨」的想法，最後決定帶傘出門。 這個決策的轉變過程，其實就是貝氏定理的精神： 針對眼前發生的現象以及獲得的新資訊，搭配過往經驗，來修正一開始的想法。 實際上，大雄已經在腦海中進行了多次貝氏定理的運算而不自知（我家大雄哪有那麼聰明）。現在讓我們用比較數學的方式來重現大雄腦海中的運算。 讓我們帶點數字進去 在了解貝氏定理的目的以後，讓我們以 發生比（odds） 的方式來闡述定理。發生比很簡單，就只是列出兩個（或以上）的事件分別（可能）發生的次數。 使用發生比的好處是可以很容易地比較不同事件發生的相對次數。後面會看到，我們也能把發生比轉成機率。 假設依據過往氣象紀錄，大雄住的地區一年 365 天中有 270 天放晴，下雨的天數為 365 - 270 = 95 天。則下雨的發生比為： 雨天數：晴天數 = 95：270 你可以把發生比想像成是一種相對關係，上面這個發生比代表，在大雄所住的地區，每觀測到 95 個雨天的日子，我們同時會觀測到 270 個晴天。晴天約是雨天的三倍之多（270 / 95）。 轉換成機率來看的話，就是把雨天的天數，去除以所有天數： 95 / (95 + 270) = 0.26 = 26% 一年也就只有 26% 的降雨機會，這也是為何大雄一開始在還沒觀察到新現象（烏雲、螞蟻及胖虎）的時候，合理認為今天「應該」不會下雨的原因。 我們再繼續假設，依據大雄的過往經驗，他發現： 雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍 這個 9 倍是怎麼來的呢？ 這其實是所謂的 概度比（likelihood ratio） 。分別計算雨天及晴天發生的情況下，出現「烏雲密佈」現象的機率以後，再將兩者相除： 雨天時烏雲密佈的機率 = P(烏雲|雨天） ------------------------------- 晴天時烏雲密佈的機率 = P(烏雲|晴天） 這兩個機率又被稱為 條件機率（conditional probability） 。一般 P(A|B) 代表在事件 B 發生的情況下，事件 A 發生的機率。 假設平均來看，在 10 個雨天裡頭，早上烏雲密佈的天數為 9 天（也就是說平均有 1 天的雨天是早上沒有烏雲密佈的），則我們可以說，「給定雨天的條件下，烏雲密佈」的機率是： P(烏雲|雨天） = 9 / 10 另外在 10 個晴天裡頭，早上烏雲密佈的天數平均為 1 天（也就是說早上雖然烏雲密佈，但最後並沒有下雨的天數），則「給定晴天的條件下，烏雲密佈」的機率是： P(烏雲|晴天） = 1 / 10 則烏雲密佈的概度比即為： P(烏雲|雨天） 9 / 10 ----------- = --------- = 9 P(烏雲|晴天） 1 / 10 雖然「概度比」一詞很饒舌，但它就是一個比例，也就是「幾分之幾」的概念。 9 就是「一分之九」＝ 9 倍，而因為分母是「晴天」，你可以解讀這個 9 為 「在烏雲密佈發生的情況下，每觀測到 1 個晴天，就會同時觀測到 9 個雨天」。 也可以像是大雄觀察到的： 「雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍」。 經驗告訴我們，早上烏雲密佈的情況下，該天是雨天的機率就隨著上升 貝氏定理初顯鋒芒 所以有了這個倍數可以做什麼？直覺及經驗告訴我們，在觀測到烏雲密佈的前提下，下雨機率理論上會有所提升。 換句話說，在烏雲密佈，且觀測到的晴天數不變的情況下，觀測到的雨天數應該要有所上升，這樣下雨的天數在所有天數裡頭的比例才會上升。 而其上升的倍數就是前面的概度比（ 9 倍）。因此在烏雲密佈發生的情況下，新的下雨發生比（odds）可以寫成： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 95 * 9 ： 270 = 855 ： 270 新的下雨發生比是我們利用觀測到的現象重新計算的，因此一般稱為「事後發生比」（posterior odds）；而一開始的發生比則被稱為「事前發生比」（prior odds）。 事後發生比告訴我們，在烏雲密佈的情況下，每觀測 855 個雨天，就會同時觀測到 270 個晴天。跟事前發生比相反，現在雨天數反而超過晴天的三倍（855 / 270）。 要計算新的下雨機率，我們一樣把雨天數去除以所有天數： 855 / (855 + 270) = 0.76 = 76% 跟一開始的 26% 相比，在觀測到烏雲密佈這個現象以後，下雨的機率足足上升了 50 個百分點，現在我們有更充分的理由請大雄帶把傘了。 實際上，透過上面的計算，我們已經套用貝氏定理的公式了（ 發生比版本 ）： 事後發生比 = 概度比 * 事前發生比 如同大雄的例子，一般應用貝氏定理的情境如下： 對一件未知事物有初步的猜測（事前發生比） 觀測到跟該事物相關的現象 利用先前跟該現象有關的經驗計算出概度比 利用概度比修正該猜測，得到修正後結果（事後發生比） 重新評估、做決策 又觀察到新現象，重複步驟 3 到 5 透過貝氏定理，我們可以很快速地利用過去的經驗改善自己的想法，並產生更好的決策。 大雄不死心：單純貝式 雖然觀察到了烏雲密佈，且利用過往經驗修正下雨的機率到了 76%，懶惰的大雄一開始還是不想帶傘出去。但為何最後還是帶傘出門了呢？那是因為除了烏雲密佈以外，他還觀察到了其他兩個影響下雨機率的現象： 螞蟻開趴 胖虎出沒 貝氏定理本身雖然強大，但其中一個使它被廣泛利用的是 單純貝式（Naive Bayes） 的概念：假設不同現象之間出現的機率為 獨立 )。 設成獨立有什麼好處？事情變得很簡單，我們不用考慮現象 A 跟現象 B 之間的關聯性，能針對每個現象，分別去計算概度比，修正從「前面」的現象得到的結果，持續改善我們的認知。也就是上一節提到的貝氏定理的應用步驟 6。 如法炮製，讓我們假設大雄針對其他兩個現象的經驗是： 雨天時，螞蟻出現在室內的天數是晴天的 2 倍 雨天時，胖虎出遊的次數是晴天的 3 倍 讓我們再次套用貝氏定理，但這次不是套用在一開始什麼都不知道的事前發生比： 雨天數：晴天數 = 95：270 而是在觀察到烏雲密佈後的事後發生比： 烏雲密佈下的雨天數：晴天數 = 855：270 首先，讓我們套用跟螞蟻相關的經驗： 雨天時螞蟻出現在室內的天數是晴天的 2 倍 概度比已經算好，所以依照貝氏定理的公式： 事後發生比 = 概度比 * 事前發生比 新的（螞蟻）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 855 * 2 ： 270 = 1710 ： 270 下雨的機率則提升為： 1710 / (1710 + 270) = 0.86 = 86% 比起只有烏雲密佈，在螞蟻也出現的情況下，降雨機率又提升了接近 10%。大雄是一個降雨機率不大於 90% 就不帶傘的傢伙，讓我們看看胖虎出沒能不能使他改變心意。 同樣，再次套用定理到上一個（螞蟻的）發生比，則新的（胖虎）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 1710 * 2 ： 270 = 3420 ： 270 在轉換成機率之前，我們發現新的雨天數是晴天數的 10 倍以上，因此可以想像新的機率至少是 90% 以上。而實際計算下雨的機率： 3420 / (3420 + 270) = 0.93 = 93% 在觀察到烏雲密佈、螞蟻以及胖虎出沒以後，大雄預估降雨機率上升至 93%，這下不得不帶傘出門了。 重複套用貝氏定理以修正想法的過程就像是在創作：把眼前所有所見（顏料）一個一個納入考量，做出最後的判斷（作品） 動動腦時間 到了這邊，我相信你現在應該已經可以在腦中直觀地運用貝氏定理：針對眼前發生的現象，運用過去相關的經驗（計算概度比），來理性地評估某事件可能發生的機率。 事實上在你繼續讀下去之前，我建議先停一停，思考幾個可以實際在生活中運用（或者已經在用）此定理的現象，以幫助你內化（internalize）這些概念。 如果你一時想不到點子，這邊提供幾個例子： email 內文裡頭出現「週年慶」時，該郵件為垃圾信的機率 新聞內文出現「柯文哲」時，文章主題為政治的機率 醫生說你得胰腺癌 時，你真的得病的機率 玩 英雄聯盟 時， KDA 超過 4 的對手排位是鑽石以上的機率 在東京藥妝店血拼，旁邊講中文的人是台灣人的機率 如果你有其他有趣的例子，歡迎留言跟大家分享。（現在留言不用登入了！） 如同上述，貝氏定理有非常多應用。不過這邊想深入探討第一個 email 的案例：給定一封電子郵件的內文，你要怎麼判斷該信是不是垃圾信件？ 從人腦到電腦：讓機器幫我們做判斷 一個典型的垃圾郵件內文 你說沒有什麼事情難得了我們人腦。依照過往經驗： 垃圾信件裡頭出現「週年慶」一詞的機會是一般信件的 20 倍 垃圾信件裡頭出現「折扣」一詞的機會是一般信件的 10 倍 在假設所有信件裡頭一半是垃圾信件（發生比 1：1）的前提下，依照單純貝氏的公式，這封信是垃圾信件的可能性上升 200 倍（20 * 10），我們可以放心把這封信丟入垃圾信分類。 但是沒有人會想要在腦中對每封信做這個運算。人類是懶惰的，能自動化的東西就請電腦幫我們解決就好了。 另外你也不可能記得每一個詞的倍數，實際上也沒有必要。只要讓電腦幫我們記住每個詞分別在垃圾郵件以及一般信件出現的次數，就能計算所有詞彙的概度比（odds）。 等到一封新的信件來以後，找出裡頭的字對應的倍數做相乘以後，電腦就能自動分類郵件了。事實上這就是 機器學習 中 單純貝氏分類器（Naive Bayes Classifier） 在做的事情。 讓機器取代人腦自動判斷，有幾個顯而易見的好處： 判斷速度倍增 記憶能力超強 （可以把分類郵件空出的時間拿去看貓咪影片） 唷呼！垃圾郵件自動變不見！小鎮村又變得更美好了。 當然你想自己實作單純貝氏分類器的話，Python 可以使用 scikit-learn 來實作。 小心！你的經驗可靠嗎？ 我們花了很長的篇幅講了幾個貝氏定理/單純貝氏的應用，也看到它既簡單又強大的特性。但在你摩拳擦掌並實際應用此定理的時候，有幾點需要注意： 不同現象/事件真的獨立嗎？ 一開始的猜測以及經驗可靠嗎？ 很多現象不一定是完全獨立而是相關的。不過一個常見的解決方法是想辦法增加更多的現象/事件/特徵值（features）來讓做出來的貝氏分類器比較可靠。貝氏定理當然不完美，但正如統計學家 喬治·E·P·博克斯 所說 ： 所有模型（models）都是錯的；但有些是有用的。 儘管「獨立」這個假設在某些情況下不合常理，但在如垃圾郵件分類等問題上，貝氏分類器有不錯的表現。而且重點是它實作簡單，可以拿來當作 baseline。 而一開始的猜測跟經驗可不可靠這個問題，英國數學家 卡爾·皮爾森 ，針對貝氏定理則給出一個我很愛的 名言 ： 一個信奉貝氏定理的人常常做這樣的事情：模糊地期待著馬的出現，瞥見驢子的蹤影，強烈地相信他是見到了一匹駝子。 「先入為主」大概是應用貝氏定理最忌諱的點了。下次再套用定理時，記得先思考自己一開始的假設以及經驗是否值得信任或者有什麼盲點。需不需要搜集更多資料來修正一開始的想法。 我真的是驢子不是駝子啊！（豆知識：駝是馬跟驢生下的動物） 總結 我們在這篇開頭首先用「大雄評估下雨」的例子來直觀地理解貝氏定理背後的精神，接著透過簡單的數學概念、發生比（odds）以及概度比（likelihood ratio）來推出基本的貝氏定理公式。 接著進一步延伸至單純貝氏（naive bayes）的概念，讓機器透過過去累積的資訊，為我們自動分類垃圾郵件。 最後我們提到一些應用貝氏定理需要注意的事情。 即使基本的貝氏定理不難，延伸的領域非常的廣。這篇沒辦法包含所有範圍，但希望透過這篇基礎介紹，能讓讀者能利用貝氏定理的概念，更理性地評估未知並從經驗中學習（或者是建立自己的貝氏分類器）。 另外如果你有其他有趣的例子可以應用在貝氏定理，歡迎留言跟大家分享。（現在留言不用登入了！）","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/intuitive-understandind-of-bayes-rules-and-learn-from-experience.html"},{"title":"揭開資料科學的神秘面紗","text":"幾乎每天我們都能看到跟資料科學（Data Science）相關的新聞與文章，像是最近 Google 利用遞迴神經網路建立可以跟真人對話而不被發現的語音助理 、 成為 Apple 等公司的資料科學家前必讀的面試題目 等等。 市面上有大量資料科學相關課程、書籍供我們自由學習，事實上，多到一個人不可能看完。你有想過為何我們需要學習資料科學嗎？為什麼資料科學現在那麼夯？我們應該拿資料科學來做什麼？ 抽離技術實作或者分析手法的討論，這篇文章試著用簡單的經濟學原理回答這幾個問題。 希望閱讀完本文的讀者能了解為何資料科學在資訊時代扮演重要角色，以及我們要怎麼有效率地把握「資料科學力」以創造更大的價值。 目錄 本文大致上會分成以下段落： 聽說你想當資料科學家？ 資料科學到底在夯什麼？ 啊所以那個資料科學勒？ 充實你的資料科學力 結語 讓我們開始吧！ 聽說你想當資料科學家？ 資料科學大概是近年最夯的流行語之一了。不管在哪邊，你都可以聽到媒體相關的報導： 食農教育科研成果農業大數據結合資料科學 想成為資料科學家？來挑戰 Google、FB、Apple 等六間公司人工智慧最新面試題 台灣產業AI化 最大問題人才不足 成為搶手資料科學家應具備什麼技能？先學Python準沒錯 ... 族繁不及備載。 而因為企業對擁有資料科學能力的人才需求大，想成為資料科學家（Data Scientist）的同學們也不少，相關的教學文章、線上課程如雨後春筍般湧現。這邊我沒辦法把它們一一列出，但你可以前往一些知名的線上課程平台如 Coursera 、 Udemy 、 DataCamp 並搜尋「資料科學」（或者 Data Science）就知道我的意思了。 DataCamp : 基本上全部都是資料科學相關課程，寫程式寫到飽 如果我們把這些新聞報導或者教學課程，依照主題/領域做個粗略分類的話，還可以得到一些關鍵字： 大數據（Big Data） 人工智慧（Artificial Intelligence） 資料視覺化（Data Visualization） 機器學習（Machine Learning） 深度學習（Deep Learning） 統計分析（Statistical Analytics） 雲端運算 （Cloud Computing） Python、R、SQL ... 資料科學 : 涵蓋大量領域，各領域的專業知識就像一棟棟大樓將你包圍吞噬 想學習資料科學的同學這時候就頭疼了： 「全部都要學嗎？從哪邊開始 .. 」 「選 Python 或是 R 語言 ？還是先學 SQL ？」 「資料視覺化要學 Python 的 Matplotlib 還是 R 的 ggplot2 ？」 現在有些網站很用心，為了解決你的煩惱，還將相關的課程集結起來成一個 專業課程（Specialization） 讓你一步一步跟著學。 勤學如你，上了幾門課以後學會如何利用 Python 做簡單的機器學習模型 、 使用 R 做資料視覺化 ，甚至也懂得 使用 SQL 存取資料庫 。 恭喜！你是個資料科學家了！ ... 痾.. 這麼簡單？好像哪裡怪怪的？ ... 你會不會開始思考： 所以到底啥是資料科學？資料科學到底在夯什麼？為什麼我要學資料科學？ 實際上會這樣想的不止你一人。在仔細思考並給上述問題一個合理的解釋之前，就算學了再多門課，充其量只是在不斷擴充自己的「資料科學工具盒」，但卻不知道「為何要買這些工具」、「要拿這些工具做什麼」。 資料科學工具箱 : 琳瑯滿目，酷！但你要用這些工具創造或是改善什麼？ 因為你學的是方便實踐資料科學的程式語言、工具、方法論（Methodology），而不是「為什麼資料科學重要」。我會用剩下的篇幅試著對此問題給出一套解釋。解釋方法有很多種，所以非常歡迎在底下留言分享你的看法。 不過現在，且聽我娓娓道來。 資料科學到底在夯什麼？ 除了耳熟能詳的「技術發展快速」、「資料量龐大」的理由以外，資料科學之所以那麼夯，背後還有一個可想而知的巨大推手：「商業利益」。 要進一步解釋這個概念，我們可以從 Google 首席經濟學家 哈爾·范里安 在 2009 年接受麥肯錫的訪問，探討 網際網路對企業的挑戰 中看出一些端倪。（真知灼見，建議作課外閱讀） 近年網際網路快速發展。要在網路上發表內容，對任何人或者任何企業來說都是輕而易舉的事情。這邊說的內容（Content）可以是任意資訊，比如説： 一則 Facebook 粉絲團貼文 一則銷售青島啤酒的網頁 一個教你學習資料科學的線上課程網頁 一篇部落格文章（像你正在看的這篇） 因為傳播媒介以及科技的進步，要在網路上發布這些資訊並讓他人注意到的成本趨近於零，而其導致的結果就是 全球的資訊量急速成長 。被稱為人工智慧之父之一的經濟學家 赫伯特·西蒙 針對這種現象就曾說過一句 名言 ： 在一個資訊豐富的世界裡頭，資訊量的富裕導致人們注意力的貧窮。 以個人的角度來看，在時間以及精力有限的情況下，我們每天能接受資訊的時間以及注意力都是有限的。如何分配這些寶貴的注意力以接收對的資訊，變成現代人的課題。 痛點即商機。很多企業透過解決這個 資料超載（Information Overload） 的問題來提供使用者價值： 漫畫網站把所有知名漫畫整理在一起供你閱讀 價值：統整、數位化、自動更新散落各地的漫畫資訊 Google 提供搜尋功能給你 價值：讓你快速找到存在地球上的任何相關資訊 Youtube 讓你免費看到飽 價值：讓你隨時看全世界最新的貓咪影片 只要喊「+1」Facebook 粉絲團就免費把「珍貴」的內容給你 價值：給你數位內容如新產品資訊、整理過後的旅遊資訊等 天下沒有白吃的午餐，企業願意這麼做必定有得到什麼。你的確取得了免費的數位內容（文章、影片、漫畫），但又付出了什麼？ 資訊時代最珍貴的資源 : 人們（與喵）的關注 實際上，不管是閱讀文章、觀看影片、瀏覽漫畫，你都是在拿了你最寶貴的「注意力」跟企業交換這些價值。而在成功獲得你目光的同時，這些企業則透過秀廣告給你來獲利（例 1 - 3，暫不考慮 AdBlock）。 註：在這邊，「注意力」跟「時間」有些微秒差異。不過你只要回想昨天晚上跟朋友或是家人吃飯的時候，各自滑手機的景象就可以了：你把「時間」花在跟身旁的人吃飯，卻把「注意力」（或者說是關注）放在手機裡頭的數位資訊。（如果你沒用手機，我很抱歉。） 例 4 很有趣，你是拿「你自己以及你朋友圈的人的注意力」來做價值交換（你的留言讓 Facebook 的演算法自動推播該貼文到你朋友的動態牆上，粉絲團賺到他們的關注），但基本上是同樣的道理。 資訊時代最常見的價值交換 : 給我你的關注，我就給你免費資訊（外加廣告） 以經濟學的角度來重述前面的觀點，現在的資訊時代最不缺的資源就是「資料」；稀有、價值高且需要小心分配的稀有財是「人們的注意力」。在這個資訊爆炸的時代，企業透過加工處理大量的原始資料，產生新產品、服務及價值來換取該稀有財： 誰能善用資料科學的力量、從現有數據創造新價值、服務或產品，並以此吸引人們珍貴的關注，就能獲得商機。 這就是為何資料科學那麼夯的其中一個原因：從資料中創造新價值，進而產生商業利益。 啊所以那個資料科學勒？ 聽到上面的例子，有些人的想法可能是： 「哇這些企業好狡猾把我的注意力都偷走了！」 「這樣回覆 +1 好有罪惡感喔嗚嗚」 「好險我用 AdBlock 嘻嘻」 但這邊重點是要說明，這種依靠廣告的商業模式已經行之多年。Facebook、Google 等企業為了抓住我們的目光，持續不斷地在精進，以求能有效率地儲存、處理以及分析由我們產生的大量數據。 而他們用來處理、分析、視覺化以及理解數據的這些程式語言、工具、方法論的總集合就構成所謂的資料科學。 搜集、理解、分析、處理、視覺化資料數據並從中萃取有用的價值就是資料科學。 讓我們以一個簡單的 Google 搜尋做更進一步的解釋。 想像你在 Google 上搜尋「 data science courses 」後可能跑出以下結果： Google 日常：搜尋結果之上有幾個相關廣告 沒什麼特別的，Google 日常不是嗎？ 現在試著做以下步驟： 開一個新的分頁/視窗 隨便搜尋一個你有興趣的商品/產品，記下出現的幾個廣告還有它們的順序。 隨便點幾個連結或者什麼都不做 重複步驟 2 跟 3 幾次以後，你應該可以觀察到顯示的廣告消失或者順序改變了：而這是因為背後有 Google 的廣告競價系統在運作。下面是這系統的超級簡化示意圖： Google 廣告競價：運用使用者的行為資料，即時地推算出該使用者點擊各廣告的機率。搭配業主的出價，選出適當的廣告顯示。 要完成此系統需要強大的資料科學技術支持。只有一個人搜尋的時候事情還好辦，但你得知道，在本文撰寫當下，Google 平均 1 秒鐘處理 67, 000 筆 搜尋。試著想像一下，為了實現這個系統，Google 可能需要完成以下幾件事情： 使用深度學習進行自然語言處理，判斷使用者輸入的語言以及想要表達什麼 即時處理所有使用者查詢的串流數據 利用使用者過往的瀏覽紀錄來預測點擊某廣告的機率 在公司內部監控目前台灣使用者的搜尋趨勢（類似 Google Trend ) 機器學習、統計分析、大數據 ... 這些工作運用到的技術，不就是那些我們在 聽說你想當資料科學家 章節裡頭看到的關鍵字嗎？ 我們這篇只以 Google 的廣告系統為例，但實際上現在幾乎可以說是全世界都在想辦法利用資料科學的力量來處理資料並創造新的價值、服務、公司。看看現在的新創，有哪些沒有用到資料科學？ 所以你現在知道為何資料科學那麼重要了。 全世界都在想辦法活用資料科學以從龐大數據中為使用者創造更多價值。 充實你的資料科學力 綜觀資料科學一詞萌芽到最近的過程，全世界的資料量 持續成長 ，而人們也不斷地在想辦法追趕這些資料： 用最有效率的方式儲存這些資料 用最快的速度處理及分析這些資料 對這些資料做實驗，重複再重複測試不同的假說及演算法 快速地從資料萃取出新的洞見（Insight） 以這些洞見創造新的價值、產品、服務 加速以上步驟所需要的循環時間 如同前面 Google 的例子，這些都是資料科學。 你會發現，所謂的資料科學（Data Science）就是對資料（Data）做科學、有系統地（Scientific）的處理罷了。資料科學一詞或許誕生沒多久，但對資料做科學這概念老早就存在了。只是近年因為 數據量的快速成長（如 物聯網裝置的火紅 ） 運算能力的進步 人工智慧的突破 等等原因，讓我們更急迫地想辦法用以往做不到的方式來理解這個世界的龐大數據。 Youtube 現在能夠分析出你喜歡看貓咪影片 ， Google 可以建立跟真人對話而不被發現的語音助理 。這些都是他們利用資料科學，從現有的大量數據創造額外價值的例子。如同 這篇 所說的： 未來是屬於那些能從大量資料數據創造價值的企業以及人才的。 一個好消息是： 一企業擁有的資料量 一企業裡能夠處理、分析此資料量的資料科學人才數量 這兩者在多數企業都是不成比例的（後者短缺），因此擁有資料科學能力的人才薪水可以說是水漲船高。而這當然也變成為何近年那麼多人想成為資料科學家的動機（儘管有些人可能不知道背後原因）。 了解資料科學相關知識的人才 : 是大多數的企業積極尋找的對象 在了解這點以後，你可以先想想自己的興趣在哪裡、想用資料科學創造什麼價值。這邊想強調的是，先思考你能透過資料科學，創造什麼新的「價值」，而不是什麼「商業利益」。 如同我們前面看到的，資料科學是現行廣告經濟的背後推手，但為何我們願意看 Google、Facebook 丟給我們的廣告？那是因為他們「先」從資料創造了價值（方便的搜尋功能、社群網路功能）從而取得我們的關注。 實際上，在取得關注以後，你的商業模式不是一定要秀廣告給使用者看。訂閱制（Subscription）或會員制是一個替代方案： NetFlix 和 Amazon 都是這樣。甚至，你可以 不像 Google 一樣思考 ，使用新的商業模型。 但「商業模式」不是這篇想討論的議題。重點是「價值」： 在資訊爆炸的時代，各行各業的每個人都需要學習善用資料科學，從資料數據創造新的使用者價值。 事實上，與其想著要成為一個資料科學家，不如先好好想想，在自己目前所在的業界、公司、職位能怎麼利用手邊的資料數據搭配資料科學來創造新的價值。 結語 如果你耐心地看到這邊，代表我得到你最珍貴的關注了，賺賺賺！ 稍微複習一下，我們在這篇文章開頭假想了一個有志學習資料科學的同學。在他/她學習資料科學的過程產生了幾個疑問：「為何資料科學那麼夯？」「為何我們需要資料科學？」 而本篇則以非常簡單的經濟學供給概念，加上 Google 以及 Facebook 的運作方式來說明現在的企業是怎麽利用資料科學來創造新的使用者價值來交換人們的關注。 我們接著說著為何今後各行各業都需要「資料科學力」來處理日益增加的資料數據並為人們建立新的價值。事實上很多職稱不是「資料科學家」的人現在都已經在做著資料科學： 搜集、理解、分析、處理、視覺化資料數據並從中萃取有用的價值 當年網際網路開始蓬勃發展，軟體工程師是最夯最潮的行業。儘管現在工程師的重要性並沒有下降，隨著人們的程式能力穩定上升，軟體工程師回歸平凡，甚至還有人戲稱為「碼農」、「程式猿」。 歷史總是不斷重演。 或許再過幾年，等人們的資料科學力上升到一定階段，資料科學變成呼吸喝水般的知識以後，資料科學家們也會被人戲稱為「資料農」。 或許當你幾年後遇到我，我可能這樣回你： 嘿！我就只是個資料農！你也是嗎？","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/demystify-the-hype-of-data-science-and-its-value.html"},{"title":"為何資料科學家需要學習 SQL","text":"這篇簡單討論 結構化查詢語言（SQL） 在概念上跟命令式程式語言如 Python 有什麼不同之處，以及在什麼樣的情況下我們會想要利用 SQL 做資料分析。 這篇注重在為何你會想要使用 SQL 做資料分析，而非 SQL 本身功能的教學。如果要學習 SQL 本身，可以參考最後面的 推薦閱讀 。 使用 SQL 與數據對話 身為資料科學家或者是分析人員，我們都知道 SQL 基本上是必備的分析工具。 簡單來說， SQL 是一種程式語言 ，我們可以透過它對被儲存在 關聯式資料庫 裡頭的資料進行查詢或操作。 SQL 是資料科學家與資料庫（Database）溝通的語言 在沒接觸過 SQL 之前，你可能會想 「做為一個程式語言，為何 SQL 有那麼多人在使用？ 」 「我們有 Python、R，不學 SQL 應該也沒關係吧？」 「又要學一個程式語言好麻煩。」 為了釐清這些疑問，讓我們做一個假想實驗。比方說我們現在想要知道某個特定顧客過去的所有購買記錄。 如果你熟悉 SQL 的話，可以對資料庫下一個簡單的查詢（Query）： SELECT c . name AS customer , o . totalprice , o . orderdate FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey WHERE c . name = 'Customer#000000001' ORDER BY o . orderdate ; 上面這個查詢翻為白話就是： 從顧客清單 customer 還有購賣紀錄 orders 裡頭 FROM customer AS c INNER JOIN orders AS o ON c.custkey = o.custkey 找出名為 Customer#000000001 的顧客的所有購買紀錄 WHERE c.name = 'Customer#000000001' 並把那些紀錄依照購買日期排序 ORDER BY o.orderdate 最後只回傳顧客名稱、總購買金額、購買日期幾個項目 SELECT c.name AS customer, o.totalprice, o.orderdate 這個查詢對第一次寫 SQL 的人可能會覺得很複雜，但注意，我們並沒有告訴資料庫「如何」取得這些資料，比方說： 怎麼合併顧客跟購買紀錄？ 怎麼過濾特定顧客？ 怎麼排序？ 我們只告訴它該給我們「什麼資料」。而得到的結果是： customer | totalprice | orderdate --------------------+------------+------------ Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 如同我們預期，只有該顧客的購買紀錄被回傳，且依照購買日期 orderdate 從早排到晚。 實際上，資料庫可能需要做以下運算來取得資料： 將顧客表格 customer 以及購買紀錄的表格 orders 分別命名為 c 及 o 依照共通的鍵值 custkey 合併（ JOIN ）兩表格 找出特定顧客 Customer#000000001 的購買記錄 將該紀錄依照購買日期 orderdate 排序 選出要顯示的欄位 這些運算最後都得依照「某個」順序執行，但是我們不需要考慮這些事情，完全依靠資料庫的 查詢最佳化器（Query Optimizer） 來幫我們決定。 寫 SQL 敘述時，你可以理解成我們是指定「要的資料」，而查詢最佳化器會依照此需求，找出一個最佳路徑來取得必要的資料。 SQL 查詢 : 專注在你的目標，查詢最佳化器會負責找到達成目標的最佳路徑 換句話說，當我們在寫 SQL 的時候，是在進行 宣告式程式設計（Declarative Programming） ：我們只告訴資料庫，我們想要什麼資料（What），而不是怎麼取得（How）它們。 這跟一般常見的 命令式程式語言（Imperative Programming） 如 Python、Java 有所不同。在寫 SQL 時，我們告訴資料庫它該達成的目標 - 取得什麼資料（What）；在寫 Python 時，我們得告訴程式該怎麼達成該目標（How）。 為了進一步闡述這個概念，接著讓我們試著使用 Python 來取得跟上面的 SQL 查詢一樣的結果。 用 Python 達到 SQL 查詢效果 首先先假設所有顧客資料是透過一個 list 儲存，裡頭包含多個 dict 。每個 dict 則代表一個顧客的資料： customers = [ { \"name\" : \"Customer#000000001\" , \"custkey\" : \"1\" }, { \"name\" : \"Customer#000000002\" , \"custkey\" : \"2\" } ] 而購買記錄則是一個 dict ， dict 的鍵值為所有顧客的 custkey ；鍵值對應的值則是包含該顧客所有購買紀錄的 list ： orders = { \"1\" : [{ \"totalprice\" : 152411.41 , \"orderdate\" : \"1993-06-05\" }, { \"totalprice\" : 270087.44 , \"orderdate\" : \"1997-03-04\" }, { \"totalprice\" : 165928.33 , \"orderdate\" : \"1995-10-29\" } ] } 所以 orders[\"1\"] 就代表 custkey = 1 的顧客的購買紀錄。 了解背後的資料結構以後，我們可以寫一段 Python 程式碼來取得資料： print ( \"customer | totalprice| orderdate \" ) print ( \"------------------ | ----------| --------- \" ) # 從所有顧客找符合條件的人 for c in customers : # 跳過我們沒興趣的顧客 if c [ 'name' ] != 'Customer#000000001' : continue # 利用 custkey 取德該顧客的購買紀錄 c_orders = orders [ c [ 'custkey' ]] # 依照 orderdate 排序購買紀錄 c_orders_sorted = sorted ( c_orders , key = lambda x : x [ 'orderdate' ]) # 將所有排序後的記錄回傳 for o in c_orders_sorted : values = [ c [ 'name' ], str ( o [ 'totalprice' ]), str ( o [ 'orderdate' ])] print ( \" | \" . join ( values )) # 已經找到該顧客，提早結束迴圈以減少處理時間 break customer | totalprice| orderdate ------------------ | ----------| --------- Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 所以我們使用 Python 達到跟上面的 SQL 查詢一樣的結果了。但兩者在執行上有什麼差異？ 使用命令式程式語言來處理資料時，我們需要： 了解資料結構以操作資料（顧客是存在 list 還是 dict ？） 明確地定義執行步驟（先排序購買記錄 orders 還是先把顧客 customers 跟購買紀錄合併？） 最佳化（如最後的 break ） 再看一次先前的 SQL 查詢（+註解）： -- 給我以下幾個欄位：顧客名稱、總購買金額、購買日期 SELECT c . name AS customer , o . totalprice , o . orderdate -- 將有相同 custkey 的顧客跟購買紀錄合併 FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey -- 只需要此顧客的購買紀錄 WHERE c . name = 'Customer#000000001' -- 依照購買日期排序 ORDER BY o . orderdate ; 這裡頭我們不需要了解資料被以什麼形式儲存，也不需要定義要以什麼順序執行查詢，更不用做最佳化。這些事情全部交給背後的資料庫處理，使得資料科學家可以專注在更高層次的問題：「我們需要什麼資料？」 而這正是 SQL 最強大的地方： SQL 讓資料科學家可以專注在需要「什麼」資料而非要「怎麼」取得。 結語 雖然我們這篇只舉了一個十分簡單的例子，但一般來說 SQL 非常適合以下的使用情境： 將多個資料來源（例：表格）合併起來並依照一些條件篩選結果 依照取得的資料做一些簡易的 aggregation （如：加總、平均、最大值） 簡單的資料轉換（例：把 datetime 欄位取出年份） 如果需要十分複雜的資料轉換或者計算時，一般我還是推薦使用 Python 或 R。但是下次當你有機會使用 SQL 取得想要的資料時，不妨試著專注在「想要什麼資料」而不是「怎麼取得資料」。說不定一個 SQL 查詢就能幫你省下一些花在搜集資料的時間。 推薦閱讀 DataCamp - Intro to SQL for Data Science DataCamp - Joining Data in PostgreSQL LinkedIn Learning - Advanced SQL for Data Scientists","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/why-you-need-to-learn-sql-as-a-data-scientist.html"},{"title":"資料科學家為何需要了解資料工程","text":"透過描述資料科學家的一天日常，本文將簡單介紹資料工程（Data Engineering）的概念、其如何跟資料科學相關。以及最重要的，作為一個資料科學家（Data Scientist）應該如何學習並善用這些知識來創造最大價值。 身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 目錄 資料科學家的一天 資料準備 第一挑戰：資料量大增 第二挑戰：非結構化資料 資料為本 資料管道 資料倉儲 資料湖 如何實際應用資料工程？ 結語 資料科學家的一天 一說到資料科學，在你腦海中浮現的幾個關鍵字可能是： 資料分析 資料視覺化 A.I. / 機器學習 等為人津津樂道的面向。 的確，這些都在資料科學的範疇裡頭，但實際上佔用多數資料科學家大部分時間，卻常被忽略的部分是資料準備： Forbes : 依據調查，多數資料科學家花 80 % 的時候在準備資料 資料準備 說到資料準備，你可能會聯想到我們在前一篇 淺談資料視覺化以及 ggplot2 實踐 裡頭，使用 R 語言做的簡單資料清理： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read.csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as.numeric ( ramen_all $ Stars ) # Subset 資料 ramen <- ramen_all %>% filter ( Country %in% count ( ramen_all , Country , sort = TRUE )[ 1 : 6 , 1 , drop = TRUE ]) %>% filter ( Style %in% count ( ramen_all , Style , sort = TRUE )[ 1 : 4 , 1 , drop = TRUE ]) 在做分析之前，我們做了以下的步驟來準備資料： 讀進 ramen-ratings.csv 轉變某些欄位的資料型態 依照一些條件取出想要分析的資料 雖然資料量不大，你仍然可以試著想像我們實際上建立了一個 ETL 工作： 將資料從來源（硬碟）擷取出來（ E xtract） 做了一些轉換（ T ransform） 載入（ L oad）目的地（記憶體） 假設我們把一般的資料分析專案分為以下兩階段： 資料準備：將資料轉換成適合分析的格式 資料分析：探索資料、建構預測模型 上面的 ETL 就屬於第一個步驟。又因為此資料集大概只包含 5,000 筆資料，步驟 1 所花的時間跟步驟 2 的所需時間相比，可以說微乎其微，它不會是你做資料科學的一個 bottleneck。 但如果你要處理的資料量是這個的 1,000 倍大呢？你還能馬上進入分析階段嗎？ 第一挑戰：資料量大增 實際上一個資料科學家每天需要分析的資料量可能要乘上幾個級數。現在假設你從銷售部門拿到一個包含數百萬筆銷售紀錄，大小為 60G 的 CSV 檔案，我想你應該不會想要直接打開它，即使它在某些人眼裡還不夠資格稱為大數據 (´；ω；｀) 你殫精竭慮，最後決定去問公司內一位資深的 資料工程師（Data Engineer） 該怎麼解決這問題。 該仁兄施了點你不曉得的魔法，過了幾分鐘從 Slack 丟來個神秘的 URL。連到上面，你發現熟悉的 Jupyter Nook 介面，而且 CSV 還幫你載好了 Σ(ﾟдﾟ; Bonus : Jupyter Lab 是 Jupyter Notebook 的改善版，大推 你開心地在資料工程師幫你搞定的機器上做出分析，最後在大家面前做口頭報告。大家針對報告的反應不錯，但坐在底下的廣告部門的人這時候提問了： 「可以把這些銷售紀錄跟廣告點擊的串流日誌（log）合在一起分析嗎？這樣我們會有更多有趣的結果！」 你的頭又痛了起來。 第二挑戰：非結構化資料 除了資料量級的差異，一個資料科學家在企業裡頭會遇到的另外一個挑戰是非結構化資料（Unstructured Data）的快速增加。你如何將各種不同格式（JSON、存取日誌、CSV 等）的資料以有效率的方式跟平常熟悉的關聯式資料庫如 PostgreSQL 裡頭的資料結合以供分析？ AWS Reinvent : 非結構化資料快速增加，但因為不存在關聯式資料庫裡，無法直接被拿來分析 如果我們能寫一個簡單的 SQL 查詢，把銷售資料（sales）跟廣告點擊（clicks）資料依照共有的鍵值 sale_id 合起來該有多好： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 你想著想著就到下班時間了。 「算了，還是先回家睡個覺，明天再厚著臉皮問資料工程師吧！反正之前他也幫我在 資料倉儲（Data Warehouse） 加了新的表格。」 資料為本 從上面這個資料科學家的一天，我們得到什麼啟示？ 資料（的基礎設施）為資料科學之基礎 - 巧婦難為無米之炊 老實說這個例子裡頭的資料科學家已經非常幸運：公司裡有資料工程師能幫他把大量、複雜格式的資料做 ETL 並以資料倉儲中的一個新表格（Table）的方式呈現轉換過後的資料以供他使用。硬要說稍微不方便的地方，頂多就是該資料科學家得等資料工程師搞定好資料就是了。 然而因為資料工程師是一個很新的職位，多數的企業現在並沒有這樣的人存在。大多數的資料科學家只能自己下海，想辦法生出可以用的資料。實際上， Monica Rogati 在 The AI Hierarchy of Needs 提到，一些常見的資料科學專案像是 建置 AI 建置簡單的機器學習模型 資料分析 都得建立在「有完善且可靠的資料」這個基礎之下： 資料科學的金字塔層級要求 : 你需要建立好資料科學的基礎設施才有本錢往「上」發展 以金字塔最下三層為例，要讓資料科學的專案順利進行，你最少要（由下而上）： 持續搜集（COLLECT）原始資料 將該資料轉移（MOVE / STORE）到適合分析的地方如資料倉儲、 資料湖 轉換（TRANSFORM）被轉移的資料，進行前處理以方便分析 我認爲資料工程的重頭戲在上面的 2, 3 點：將資料以「轉換好」的形式「送」到可供分析的地方。（當然也可以先送再轉換，或者不轉換，詳見下面章節的 資料湖 ） 身為資料科學家，如果你夠幸運，公司內部有專業的資料工程師幫你把上面這件事情做好，恭喜！你可以多專注在分析以及建置預測模型上面； 但假設公司裡頭只有資料科學家，而企業又想要處理大數據的話，抱歉，你得擔起這個攤子，想辦法把資料的基本設施搞定： 每個成功的資料科學家背後都有個偉大的資料工程師。或者該資料科學家就是那個資料工程師。 身為資料科學家，如果我們也能了解資料工程相關的知識的話，不就能更快地、更有效率地進行資料分析了嗎？ 這個想法即是所謂的 從鄰近專業（Adjacent Disciplines）學習 ：透過學習跟本業息息相關的資料工程，資料科學家可以加速資料科學的專案進行，並為個人以及團隊創造更大價值。想閱讀更多，可以看看 在 Airbnb 工作的資料科學家怎麼說 。 接著讓我們稍微聊聊到底什麼是資料工程以及一些相關例子。 資料管道 依照前面的論述，資料工程最主要的目的就是建構資料科學的基本設施（Infrastructure）。而這些基礎設施裡頭一個很重要的部分是 資料管道（Data Pipeline） 的建置：將資料從來源 S ource 導向目的地 T arget 以供之後的利用。有必要的話，對資料進行一些轉換。 一些簡單的例子像是我們之前部落格提到的： 將 NoSQL（MongoDB） 資料導向資料倉儲（Redshift） 將串流資料（Kinesis）導向資料湖（AWS S3） 從上面的例子也可以看到，實際上資料管道是一個涵蓋範圍很廣的詞彙，包含 即時的串流資料處理 Batch 處理（如：每 12 小時作一次） ETL 做的事情跟資料管道類似，但偏重在 Batch 處理，這篇文章將 ETL 視為資料管道裡頭的一個子集。 ETL : 從資料來源擷取、轉換資料並將其導入目的地 資料的來源或目的地可以是： 分散式檔案儲存系統（如 HDFS 、 AWS S3 ） 一般的資料庫 / 資料倉儲（如 AWS Redshift ） ... ETL 最重要的是轉換步驟，一些常見的轉換包含： 改變欄位名稱 去除空值（Missing Value） 套用商業邏輯，事先做資料整合（Aggregate） 轉變資料格式（例：從 JSON 到適合資料倉儲的格式如 Parquet ） 資料工程師 : 建構資料管道以讓大量的資料可供分析 這些轉換都是為了讓之後使用資料的資料科學家們能更輕鬆地分析資料。為了建置可靠的資料管道 / ETL 流程，我們常會需要使用一些管理工具像是 Airflow 、 AWS Glue 以確保資料的處理如同我們預期。 一些關鍵技術 Hadoop 生態環境 分散式系統上的 ETL 設計 SQL-on-Hadoop 的專案了解（如 Apache Hive, Spark SQL, Fackbook Presto） 資料流程管理（如 Airflow、AWS Glue） 那經過資料管道處理後的資料要怎麼存取/分析？依照存取方式的不同，資料管道的架構方式也會有所不同。 而存取資料的方式大概可以分為兩種： 資料倉儲（Data Warehousing） 資料湖（Data Lake） 資料倉儲 資料倉儲的概念就跟實際的倉儲相同：你在這邊將原料（原始資料）轉化成可以消化的產品（資料庫裡頭的經過整理的一筆筆紀錄）並存起來方便之後分析。 這邊最重要的概念是：為了方便商業智慧的萃取，在將資料放入資料倉儲前，資料科學家 / 資料工程師需要花很多的心力決定資料庫綱目（Database Schema）要長什麼樣子。 也就是說資料庫的綱要（Schema）在建立資料管道的時候就已經被決定了：這種模式稱之為 Schema-on-Write。這是為了確保資料在被放進資料倉儲的時候就已經是可以分析的形式，方便資料科學家分析。 你可以想像資料工程師在建構資料管道 / ETL 的時候，得對原始資料做大量的轉換以讓資料在被 寫 入資料倉儲時就已經符合一開始定義的 Schema。而資料倉儲最常被拿來使用的一個資料模型（Data Model）是所謂的 Dimensional Modeling （Stars / Snowflaks Schema）。 資料倉儲最被廣泛使用的 Data Model : Stars Schema 資料工程師將企業最重要的事件（如：使用者下了訂單、發了一個 Facebook 動態）放到最中間的 Fact Table，並且為了可以使用所有想像得到的維度來分析這些事件，會把事件本身的維度（Dimensions）再分別存在外圍的多個 Dimension Tables。常見的維度有： 時間（此事件什麼時候產生、年月份、星期幾等） 商品的製造商的資料、其他細節 ... 因為看起來就像是一個星星，因此被命名為 Stars Schema。Snowflakes 則是其變形。 一些關鍵技術 在資料倉儲的部分，關鍵的技術與概念有： 了解正規化（Normalization）的好處 分散式 SQL 查詢引擎的原理（如 Presto ） 分析專用的資料模型的設計原理（如 Stars / Snowflakes schema） 了解分散式系統背後各種 JOIN 的原理（Sort-Merge JOINs、Broadcast Hash JOINs、Paritioned Hash JOINs 等） 資料湖 「每天新增的資料量太多，要把所有想分析的資料都做詳細的 Schema 轉換再存入資料倉儲太花人力成本。總之先把這些資料原封不動地存到分散式檔案儲存系統上，之後利用如 AWS Glue 等服務將資料的 schema 爬出來並分析。」這就是以資料湖為核心的資料管道架構想法。一般這種存取資料的方式我們稱之為 Schema-on-Read，因為 Schema 是在實際載入原始資料的時候才被使用。 AWS Athena 就是一個 AWS 依照這樣的想法打造的服務。 舉個簡單的例子，假設我們現在想把 資料科學家的一天 提到的銷售資料以及廣告資料合併起來做分析，在 AWS 上我們可以實作一個這樣的資料管道： 利用 AWS Athena 及 AWS Glue 實作以資料湖為基礎的分析架構 : 即時合併並分析不同格式的資料 我們將存在關聯式資料庫的銷售資料透過 ETL 存到資料湖（AWS S3）裡頭以後，利用 AWS Glue 將資料的中繼資料（Meta Data）存在資料目錄（Data Catalogue）底頭。常見的中繼資料有 表格定義（有哪些欄位，如： sale_id ） 各個欄位的資料型態 各個欄位實際在原始資料（如 CSV ）裡頭的排列順序 接著我們就可以利用提到的 SQL 查詢把銷售資料跟廣告資料合併： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 收到以上的 SQL 查詢，Athena 會分別把銷售資料以及廣告資料依照對應的資料目錄解析資料後合併再回傳結果給我們。 我認為今後這種以資料湖為基礎的分析架構會越來越熱門，原因如下： 非結構化資料量越來越大，花費人力在事前為資料倉儲建立完整的 schema 越來越不實際 分散式 SQL 查詢服務像是 Athena 抽象化複雜的資料格式，允許資料科學家下 SQL 查詢做 ad-hoc 分析 透過 Parquet / ORC 等資料格式來自動減少資料湖沒有做正規化而導致的效能損失 一些關鍵技術 雖然再過幾年，等到資料工程的人才增加，資料科學家或許可以完全不用介意背後的資料基礎設施的建置，但近幾年這部分可能還是要靠資料科學家自己實作。 資料湖的概念 AWS Glue + AWS Athena 的運用（Bonus: Serverless 分析架構，不需管理機器） Hive MetaData Store 在資料湖的例子我主要都用 AWS 的服務來舉例，但你可以自由使用其他雲端服務或者 Hadoop。 如何實際應用資料工程？ 首先你得先了解目前環境的資料基礎設施。而為了釐清這點，你可以問自己或者相關人員以下問題： 資料科學的金字塔，我們建到哪一層了？ 我們過去有哪些專案是在取得、準備資料階段就陷入瓶頸？ 我們有專業的資料工程師或相關人員在做資料倉儲或者是資料湖的準備嗎？ 我們的資料是儲存在什麼分散式檔案儲存系統上面？ HDFS 還是 S3？ 我們是怎麼管理/監管 ETL 工作的？ 要考慮用 Airflow 嗎？ 要建構一個新的資料管道的話，要自己架 Hadoop 群集還是使用雲端服務？ ... 在你思考過以上幾個問題以後，你就會發現為何過往有些資料科學的專案進展緩慢了。這時候與其一直在等待資料的到來，你可以把你想到的幾個問題拿去跟相關的工程師討論。相信我，從你開口跟他們討論如何解決資料基礎設施的瓶頸這點開始，他們將不再視你為「那個只想要拿到資料」的敵人，而是同伴。 Hadoop 的分散式基礎設施 : 要學的東西太多，不如就用雲端服務吧 假如很不幸，你們公司沒有專業的工程師，而你得自己想辦法兜出一個可以處理這些大量資料的方法，我會建議先從現存的全受管（Full-Managed）雲端服務找能解決痛點的方案。 使用現成的雲端服務來建置資料基礎有幾個好處： Pay-as-you-go，通常是用多少花多少 Proof-of-concept，你可以直接開始嘗試建立最重要的商業邏輯而非架機器 Serverless 架構，不需管理機器（如 AWS Glue + Athena） 導入成本降低（相較於自己架 Hadoop Cluster） 結語 我嘗試在這篇文章說明資料工程對資料科學家的重要，以及你可以如何開始學習資料工程。 在這個大數據時代，資料科學家的價值在於找出「大量」資料背後的潛在價值，不要反而讓「資料量太多」這邊成了你最大的限制。 從雲端服務開始，多學一點資料工程，讓你的資料科學專案前進地更快吧！ 如果你有任何想法想要提出或分享，都歡迎在底下留言或者透過社群網站聯絡我 B-)","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/why-you-need-to-learn-data-engineering-as-a-data-scientist.html"},{"title":"淺談資料視覺化以及 ggplot2 實踐","text":"這篇主要描述自己以往在利用 Python 做資料視覺化 (data visualization) 時常犯的思維瑕疵，而該思維如何在接觸 R 的 ggplot2 以後得到改善。 本文會試著說明資料視覺化的本質為何，以及在設計視覺化時，概念上應該包含什麼要素以及步驟。最後展示如何透過 ggplot2 活用前述的概念，來實際做資料視覺化。 目錄 文章內容大致上會分為以下幾個小節： 資料視覺化是資料與圖的直接映射？ 資料視覺化應該是 .. ggplot2 實踐 結語 References 資料視覺化是資料與圖的直接映射？ 身為一個 Python 起家的資料科學家，在做資料視覺化的時候，我很自然地使用 Python ecosystem 裡像是 matplotlib 以及 seaborn 等繪圖 packages。針對手中的資料，我會想辦法找到一個「對應」的圖然後把資料塞進去。簡單無腦 (:3 」∠) 舉例來說，當我們手上有三個變數 x, y, z 且其各自的資料型態為： x: 定量變數 (quantitative) y: 定量變數 z: 定性變數（categorical） 則我們想要進行資料視覺化的時候有幾種選擇： 想分析 x, y -> 都是定量資料 -> 散佈圖 (scatter plot) 想分析 x, z -> 一定量一定性 -> 長條圖 (bar chart) 在這，「資料視覺化」的定義是一種映射關係 (mapping)：也就是如何將資料直接對應到某個「特定」圖表形式（折線圖、散佈圖 etc.）。基本上這種映射關係在做簡單的分析的時候沒有什麼問題，但是當想要同時分析/呈現的變數超過兩個 （例： x & y & z ）的時候就不容易找到適合的圖。一個折衷的方法是我們把變數兩兩畫圖做比較，但這樣會侷限我們能分析的資料維度數目，錯過一些有趣的洞見。 資料視覺化應該是 .. 先確認觀眾及目的 在完成一些 ggplot2 的 tutorials 後，可以發現資料視覺化一般依用途可以分為兩種： 探索、了解資料特性 說故事：將探索過後得到的洞見 (insight) 傳達給其他人 Image Credit : 搞清楚資料視覺化的目的以及觀眾是重要的第一步 依照目的以及觀眾的不同，資料視覺化的方式會有所不同。一個常見的例子是當我們第一次接觸某個資料集。這時候資料視覺化的觀眾是自己，目的是在最短的時間了解資料特性。則這時我們在做圖的時候的要求就可以很寬鬆，像是不加上標題，或是只要能做出自己能理解的視覺化即可。 正式定義 在確認觀眾及目的以後，我們終於可以開始進行資料視覺化了！資料視覺化的定義因人而異，而這邊我想給出一個非常直觀的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 一些常見且肉眼容易識別的視覺變數 / 刻度（visual variables / scales）包含： 位置（x / y axis） 顏色（color） 大小（size） 透明程度（alpha） 填滿（fill） 形狀（shape） 用更口語的方式來解釋：在做資料視覺化的時候，我們希望能將 肉眼難以分析的資料 對應到： 肉眼容易解讀的視覺元素 透過這個映射關係，我們可以將原本的變數的數值變化也映射到視覺變數的變化。而因為我們人類容易區別視覺變數的變化（位置差異、大小長度變化 etc），我們能更容易地理解原始資料的樣貌、變化以及模式。 舉例來說，我們可以： 把不同捷運路線（文湖線、板南線）對應到不同顏色 把各國的 GDP 對應到點的大小 把某個資料的年份對應到 Ｘ 軸，越右邊代表越接近現代 一個簡單例子 事實上，我們可能平常每天都在做資料視覺化而不自知。比方說我們有一個數列 y ： y = [ - 2.055 , - 1.132 , - 0.522 , - 1.229 , 0.013 .. ] 光是看這個數字，肉眼無法看出什麼模式，但我們可以簡單畫個圖： 這邊我們利用視覺變數「Y軸位置」來呈現數值的變化，可以馬上看出數列裡頭的值都落在 -3 到 3 之間，而這是因為我們肉眼很容易辨別「位置」這個視覺變數的變化。 圖像的分層文法 在 A Layered Grammar of Graphics 裡頭， Hadley Wickham 闡述所謂的圖像（包含由資料視覺化產生的圖像）實際上如同我們平常使用的語言，是有文法的。而其文法可以拆成 7 個部分（層）。前述的 原始資料 = 資料層（Data） 視覺變數層（Visual variables = Aesthetics） 則恰好是這個架構裡頭最底下的兩層。視覺變數是我為了方便理解建立的名詞，在原文以及 ggplot2 裡頭被稱作 Aesthetics 。（中文翻作「美學」，當初看好久也無法理解啊 (╯°Д°)╯ ┻━┻） Image Credit : 圖像的分層文法 看到這你一定會「哇靠那我每次畫個圖都要實作七層？」。實際上不需要，上面幾層像是主題（Theme）比較像是裝飾品，給我們更大的自由與彈性來訂製（customize）視覺化結果。在下一節我們會看到，ggplot2 會自動幫我們設定合適的主題或座標。（如果沒特別指定的話） 但一般而言，一個圖像最基本的組成是底下三層。也就是除了前述的兩層（資料、視覺變數）以外還需要加上 幾何圖形層（Geometries） 為何還要這層？假如我們有了資料，決定了視覺變數（第二層，例：把資料中的變數 A 對應到 X 軸；變數 B 對應到 Y 軸）後，實際上就可以畫一個充滿點（point）的散佈圖了不是嗎？ 這樣的思維如同 資料視覺化是資料與圖的直接映射？ 部分所提到的，有所瑕疵。如果變數 A 是分類型變數（Categorical）的話，單純以 點 為圖形的散佈圖就會變得十分難以理解（下圖左）；這時候以 長條 為圖形（下圖右）的方式會比較清楚： 獨立幾何圖形層 : 讓資料視覺化不再侷限於「我要畫什麼圖」，而是「我想要怎麼畫」 將「幾何圖形」這個選擇獨立出來一層讓我們在資料視覺化的時候有更大的彈性。有了這些基本概念以後，我們可以開始嘗試使用 ggplot2 來實際做一些資料視覺化。 ggplot2 實踐 在這個章節裡頭我們將使用 Kaggle 的 Ramen Ratings 來做資料視覺化。這資料集紀錄了各國泡麵所得到的星星數。首先我們要先載入這次的主角：R 語言裡頭最著名的視覺化 package ggplot2。 dplyr 則是 R 語言用來處理資料的 package。 載入 packages library ( ggplot2 ) library ( dplyr ) 值得一提的是它們都是同屬於 TidyVerse 的一員。TidyVerse 是 R 裡頭常被用來做資料科學的 packages 的集合，以 Python 來說大概就像是 Pandas + Matplotlib + Numpy 的感覺吧。 載入資料 + 簡單資料處理 如下註解所示，這邊將資料集讀入，做一些簡單的資料型態轉變後選擇一部分的資料集（subset）來做之後的視覺化： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read . csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as . numeric ( ramen_all $ Stars ) # Subset 資料，選擇拉麵數量前幾多的國家方便 demo ramen <- ramen_all %>% filter ( Country % in % count(ramen_all, Country, sort = TRUE)[1:6, 1, drop=TRUE]) %>% filter ( Style % in % count(ramen_all, Style, sort = TRUE)[1:4, 1 , drop=TRUE]) 除了我們使用 dplyr 的 filter 依照條件 subset 資料集以外，值得一提的是 pipe 運算子 %>% 。它是前面提到的 TidyVerse 裡頭的 packages 共享的介面（interface），將前一個函示的輸出當作下一個函式的輸入，讓我們可以把運算全部串（chain）在一起。在 Linux 裡頭就是如同 | 的存在。 而實際我們的資料長這樣： head ( ramen ) Review.. Brand Variety Style Country Stars Top.Ten 2580 New Touch T's Restaurant Tantanmen Cup Japan 37 2579 Just Way Noodles Spicy Hot Sesame Spicy Hot Sesame Guan-miao Noodles Pack Taiwan 7 2578 Nissin Cup Noodles Chicken Vegetable Cup USA 16 2577 Wei Lih GGE Ramen Snack Tomato Flavor Pack Taiwan 19 2575 Samyang Foods Kimchi song Song Ramen Pack South Korea 47 2574 Acecook Spice Deli Tantan Men With Cilantro Cup Japan 39 簡單資料視覺化 有了資料，讓我們再確定一下資料視覺化的目的及觀眾： 目的：探索資料 觀眾：我們自己 這樣的條件讓我們知道視覺化的條件是快速做出結果，不需調整如標題、主題的設定。 現在讓我們問一些簡單的問題。像是 泡麵的包裝（碗裝、袋裝等）各佔多少比例？ 不同國家各有多少泡麵在資料集裡頭？ 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 其中一種能解決第一個問題的資料視覺化是： ggplot ( ramen , aes ( x = Style )) + geom_bar () 在 ggplot(ramen, aes(x = Style)) + geom_bar() 裡頭，我們實際上已經建構了圖表最基礎的三層元素： 資料層： ramen 告訴 ggplot2 使用此資料框架 視覺變數層： aes(x = Style) 告訴 ggplot2 我們將使用「 X 軸位置」這個視覺變數來反映泡麵包裝 Style 這個變數的變化 因為包裝的值有四種可能，你可以想像 ggplot2 已經準備好要幫你在 X 軸上的四個位置畫圖 aes 是我們前面提到 aesthetics 的縮寫 幾何圖形層： geom_bar() 告訴 ggplot 去計算對應到 x 視覺變數的變數裡頭，所有值的出現次數後將結果以 長條 來呈現 我們通常透過 + 來疊加不同層的結果。 基本層數缺一不可 上面的例子很簡單，但假如我們沒有指定幾何圖形層的話，圖會長什麼樣子呢？ ggplot ( ramen , aes ( x = Style )) 就像我們剛剛所說的，雖然 ggplot2 已經知道要用什麼資料框架、要用什麼視覺變數，不知道要用什麼圖形表示的話就會是空白一張圖。 另個簡單例子 讓我們依樣畫葫蘆，來解決第二個問題： 不同國家各有多少泡麵在資料集裡頭？ ggplot ( ramen , aes ( x = Country , fill = Style )) + geom_bar () + coord_flip () 這邊有兩個值得注意的地方： 除了基本的三層以外，我們透過 + coord_flip() 額外對座標層（Coordinates）做操作，請 ggplot2 把 x, y 軸互換。 透過 aes(..., fill = Style) 裡頭的 fill = Style ，我們告訴 ggplot2 將長條圖裡頭的填滿空間（fill）這個視覺變數，依照泡麵包裝（Style）做變化 第二點是在做資料視覺化的時候，想辦法增加 資料墨水量（Data Ink Ratio） 的例子。透過增加顯示在同張圖上的變數數目，進而提高該圖能傳達的訊息量。 舉例而言，我們可以很明顯地看到，在這資料集裡頭，台灣的杯裝泡麵（Cup）沒有被記錄到多少；而日本被記錄到的泡麵量最多，且袋裝（Pack）數目最多。這些是在我們沒有用「填滿」這個視覺變數時無法察覺的。而在 ggplot2 裡，要實現這種視覺化非常容易。 複雜例子 讓我們解決最後一個問題： 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 資料視覺化一個有趣的地方就是：同個問題不同的人會有不同的做法。而針對這問題其中一種做法是： 將包裝 Style 對應到 X 軸、星星數 Stars 對應到 Y 軸，然後使用長條 geom_bar 顯示數值 依照每個國家重複步驟一 而 ggplot2 的實作為： ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) + facet_wrap ( ~ Country ) 實際上在上面的程式碼裡頭，我們多操作了額外兩層： 統計層（Statistics）：專門負責匯總資料 小平面層（Facets）：依照選定的變數分別畫圖，如上述的步驟二 首先 ggplot2 的 geom_bar 預設只需要 x 視覺變數，因為匯總資料的統計層會把 x 依照不同的值分別計數（也就是各個包裝的數量），然後讓 geom_bar 顯示。但我們並不希望 geom_bar 使用這個數值，因此使用 geom_bar 裡頭的 stat = \"identity\" 是告訴統計層不要分別計數，而是使用我們給定的星星數 y 。 而 facet_wrap( ~ Country) 則是告訴小平面層依照 Country 這個變數重複畫 ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) 注意所有的圖的 x, y 軸都是一致的，方便我們做比較。 結語 資料視覺化需要統計知識以及設計美感，涵蓋範圍非常廣大。這篇雖然打了落落長，但真的只有碰到皮毛（淚）。資料視覺化感覺都可以打個系列文了。但最後再次重申資料視覺化的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 總之先確認你的觀眾與目的，選好你想要觀察的變數，選擇適當的視覺變數做可視化吧！ References DataCamp - Data Visualization with ggplot2 (Part 1) r-statistics.co - ggplot2 tutorial Safari - Data Visualization in R With ggplot2","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/data-visualization-from-matplotlib-to-ggplot2.html"},{"title":"利用 Kinesis 處理串流資料並建立資料湖","text":"所謂的 資料湖 (data lake) 指的是一企業裡頭所有形式的資料的集合。這些資料包含原始資料 (raw data)，以及經過轉換的衍生資料 (derived data)。 資料湖的核心概念是將所有可用的資料全部整合在一個邏輯上相近的地方以供企業自由結合並做各式各樣的運用。資料湖可以用很多方式建立，這裏我們主要介紹如何利用 Amazon Kinesis 將串流資料 (streaming data) 載入資料湖。 概觀 資料湖概念上可以說是企業的所有資料的最終目的地。現在假設我們打算以 Amazon S3 中作為我們的資料湖，問題就變成：要如何將串流資料穩定地傳到 S3。這部分我們將透過 Amazon Kinesis 來達成。 Kinesis 本質上是跟 Apache Kafka 類似的 message broker ，將訊息依照 message producers 產生的順序傳遞給 message consumers。實際上資料的流動會如下圖所示： Simple Dataflow : 將 streaming data 透過 Kinesis 保存在 S3 上圖有幾點值得說明： 作為一個簡易的 demo，這邊我們的串流資料產生者 (streaming data producer) 是一個簡易 python script Streams 指的是 Amazon Kinesis Data Streams 。在 Kinesis 架構裡頭，一個 data stream 通常代表一個主題 (topic)， 跟這個主題相關的 producers 會把資料傳入該 stream 以讓該主題的 consumers 之後能接受訊息。 Firehose 指的是 Amazon Kinesis Data Firehose ，是專門把接受到的串流資料寫入 AWS 上的資料存放區（如 S3、Redshift、ElasticSearch）以供後續分析的服務。 建構流程 要完成上述的資料傳輸 pipeline，我們會 follow 以下步驟： 建立一個 Kinesis data stream 建立一個 Firehose delivery stream 用 Python 傳串流資料 確認 S3 上的資料 在每個步驟裡頭會稍微澄清一些概念。 建立一個 Kinesis data stream 現在假設有一個名為 naive-app 的應用程式，我們想要把使用者在上面做的操作紀錄下來。這時候我們可以建立一個新的 Kinesis Data Stream 來接受 app 的 streaming data。這邊指的 streaming data 是使用者存取應用程式時產生的 access log。 Scalability 這邊最重要的是 Number of shards 的設定。Kinesis 將接收到的資料以 log 的方式儲存在硬碟上，而為了提高 scalability，Kinesis 利用 Partitioning 的概念將 log 切割成多個部分並分配到不同的 shards 上，再將這些 shards 分別存在不同機器上面以提高 read/write capacity。因此我們可以理解一個 Kinesis Stream (Topic) 的資料吞吐量 (throughput) 直接受到 shard 的數目影響： shard 數目越多，同時能處理 read/write 的機器越多，資料吞吐量越高。 How to scale 理想上是一開始就掌握該 Stream/Topic 需要的資料吞吐量，進而決定最佳的 Number of shards ，但有時候事與願違。事後想要改變 shard 數目時需要透過 AWS Streams API 做 Resharding。Resharding 實際上就是在改變 shard 數目：增加 shard 會讓已存在的 shard 再度被切割；減少 shard 則會合併已存在的 shard。 在這邊我們就只直接使用一個 shard for demo。 Availability 另外值得一提的是 Kinesis 為了避免資料損失，會在三個不同的 availability zones 進行資料的 replication。因為這個額外的 overhead 可能使得在同樣設定下， Kinesis 比 Kafka 慢 的情況。因為是 log-based message broker，資料會被暫時存在硬碟上，預設保留 24 小時，而最多可以付費提升到維持 7 天以用來 replay data。 建立一個 Firehose delivery stream 有了接受 naive-app 串流資料的 Kinesis stream 以後，我們要建立一個 Firehose delivery stream 來接收 Kinesis stream 的資料。 Firehouse delivery stream 簡單來說是一個將串流資料存到 AWS 資料存放區的服務（如 S3、Redshift、ElasticSearch）。因此除了 Kinesis stream 的串流資料 以外，當然也可以接其他的串流資料： CloudWatch 的 log AWS IoT 使用者自定義的串流資料 在這篇裡頭我們的串流資料是 Kinesis stream，因此 Source 選擇 Kinesis stream 並填入我們剛剛建立的 stream 名稱： naive-app-access-log 。 值得一提的是 Firehose delivery stream 會 auto-scale，並不像 Kinesis stream 要手動調整 shard 數目。不過當然傳越多花越多。 如上張圖所示，實際上 Firehose 還允許我們在 delivery stream 接受到串流資料以後把原始資料傳到指定的 Lambda function 做進一步的轉換。 但因為我們想要資料湖儲存原始的串流資料，這邊我們省略這步驟。 Configuration 實際上 Firehose 不會一接收到資料就進行資料轉移。我們可以設定 Buffer size 以及 Buffer interval 讓 Firehose 在達到其中一個條件的時候把接收到的訊息統整起來一次做資料的轉移 (batch processing)。這邊為了能讓 Firehose 盡快把收到的資料轉移到 S3，設定 Buffer interval 為 60 秒。 選擇 delivery stream 目的地 在設定好 Firehose delivery stream 的串流資料來源（e.g., Kinesis stream）以及基本設定以後，我們要決定串流資料的目的地。這邊基本上很直覺， Destination 選擇 Amazon S3 以及想要放資料的 bucket 即可。 比較需要注意的是我們可以指定此 Firehose delivery stream 在放資料進入 bucket 時要為檔案加什麼前綴。 假設未來其他的串流資料我們也想要統一放在 me-data-lake 這個 bucket 裡頭。為了方便管理，我們可以為每個 delivery stream 設定一個識別用的 Prefix。以 naive-app 來說，我們指定 Prefix 為 naive-app-access-log/ 。加上 Firehose 預設的 YYYY/MM/DD/HH/ ，該 stream 的每個檔案的路徑就會變成如下圖的 naive-app-access-log/YYYY/MM/DD/HH/file_name 。 加入 Prefix 後實際將串流資料存入 S3 時的檔案路徑 用 Python 傳串流資料 確保 Kinesis stream -> Firehose delivery stream -> S3 的資料流設定以後，我們可以寫一個簡單的 Python script 實際傳資料進 Kinesis stream 做測試。但首先先讓我們使用 AWS SDK for Python 實作一個寄訊息給 Kinesis stream 的 function write_to_stream ： import boto3 import json def write_to_stream ( event_id , event , region_name , stream_name ): \"\"\"Write streaming event to specified Kinesis Stream within specified region. Parameters ---------- event_id: str The unique identifer for the event which will be needed in partitioning. event: dict The actual payload including all the details of the event. region_name: str AWS region identifier, e.g., \"ap-northeast-1\". stream_name: str Kinesis Stream name to write. Returns ------- res: Response returned by `put_record` func defined in boto3.client('kinesis') \"\"\" client = boto3 . client ( 'kinesis' , region_name = region_name ) res = client . put_record ( StreamName = stream_name , Data = json . dumps ( event ) + ' \\n ' , PartitionKey = event_id ) return res write_to_stream 基本上是把一個 Python dict event 利用 json.dumps 轉成字串後傳到指定的 region 的 Kinesis stream 裡的函式。（完整的 Gist ） 這邊值得注意的是 Data=json.dumps(event) + '\\n' 裡頭的 '\\n' 。如果之後想要利用 AWS Glue 或者 Athena 來進一步分析此串流資料的話，推薦在代表一個 event 的字串後面加上換行符號以維持「一行一事件」的資料形式，方便 schema 的自動產生。 範例日誌檔案內容會像是這樣： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 另外值得一提的是因為 Kinesis 背後是使用 Hash partitioning 來分配資料到 shard，基本上 PartitionKey=event_id 裡頭的 event_id 只要每個訊息都是獨一無二的，就能保證資料能「平均地」分配到各個 shard 上。 有了此函式以後，我們可以實際傳一些訊息進 Kinesis stream： while True : event = { \"event_id\" : str ( random . randint ( 1 , 100000 )), \"event_type\" : random . choice ([ 'read_post' , 'write_post' , 'make_comments' ]), \"timestamp\" : calendar . timegm ( datetime . utcnow () . timetuple ()) } # send to Kinesis Stream event_id = event [ 'event_id' ] write_to_stream ( event_id , event , REGION_NAME , KINESIS_STREAM_NAME ) time . sleep ( 5 ) 假設我們的 naive-app 可以讓使用者讀文章、寫文章以及寫評論，則上面的程式碼是模擬使用者使用 naive-app 時產生的事件，並將該事件的內容傳到 Kinesis stream naive-app-access-log 。60 秒內幾筆產生的事件如下： {'event_id': '56262', 'event_type': 'write_post', 'timestamp': 1522740951} {'event_id': '35672', 'event_type': 'make_comments', 'timestamp': 1522740956} {'event_id': '71613', 'event_type': 'read_post', 'timestamp': 1522740962} {'event_id': '48160', 'event_type': 'make_comments', 'timestamp': 1522740967} {'event_id': '96093', 'event_type': 'write_post', 'timestamp': 1522740972} 確認 S3 上的資料 注意因為上面的 5 個事件在 $5 * 5 = 25$ 秒內就產生了。且因為我們前面設定 Firehose delivery stream 的 Buffer interval 為 60 秒，Firehose 會把以上的事件的訊息全部串接起來，放到一個檔案裡頭，而不是分成五個檔案： 而實際檔案的內容如下： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 結語 到這邊為止成功把（偽）串流資料透過 Kinesis 存到 S3 了！為了方便之後的應用，輸出的檔案的內容格式或許還可以再改進，但資料湖的其中一個想法是 Command Query Responsibility Segregation (CQRS) ，也就是在存放資料的時候就只專心丟資料，不去在意之後資料會被以什麼方式、schema 使用，可以保證之後實際應用資料時有最大的彈性。 另外在確保資料好好地儲存在資料湖以後，我們通常會實際針對串流資料再進行一些處理 / 分析像是： 放到 Elasticsearch 並用 Kibana 做 Visualization 觸發 Lambda function 做進一步處理 使用 Athena 做 ad-hoc 分析 ... 但這邊時間有限，之後有機會再來記錄資料湖之後的分析筆記。 References Youtube: Introduction to Amazon Kinesis Firehose sumologic - Kinesis Stream vs Firehose A Cloud Guru - difference betwwen Kinesis Streams and Kinesis Firehose Getting started with AWS Kinesis using Python opsclarity - Evaluating Message Brokers: Kafka vs. Kinesis vs. SQS","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/use-kinesis-streams-and-firehose-to-build-a-data-lake.html"},{"title":"AWS Data Migration Service - 從 MongoDB 遷移到 Redshift","text":"同樣一份資料因應不同的使用案例，可能需要使用不同的存取方式。而針對這些不同的存取方式，我們通常需要選擇最適合的資料庫來最佳化使用者體驗。 這篇文章將簡單介紹如何使用 AWS Database Migration Service (以下簡稱 AWS DMS )來快速地達到我們的目標：將 MongoDB 資料遷移到 Redshift 上。 使用案例 舉例來說，一個電子商務網站的後端可以使用一個具有高度彈性的 NoSQL 資料庫如 MongoDB 來應對變化快速的使用者需求；而公司內部的資料科學家可以利用資料倉儲如 Redshift 來找出 business insight 。但這時候一個問題產生了：資料科學家用的資料倉儲 (例：Redshift) 的資料哪裡來？ 常見的方式是對 MongoDB 裡頭的資料定期做 ETL 以後將轉換過後的資料載入 Redshift 供分析需求。理論上在做 ETL 時要依照資料倉儲的 Data Model 重新設計 Tables (例： Star Schema )，但為了能在最短的時間將 MongoDB 上的資料轉到 Redshift 進行一些 Query，這篇文章將簡單介紹 AWS DMS 的運作方式，以及如何運用它來實際進行資料遷移所需要的步驟。 AWS DMS : 遷移（並轉換） AWS 上的資料庫 AWS DMS 基本介紹 DMS 基本上運作方式就是幫我們啟動一台 EC2 機器 (稱之為 replication instance) ，然後在上面跑 replication task(s) 。 一個 instance 上可以有多個 tasks 進行資料遷移。 instance 則分別透過 Source Endpoint / Target Endpoint 連結來源 / 目標資料庫。在後面我們會看到， endpoints 實際上就只是告訴 AWS DMS 的 replication instance 如何連結到實際的資料庫的設定罷了。在我們的例子裡頭，來源 / 目標資料庫分別對應到 MongoDB / Redshift 。 DMS 基本運作方式 : 資料遷移是由在 Replication Instance 上執行的 Replication Task 透過 endpoints 連結來源/目標資料庫完成的 基本遷移步驟 在假設來源 / 目標資料庫已經在運作的情況下，如同 AWS DMS 的 Get started, 一般會進行以下步驟來遷移資料： 建立 replication instance 確保 replication instance 能連結到來源 / 目標資料庫 定義 replication task Debugging：確保一切運作正常 以下針對每個步驟，我會紀錄一些需要注意的地方。 建立 replication instance 點擊 AWS DMS 介面的 Get started 選項會請我們建立新的 instance: 建立 Replication Instance : 注意 VPC /設定 這步驟基本上沒什麼問題， replication task 會佔用大量的 CPU 以及記憶體資源，理想上是依據需求選擇 Instance class ，不過第一次測試功能的話用預設的 t2.medium 即可。這邊值得注意的是 VPC 以及下面進階選項的 VPC Security Group(s) 設定。 如果來源 / 目標資料庫都可供公開存取的話，基本上不需要 VPC 。但一般來說我們都會有安全考量，也就是要求所有在 AWS 上的資源都要套用安全設定，則最簡單的架構是將來源資料庫、目標資料庫以及 Replication Instance 都放入同一個 VPC ，並利用 security group 設定來允許該 Instance 存取兩個資料庫。概念上此 VPC 的架構會如下 ( sg 為 Security Group 之縮寫 )： 來源 / 目標資料庫所在的 Security Group 要允許 Replication Instance 所在的 Security Group 存取 以上圖為例， Security Group sg_mongodb 以及 sg_redshift 的 Inbound Rule 要允許 sg_replicate 存取。而允許存取的 Port 則依照資料庫實際使用的 Port 設定即可 (例： MongoDB 慣用 27017； Redshift 則是 5439)。最後別忘了在建立 replication instance 的進階設定的 VPC Security Group(s) 選擇 sg_replicate 。 另外你可能已經注意到上圖的 S3 bucket 。就 replication tasks 的 log 來看， AWS DMS 在遷移資料的時候實際上會再細分為兩步驟： Replication Task 將來源資料庫的資料載出、轉換並暫存到 S3 Task 將存在 S3 的資料載入目標資料庫 雖然 ClodWatch 需要額外收費，但為了方便除錯，建議使用。在文章後面的 Debugging 我們會實際看一些例子。 確保 replication instance 能連結到來源 / 目標資料庫 上一步驟設定好以後， AWS DMS 會馬上幫我們建立一個新的 replication instance。在等待的同時我們可以開始設定資料庫的 endpoints。 設定來源 / 目標 enpoints : 在此步驟確保 Replication Instance 可以連到兩個資料庫可以減少除錯時間 這步驟基本上依照資料庫的不同，需要的輸入的項目可能不一樣。不過值得一提的是，在 建立 replication instance 的時候我們已經讓來源 / 目標資料庫以及 replication instance 都待在同個 VPC 裡頭。假設我們的 MongoDB 是運行在該 VPC 裡頭的某個 EC2 instance 之上，要允許在同個 VPC 的 replication instance 存取該 EC2 instance，我們要在 Server name 選項輸入運行 MongoDB 的 EC2 的 Private IP (上圖第一個紅框)。 MongoDB as Source Database 當 MongoDB 為來源資料庫時有一些值得注意的事情可以參考 官方文件 。以下會說明一些值得特別注意的地方。 Metadata mode Metadata mode 預設為 document （上圖第二個紅框），也就是把 MongoDB 裡頭的 json-formated 文件放到 Redshift 裡頭對應 Table 的一個 _doc 欄位。假設 MongoDB 裡有一個 users collection ，裡頭存了以下文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 將會被以下的格式載入 Redshift： _doc | --------------------------- {\"user\": \"leemeng\", \"fav .. 而這通常不是我們要的。將 metadate mode 設定為 table 模式能讓 AWS DMS 把文件裡頭的欄位扁平化後放入對應的欄位(column)： user | favorite | a.b | unnecessary_field ------------------------------------------------- leemeng | chocolate | For fun!| Don't include me! 注意到這邊有一個我們不需要遷移到 Redshift 的 unnecessary_field 。在後面的 Transformation Rules 我們會了解怎麼辦該欄位去除。 Numbers of documents to scan 而 Numbers of documents to scan 選項則讓我們決定要讓 AWS DMS 拿多少文件來決定要建立哪些欄位。如果要遷移的 MongoDB collection 的文件 schema 很常被更動（常有新鍵值）的話，建議可以讓 AWS DMS 掃描多一點文件來建立足夠的欄位。 Redshift as Target Database 如果按照 AWS DMS 的 Get started 一步一步走的話基本上沒有問題。要注意的是 Redshift 要有允許 DMS 存取的 AMI Rule，否則會出錯。 定義 replication task 在確定 replication instance 可以連線到兩個資料庫後，可以開始建立我們的 replication task： 這邊我們可以看到有三種資料遷移方式 (圖中的 Migration type)： 遷移目前 MongoDB 的資料 同上，但在遷移後之後繼續同步 MongoDB & Redshift (前提是 MongoDB 要以 Replica Set 模式執行) 只把 Task 啟動後 MongoDB 的資料變動遷移到 Redshift 這邊選擇自己的想要的遷移方式即可。接著我們要告訴 AWS DMS 想要進行遷移的 MongoDB Collections 以及在想要做的簡單轉換。兩者分別透過 Selection Rules 還有 Transformation Rules 定義。 Selection Rules Selection Rules 的用途是告訴 AWS DMS 該遷移以及不要遷移的 (MongoDB) collections 。我們可以定義一個 general rule 讓一個 task 處理某個 db 的所有 collections ；也能讓一個 task 只負責一個 collection 的遷移。後者的設定比較花時間但是彈性比較高，可以依照不同 collection 特性決定遷移的方式。 下圖是定義一個 rule 告訴 AWS DMS 遷移所有在 MongoDB 的 Collections。另外如果想要排除哪個 collection 的話就新增一個 rule 並在 Action 選擇 Exclude 。基本上想要加幾個 Selection Rules 都可以。而 Exclude Rules 的效果是在所有 Include Rules 後套用。 Selection Rules : 選擇要遷移到目標資料庫的 Tables / Collections Transformation Rules 這邊所謂的轉換並不是對欄位的實際值 (value) 進行轉換，而是針對 Table / Column 層級做 排除（不遷移該 Table / Column） 幫 Table / Column 更名、大小寫轉換或是名稱加上 prefix / postfix 這種操作。下圖是將 users collection 裡頭不需要遷移的鍵值 uncessary_field 從 Redshift 排除的 rules: 透過這個 Transformation rule，我們上面 users collection 的範例文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 就會被轉成： user | favorite | a.b ------------------------------ leemeng | chocolate | For fun! 注意 uncessary_field 不會被存到 Redshift 裡頭。 Debugging 當建立並執行一個新的 replication task 後，我們可以從 Load State 看到每個 Table 載入的狀況。 Load State 有幾種可能的值： Before Loading Full Load Table completed Table error 當出現 Table error 時，我們可以先看 log 瞭解情況： 2018-03-28T01:23:30 [TARGET_LOAD ]E: RetCode: SQL_ERROR SqlState: XX000 NativeError: 30 Message: [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState XX000] ERROR: Load into table 'users' failed. Check 'stl_load_errors' system table for details. [1022502] (ar_odbc_stmt.c:4406) 依照不同的錯誤、不同的目標資料庫，實際的 log 內容會有所不同。以我們目標資料庫 = Redshift 的情況下，上面的 log 告訴我們 replication task 在載入 users Table 時出錯，詳情可以參考 Redshift 的 stl_load_error Table ( 官方文件 )： SELECT * FROM pg_catalog . stl_load_errors ORDER BY starttime DESC LIMIT 1 ; 查看 Redshift 裡頭 stl_load_error Table 來除錯 就這個錯誤例子來看， err_reason 的內容告訴我們有個 memo 欄位 ( colname ) 的值太長導致沒辦法載入 Redshift。這時候可以把正在運行的 replication task 暫停，用前面提到的 Transformation Rules 來去除該欄位。而基本上其他錯誤也能用類似的方式解決。 到這邊為止大致上應該可以順利把 MongoDB 的資料載入 Redshift 了。之後想到什麼再補充。","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/replicate-data-from-mongodb-to-redshift-using-aws-data-migration-service.html"},{"title":"Designing Data-Intensive Applications (1) - 序言","text":"最近在拜讀 Martin Kleppmann 的 Designing Data-Intensive Applications ， 覺得受益匪淺，且我也相信透過 Feynman Technique 將學到的東西用最淺顯易懂的方式表達能幫助自己內化這些知識，遂嘗試把閱讀後的心得記錄在此。 另外在提到書內內容時都會盡量使用英文原文，不另做名詞的翻譯，以方便對照書內內容。 何謂 data-intensive applications 所謂的 data-intensive applications 如同名稱所示，專注在如何有效率地處理、儲存 密集資料 。通常一個這樣的系統的後端要用多種方式處理資料，而不是只用一個資料庫就結束了。（雖然對 end users 來說可能看起來像這樣） 舉個簡單例子，一個電子商務網頁的後端除了做為 OLTP 的 NoSQL 資料庫 (e.g., MongoDB) 以外，可能還有： 一個專門存放網頁快取的資料庫 (e.g. Redis) 給資料科學家分析用的資料倉儲 (e.g., Redshift) 處理 streaming events 的 messaging queue (e.g., Kafka) 定期將 NoSQL 資料庫的資料做 ETL 存到 資料倉儲的批次處理 (e.g., Hive jobs) 光是要把以上所列的資料庫 / 分散式系統 / 資料流 以有系統的方式組合起來就需要大量經驗，更遑論還要達到以下三個要求了： 可靠性 (reliable): 像是 zero-down time, 很短的回應時間 etc 規模性 (scalable): 即使之後資料量增加，系統也能很好地運作 維護性 (maintainable): 容易改善、新增功能的系統設計 Image Credit : 如何了解各個 data system 的優缺點並予以組合 儘管我們不可能熟悉所有資料庫以及分散式系統的細節，了解他們背後設計的核心理念、演算法以及大致上的運作方式能讓我們了解每個 data system 的特性以及優缺點，依照不同的使用案例選擇最適合的 data system 並予以組合。 何謂資料密集 書中所指的「密集」資料有以下所列的特徵（一個以上）： 大量資料 資料的（格式、 schema etc）變動速度很快 資料有複雜結構 針對「資料有複雜結構」以及「資料變動很快」這點，最為人所知的 solution 就是 NoSQL 等允許彈性 schema 的資料庫的崛起； 而針對「資料量很大」這點，則端看使用案例有各式各樣的資料庫、分散式系統。舉幾個例子： 能有效儲存大量資料的 Google BigTable 以欄 (column) 為單位儲存以壓縮大量資料的資料倉儲 Redshift Amazon 的 Single-leader Replication - DynamoDB 專門處理 realtime streaming data 的 Kafka, RabbitMQ etc. 如同前述，以上提到的系統依照它們想要解決的問題的特性，背後都會有一些假設以及 trade-off 。了解這些背後的原理可以讓我們了解哪些工具在什麼時候最 powerful 。 這本書主要分成三部分來闡述，抓到大方向會比較容易閱讀： 針對單一機器上的資料，有哪些常用的資料儲存/處理方法 類似前一部份，闡述針對分散式系統的資料儲存/處理方法 資料密集型應用：如何將多個 data systems 組合起來 一句話總結 在資料密集的時代，我們的最終目標在於如何將各式各樣的 data systems 以有系統的方式「組合」起來，以建立一個可靠、具規模性以及維護性的系統。","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/designing-data-intensive-applications-1-preface.html"},{"title":"Google Data Studio 基礎","text":"Google Data Studio 是 Google 推出的一個 Dashboard / Reporting 的服務，讓我們可以利用多種 連結器 將儲存在如 Google Analytics、 Google 試算表及 Google BigQuery 等特定資料來源的資料做出漂亮的 visualization ，用資料講故事而不用自己設計 UI。公司內部雖然有自己的 dashboards 不過想說多試一些方案沒有壞處，而且現在 Data Studio 還是 Beta 版本，雖然介面是中文，說明文件還只有英文，想說把學到的一些技巧以及使用心得記錄下來。 將 Google 試算表的資料可視化 為了快速展示 Data Studio 的功能，我們將使用 政府資料開放平臺 上由交通部觀光局提供的 105年來台旅客性別統計 資料。將 CSV 檔案下載下來，稍微簡化格式後上傳到 Google 試算表以當作報表的資料來源。下圖是簡化後的資料： 資料來源 : 2016年來台旅客性別統計 每一列代表某地區 / 國家的訪台人數以及男女比 條件欄位應用 條件欄位 讓我們可以針對試算表裡頭每一列做 IF ELSE 判斷，依照判斷結果給予不同的值。現在假設我們想知道有多少國家的男性遊客過半數，可以使用簡易的評量表來計算： 訪台男性遊客過半國家佔全部國家的比例 我們發現高達八成的國家（有些是區域）的訪台男性遊客較女性為多。我們可以調查其他國家的訪客性別比，看是不是只有台灣有此現象。要產生分母的「國家數」很直覺，我們只要新增一個欄位並計算有幾個國家即可： 新增一個名為「國家數」的欄位 但要計算分子的「男性遊客過半國家數」就稍微 tricky 了。我們想做的是，針對每一國家（每一列），只有在該國訪台男性遊客百分比過半（超過 50%)的時候才會被納入結果。而 Data Studio 的 條件欄位 就是專門針對這種情況設計的。 使用 CASE 語法對每一列做 IF-ELSE 判斷 上面的公式用白話來說就是： 針對每一列的國家，看它的「男性百分比」欄位的值有沒有大於50。有的話值為1，否則為0。在針對每列做完條件判斷以後再把所有 1 加起來，就等於符合條件的國家數。 篩選器（filter）應用 根據上個分析，我們知道女性遊客過半的國家只佔 20%。假設我們想確切知道是哪些國家的女性遊客過半，可以從女性百分比最高的國家開始列出男女比： 訪台女性遊客過半國家 我們發現女性遊客過半的都是亞洲國家，或許我們可以簡單解釋成這些國家與台灣的距離短，適合女性遊客拜訪。而為了讓圖表易讀，上面這張組合圖額外建立一個篩選器來過濾掉男性遊客比女性多的國家： 新增一個篩選器以過濾男性遊客比例較高的國家 註：一般的長條圖可以直接透過設定限制長條圖數目 維度 VS 指標 在 Data Studio 裡頭，了解 維度跟指標的差異 很重要。 以我們現在的資料集為例，每一列就是一筆紀錄（record），每一行則是一個欄位。每個欄位則是維度或指標。 指標（Metric，底下藍色） 數值型欄位，有經過「匯總」，負責 quantify 資料 如「國家數」、「總人數」 維度（Dimension，底下綠色） 分類型欄位，負責 qualify 資料 如「國家」、「居住地」 fx 則代表是額外利用公式建立的欄位 像我們前面定義的「男性遊客過半國家數」欄位因為有經過 SUM 公式匯總成為一個數值，因此為一個指標（藍）。而如果我們透過 CASE 語法新定義一個「男性過半」欄位如下： 此欄位沒有經過匯總因此被視為維度，在上一張圖被標為綠色。因此一句話總結維度跟指標的功能就是： 維度負責「描述」資料； 指標則負責「衡量」資料。 資料透視表 (Pivot Table) 資料透視表很適合拿來看在不同條件下某個指標的表現。下圖是一個依照 居住地 國家 兩個維度計算「男性人數」指標的資料透視表： 依照 官方文件 有幾點值得注意： 資料透視表最多處理 50,000 筆資料，為了避免 scan 資料太花時間，可以額外建立一些篩選器 subset 資料 列維度跟欄維度最多可以分別設定 2 個維度（上例列欄各設定 1 個維度） 限制 可能因為還處在 beta 版本，在這篇文章寫的時候（2018/03）試用了一陣子發現 Data Studio 也有一些使用案例沒有辦法做到，像是： 篩選器（filter）只能設定像是「欄位 C 大於 X」這種條件，而不能做「當欄位 C1 > 欄位 C2」這種欄位間的比較。 同上，條件欄位也只能設定像是「欄位 C 大於某固定值 X」的條件 資料透視表包含的資料稍多 (> 2000筆)就開始變慢 .. 實戰演練 這篇文章用的報表連結在 此 ，可以自己試試不同 visualization。有任何 feedback 也歡迎聯絡。","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/google-data-studio-basics.html"},{"title":"Pelican 實戰手冊(主題篇)","text":"有些人可能已經注意到這個部落格是用 Pelican 所寫成並且 host 在 Github 上。這篇主要紀錄如何使用 Jinja2 自訂主題。 Pelican 是一個用 Python 寫的靜態網頁生成器, 可以幫我們把 reStructedText, Markdown file 甚至 Jupyer notebook 轉成靜態的 HTML 檔案。 靜態網頁的好處就是我們不需要一個 web server 或者是資料庫來管理內容, 可以把 HTML 檔案 host 在想要的地方，比方說 Github Pages 。用 Pelican 官網一句來介紹的話就是： Pelican is a static site generator, written in Python, that requires no database or server-side logic. - Pelican Blog Google 一下你會發現除了 Pelican 以外還有很多其他像是 Jekyll, Hexo 等 靜態網頁生成器 。 之所以會選擇 Pelican 是因為以下幾點： Pelican 是用 Python 寫的，讓 Python 開發者（我）很容易客製化 可以把 jupyter notebook 轉成 HTML ，這對每天寫一堆 notebooks 的資料科學家很友善 主題是用強大的 Jinja2 模組引擎建立，可以用前人寫好的 主題 或是自己寫 templates，自由度很高，也是本篇重點。 如果你的需求類似而且想要自己架一個部落格，可以現在就跳入 Pelican Quickstart ，有問題再回來看這篇。 Jinja2 是 Python 知名的模組引擎 (templating engine)，可以有系統地產生 HTML，很常出現在 Flask 或是 Django Apps 裡頭。以下介紹在建立 Pelican blog 時常用到的功能。 再利用 HTML 區塊 比方說我們可以建立一個汎用的 template base.html 來定義整個部落格共用的資訊，像是 header 裡頭要 import 的 css / favicon 等等： <!DOCTYPE html> < html lang = \"en\" > < head > {% block head %} < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> {% endblock head %} </ head > < body > {% block content %} < p > 部落格內容 </ p > {% endblock content %} </ body > 注意到上面的 {% block head %} jinja2 語法。會在多個 HTML 檔案重複使用的部分我們可以用 {% block BLOCKNAME %} 以及 {% endblock BLOCKNAME %} 包起來，然後在獨立顯示一篇文章的 article.html 裡頭我們可以定義： { % extends \"base.html\" % } { % block head % } {{ super () }} < title > 文章標題 </ title > { % endblock head % } < body > { % block content % } < p > 文章內容 </ p > { % endblock content % } </ body > 上面的 code 基本上是告訴 jinja2 article.html 要繼承 base.html 的所有內容，而在 head block 除了用 {{ super() }} 繼承 base.html 的內容以外，在下面再追加新的內容。而 content block 則是完全取代。 因此最後 article.html 會被渲染成： <!DOCTYPE html> < html lang = \"en\" > < head > < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> < title > 文章標題 </ title > </ head > < body > < p > 文章內容 </ p > </ body > 為當前文章取得前/後一篇文章連結 Pagination 範例: 顯示前後文章連結 依照主題不同，有些主題可能文章頁面裡頭並沒有提供前一篇/後一篇文章的連結。要像上圖為每一篇文章取得前後文章的連結，可以在 article.html 裡存取 articles Variable 並使用 jinja2 namespace 來取得前後文章( namespace 要在 jinja 2.10+ 以後才能使用) { # get prev- and next-article for pagination #} { % set ns = namespace ( found = false , prev = None , next = None ) % } { % for a in articles % } { # 要使用 break 要安裝 extension, 最佳化效率可省略 #} { %- if ns . found % }{ % break % }{ % endif % } { # 假設文章標題不會重複, unique #} { % if a . title == article . title % } { % set ns . found = true % } { % set ns . prev = loop . previtem % } { % set ns . next = loop . nextitem % } { % endif % } { % endfor % } 上面的 code 會 iterate 所有文章，當遇到當前文章的時候利用 loop.previtem 以及 loop.nextitem 把前後文章記下來。 jinja2 預設是無法在 loop 裡頭改變變數的值 ，但使用 namespace 即可。 接著就能利用剛剛取得的前後 article 物件來渲染前後連結： {# 方便起見的 assignment %} {% set prev_article = ns.prev %} {% set next_article = ns.next %} {% if prev_article %} < div > < a href = \"prev_article.url\" rel = \"prev\" > < span > Previous Post </ span > {{ prev_article.title }} </ a > </ div > {% endif %} {% if next_article %} < div > < a href = \"next_article.url\" rel = \"next\" > < span > Next Post </ span > {{ next_article.title }} </ a > </ div > {% endif %} 傳參數給子 template 有時候多個 templates 會使用類似的 HTML，像是當首頁 index.html 以及部落格 blog.html 都用相同格式渲染最新幾篇文章時，我們可以定義一個 article_entries.html 如下： { # 簡化版 #} { % for article in articles % } < article class = \"col-block\" > < a href = \"{{ SITEURL }}/{{ article.url }}\" > {{ article . title }} </ a > < p > {{ article . summary }} </ p > </ article > { % endfor % } 注意這時候如果直接在 index.html 使用 { % include 'article_entries.html' % } 是會出現 錯誤 的。理由是被 include 的 article_entries.html 看不到定義在 index.html 的 articles 變數。 解決方法是在 index.html 裡透過 {% with %} 語法定義一個 scope： { # 選擇前五篇文章來渲染 #} { % set articles_to_show = articles_page . object_list [ 5 ] % } { # 定義 scope #} { % with articles = articles_to_show % } { % include 'article_entries.html' % } { % endwith % } 使用 with 的好處是可以把子 template article_entries.html 當作 function 來使用，我們可以依照母 template 的需要，傳進想要渲染的文章即可。 Reference Jinja2 Extension","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/build-a-pelican-powered-blog-like-a-pro.html"},{"title":"BeautifulSoup 筆記","text":"Beautifulsoup 是一個可以幫助我們 parse HTML 的 lib, 這篇主要紀錄使用 beautifulsoup 時常用的指令。 from bs4 import BeautifulSoup 找出特定 HTML 物件 假設我們有一個字串代表一個表格： html = \"\"\"<div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div>\"\"\" 渲染成 HTML: x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 實際 HTML 架構： < div > < table border = \"1\" class = \"dataframe\" > < thead > < tr style = \"text-align: right;\" > < th ></ th > < th > x </ th > < th > y </ th > </ tr > </ thead > < tbody > < tr > < th > 0 </ th > < td > -2.863752 </ td > < td > -1.066424 </ td > </ tr > < tr > < th > 1 </ th > < td > -0.779238 </ td > < td > 0.862169 </ td > </ tr > </ tbody > </ table > </ div > 利用 BeautifulSoup 物件 parse HTML: from bs4 import BeautifulSoup soup = BeautifulSoup ( html , 'html.parser' ) soup <div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> 找到第一個符合條件的 table 標籤 table = soup . find ( 'table' , { 'class' : 'dataframe' }) table <table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table> 設定新屬性 / class 因為這時候我們取出來的 table 物件是 reference 到 soup 裡頭對應的物件, 只要直接改變對應的 attr 就會直接反映結果到 soup 物件: table [ 'class' ] = table [ 'class' ] + [ 'table' , 'table-striped' , 'table-responsive' ] soup <div><table border=\"1\" class=\"dataframe table table-striped table-responsive\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> Iterate 標籤裡頭的子標籤 for c in table . children : print ( f ' {c.name} in {table.name} ' ) thead in table tbody in table 移除標籤 這邊假設我們要移除表格裡頭第一行的值 ( 第2個 tr 標籤 ), 可以對要移除的標籤物件使用 extract() func. x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 for i , tr in enumerate ( soup . findAll ( 'tr' )): if i == 1 : tr . extract () x y 1 -0.779238 0.862169 建立新標籤 假設我們想要建立一個新的 blockquote 標籤，並加入一些文字： text = 'I love BeautifulSoup!' blockquote = soup . new_tag ( 'blockquote' ) blockquote . append ( text ) blockquote <blockquote>I love BeautifulSoup!</blockquote>","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/beautifulsoup-cheat-sheet.html"},{"title":"Seaborn 筆記","text":"這篇記錄我在使用 seaborn 做資料分析還有 visualization 時常用的 code. 一般慣例會把 seaborn 更名成 sns for reference. % matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt 基本設定 這邊值得注意的是要調整的參數要一次全部設定, 用好幾次 set() 的話只有最後一次的 set() 的結果會被保留 sns . set ( font = 'IPAPMincho' , font_scale = 1.8 ) Histogram data = np . random . randn ( 1000 ) data [: 10 ] array([-0.53267554, 0.03851161, -0.16072742, -0.70889663, 0.23085979, -1.61295347, -0.46508874, 0.60112507, 0.42017249, -0.73656917]) seaborn 是建立在 matplotlib 之上, 因此 matplotlib 也可以直接拿來跟 seaborn 產生的圖互動 plt . figure ( figsize = ( 10 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . title ( 'Defualt style with kde' ) sns . distplot ( data , kde = True ); plt . subplot ( 1 , 2 , 2 ) sns . set_style ( 'dark' ) plt . title ( 'Dark style without kde' ); sns . distplot ( data , kde = False ); Scatter plot df = pd . DataFrame ({ 'x' : np . random . randn ( 100 ), 'y' : np . random . randn ( 100 )}) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 2 0.016786 -0.016519 3 0.948504 0.298314 4 2.029428 1.211997 要使用 seaborn 初始設定就再呼叫一次 set() sns . set () 注意點： 一般用lmplot畫, 然後設定 fit_reg=False 就可以讓 regression line 消失. 有時候有沒有那條線影響圖很大 一樣先 x , 再 y for fit_reg in [ True , False ]: sns . lmplot ( 'x' , 'y' , data = df , fit_reg = fit_reg , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) title = 'Show regression line' if fit_reg else 'Without regression line' plt . title ( title ) 想要將兩個 lmplot 並排 render 可以參考這個 stackoverflow 答案 . Correlation matrix / Heatmap df = pd . DataFrame ({ 'x1' : np . random . randn ( 100 ), 'x2' : np . random . randn ( 100 ), 'x3' : np . random . randn ( 100 ) }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 0 1.269566 0.349083 -0.000743 1 -1.634587 0.072568 0.042596 2 -0.581238 -0.337935 -0.412084 3 -0.080881 -1.376481 1.361046 4 -0.609886 -1.061285 0.265788 這邊利用 pandas 本身的 corr() 計算 correlation matrix 然後使用 seaborn 做 vis. corr = df . astype ( float ) . corr () corr .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x1 1.000000 -0.034731 0.032407 x2 -0.034731 1.000000 -0.192169 x3 0.032407 -0.192169 1.000000 sns . set ( font_scale = 1.5 ) sns . heatmap ( corr , cmap = 'Blues' , annot = True , annot_kws = { \"size\" : 15 }, xticklabels = corr . columns . values , yticklabels = corr . columns . values ); References API References","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/seaborn-cheat-sheet.html"},{"title":"SQLite 筆記","text":"Table of Contents Prettier output 調整每一個 column 寬度 在 sqlite3 shell 裡清空畫面 使用 SQL script 建立 tables 顯示目前的 tables 顯示 table schema 顯示 indexes 這篇主要紀錄使用 SQLite shell 下 SQL Query 的指令。基本上在 shell 裡頭都是用 dot-command, 使用 .help 可以顯示所有可用的指令. Prettier output 在 command-line program 裡頭使用的 response format .mode column .headers on Example output Code Name Price Manufacturer ---------- ---------- ---------- ------------ 7 CD drive 90 2 9 Toner cart 66 3 調整每一個 column 寬度 .width 5 18 15 缺點是不同的 tables, 不同的 columns 需要的寬度不同, 要自己調整 要重置設定: .width 0 在 sqlite3 shell 裡清空畫面 要看 OS 決定實際的 shell command .shell clear 除了 clear 以外, 其他 shell command都能使用, e.g., .shell cd 使用 SQL script 建立 tables 比方我們有一個 create_tables.sql 內容是： CREATE TABLE Departments ( Code INTEGER PRIMARY KEY, Name varchar(255) NOT NULL , Budget decimal NOT NULL ); INSERT INTO Departments(Code,Name,Budget) VALUES(14,'IT',65000); 我們可以用 .read dot-command 在 shell 跑該 script 建立 Department table: .read create_tables.sql 顯示目前的 tables .tables 顯示 table schema .schema <TABLE_NAME> 顯示 indexes .indexes 在 Table T 的 Column C 建立 index CREATE INDEX <INDEX_NAME> ON T(C); 砍掉 index DROP INDEX <INDEX_NAME>","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/sqlite-note.html"},{"title":"Find Word Semantic by Using Word2vec in TensorFlow","text":"The goal of this assignment is to train a Word2Vec skip-gram model over Text8 data using Tensorflow. Word2vec is a kind of vector space model (VSM) in natural language processing (NLP) where the core assumption/intuition is that words that appear in similar 'context' share similar meaning and they should be near in the vector space. So what word2vec trying to do is to find a vector representation (embedding) for each word in our training corpus where words with similar meanings are near in the vector space. Figure 1 : words' representation in 2D vector space Unlike supervised learning, we don't have labels that tell us 'kitten' = 'cat'. So how do we train a model that will learn the relationship between these two words? Recap the assumption mentioned before, words with similar meaning tend to appear in similar context. Because 'kitten' and 'cat' appear in similar context, if we can train a model to predict the context of the target word 'cat' and 'kitten' respectively, model should learn a similar representation for both 'kitten' and 'cat' because they produce similar context. Figure 2 : Training data generated from target word and context As shown above, for each words like 'cat' in raw text, we will treat them as target word and the words surrounding it as context and construct the training instances (x, y) where x is target word and y is one of the word in context. And the definition of 'context' is decided by the parameter 'window size'. First, let's load the text data and build the training data in order to train our model. Libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. % matplotlib inline from __future__ import print_function import collections import os import math import random import zipfile import numpy as np import tensorflow as tf # from matplotlib import pylab # use pyplot instead import matplotlib.pyplot as plt from six.moves import range from six.moves.urllib.request import urlretrieve from sklearn.manifold import TSNE from tqdm import tnrange plt . style . use ( 'ggplot' ) Raw text data Download / load data Download the data from the source website if necessary. will store the zip file in the 'datasets' subdirectory. url = 'http://mattmahoney.net/dc/' def maybe_download ( filename , expected_bytes ): \"\"\"Download a file into 'datasets' sub-directory if not present, and make sure it's the right size. \"\"\" rel_path = 'datasets/ {} ' . format ( filename ) # if file in not found, download it if not os . path . exists ( rel_path ): filename , _ = urlretrieve ( url + filename , rel_path ) statinfo = os . stat ( rel_path ) if statinfo . st_size == expected_bytes : print ( 'Found and verified {} . size: {} ' . format ( rel_path , statinfo . st_size )) else : print ( statinfo . st_size ) raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) return rel_path filename = maybe_download ( 'text8.zip' , 31344016 ) Found and verified datasets/text8.zip. size: 31344016 Turn data into words Read the data into a string. def read_data ( filename ): \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\" with zipfile . ZipFile ( filename ) as f : data = tf . compat . as_str ( f . read ( f . namelist ()[ 0 ])) . split () return data words = read_data ( filename ) print ( 'Data size %d ' % len ( words )) Data size 17005207 Look into text corpus The 'data size' above mean how many words we have in the data. That is, there are about 17 millions words! Let's show some parts of the text data to make some sense of it. Some phrases which include word 'cat' with window size = '2'. cats = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'cat' or word == 'cats' ] print ( ' \\n ' . join ( cats [: 5 ])) del cats the cartoon cat garfield would amount of cats that roam politicians autodidacts cat lovers firearm and activists cat lovers epistemologists force australia cat six two Some phrases which include word 'kitten' with window size = '2'. kitten = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'kitten' ] print ( ' \\n ' . join ( kitten [: 5 ])) del kitten put the kitten nermal in s sex kitten in the of tom kitten one nine as a kitten rudolph grey called a kitten which is Create training data In order to let TensorFlow make use of the text corpus, we have to transform the text corpus into sequence of numbers. The way to achieve this to build a dictionary which map every word to a unique number and use that dictionary to transform the corpus into number-based data. Figure 3 : Build dictionary and turn text into numbers Notice that some rare words may appear very few times in the entire text corpus. We may want to exclude these terms to keep our dictionary in a reasonable size. In order to do this, we will build the dictionary and view these terms as UNK tokens. UNK means unknown word that doesn't exist in the vocabulary set and the default number of a UNK in dictionary is 0 as shown above. Decide dictionary size Depend on the size of the vocabulary, we will construct a dictionary for top vocabulary_size - 1 common words. For example, if the vocabulary_size = 50000 , we will first count the frequencies of every word appeared in the text corpus and put the most common 49,999 terms into our vocabulary and make the rest of words as UNK token. ( thus the 50,000 th term in the vocabulary). vocabulary_size = 50000 Build dictionary and transform text corpus into sequence of numbers def build_dataset ( words ): \"\"\" Build training data for word2vec from a string including sequences of words divided by spaces. Parameters: ----------- words: a string with every word devided by spaces Returns: -------- dictionary: a dict with word as key and a unique number(index) as their value. dictionary[word] = idx reverse_dictionary: a dict with index as key and the corresponding word as value. reverse_dictionary[idx] = word counts: a list contain tuples (word, frequency) sorted descendingly by frequency while use ('UNK', unk_count) as first tuple. data: a list contain indices of the original words in the parameters 'words'. \"\"\" # count term frequencies and choose the most frequent # terms of vocabulary_size count = [[ 'UNK' , - 1 ]] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 )) dictionary = dict () # index term by their frequency. while UKN is indexed as 0, # the term with most frequencies is indexed as 1, the term with 2th frequencies # is indexed as 2, ... for word , _ in count : dictionary [ word ] = len ( dictionary ) # turn the text corpus into a sequence of number where each number is the # index of the original term in 'dictionary' dict and mark those UNK's number # as 0 which indicate that they're unknown words data = list () unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data . append ( index ) # update UNK's count in corpus count [ 0 ][ 1 ] = unk_count # create reverse dict to enable lookup the original word by their index reverse_dictionary = dict ( zip ( dictionary . values (), dictionary . keys ())) return data , count , dictionary , reverse_dictionary data , count , dictionary , reverse_dictionary = build_dataset ( words ) print ( 'Most common words (+UNK) in text corpus: \\n {} ' . format ( count [: 5 ])) Most common words (+UNK) in text corpus: [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)] See transformed text corpus sample_idx = 1000 print ( '\" {} \" \\n\\n was transformed into \\n\\n \" {} \"' \\ . format ( ' ' . join ( words [ sample_idx : sample_idx + 10 ]), ' ' . join ([ str ( i ) for i in data [ sample_idx : sample_idx + 10 ]]))) del words # Hint to reduce memory. \"american individualist anarchism benjamin tucker in one eight two five\" was transformed into \"64 10276 5234 3248 9615 5 4 13 10 16\" Function to generate a training batch for the skip-gram model. As usual, we will use mini-batch GD to update our model's parameters. Other than batch_size, we also have to decide the range of context surrounding target word (skip_window) and how many training instances are we going to create from a single (target, context) pair. Figure 4 : Build mini-batches by different num_skips # global variable to randomize mini-batch data_index = 0 def generate_batch ( batch_size , num_skips , skip_window ): \"\"\" Generate a mini-batch containing (target_word, context) pairs of `batch_size`. Parameters: ----------- batch_size: mini_batch's size, typically 16 <= batch_size <= 512 num_skips: how many times to reuse an input/target word to generate a label. skip_window: how many words to consider left and right. Returns: -------- batch: a list of target words labels: a list of context words corresponding to target words in batch \"\"\" global data_index assert batch_size % num_skips == 0 assert num_skips <= 2 * skip_window batch = np . ndarray ( shape = ( batch_size ), dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ), dtype = np . int32 ) # initialize first (target, context) sequence # = [ skip_window target skip_window ] span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ): buffer . append ( data [ data_index ]) data_index = ( data_index + 1 ) % len ( data ) # for every target word, for i in range ( batch_size // num_skips ): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] # generate #num_skips of training instances for j in range ( num_skips ): # randomly choose a context word that hasn't been chosen yet # exclude target word by default while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] # shift to next (target, context) sequence buffer . append ( data [ data_index ]) # randomize the start location of every mini-batch # by adding one offset data_index = ( data_index + 1 ) % len ( data ) return batch , labels print ( 'data: \" {} \"' . format ( ' ' . join ([ reverse_dictionary [ di ] for di in data [: 8 ]]))) for num_skips , skip_window in [( 2 , 1 ), ( 4 , 2 )]: data_index = 0 batch , labels = generate_batch ( batch_size = 8 , num_skips = num_skips , skip_window = skip_window ) print ( ' \\n (target, context) with num_skips = %d and skip_window = %d :' % ( num_skips , skip_window )) print ( ' {} ' . format ( ' \\n ' . join ( [ str (( reverse_dictionary [ t ], reverse_dictionary [ c ])) \\ for t , c in zip ( batch , labels . reshape ( 8 ))]))) data: \"anarchism originated as a term of abuse first\" (target, context) with num_skips = 2 and skip_window = 1: ('originated', 'anarchism') ('originated', 'as') ('as', 'originated') ('as', 'a') ('a', 'term') ('a', 'as') ('term', 'of') ('term', 'a') (target, context) with num_skips = 4 and skip_window = 2: ('as', 'term') ('as', 'anarchism') ('as', 'a') ('as', 'originated') ('a', 'as') ('a', 'originated') ('a', 'term') ('a', 'of') Word2Vec skip-gram model. Figure 5 : Word2vec model Computation graph batch_size = 128 embedding_size = 128 # Dimension of the embedding vector. skip_window = 1 # How many words to consider left and right. num_skips = 2 # How many times to reuse an input to generate a label. # We pick a random validation set to sample nearest neighbors. here we limit the # validation samples to the words that have a low numeric ID, which by # construction are also the most frequent. valid_size = 16 # Random set of words to evaluate similarity on. valid_window = 100 # Only pick dev samples in the head of the distribution. valid_examples = np . array ( random . sample ( range ( valid_window ), valid_size )) num_sampled = 64 # Number of negative examples to sample. graph = tf . Graph () with graph . as_default (), tf . device ( '/cpu:0' ): # Input data. train_dataset = tf . placeholder ( tf . int32 , shape = [ batch_size ]) train_labels = tf . placeholder ( tf . int32 , shape = [ batch_size , 1 ]) valid_dataset = tf . constant ( valid_examples , dtype = tf . int32 ) # Variables. embeddings = tf . Variable ( tf . random_uniform ([ vocabulary_size , embedding_size ], - 1.0 , 1.0 )) softmax_weights = tf . Variable ( tf . truncated_normal ( [ vocabulary_size , embedding_size ], stddev = 1.0 / math . sqrt ( embedding_size ))) softmax_biases = tf . Variable ( tf . zeros ([ vocabulary_size ])) # Model. # Look up embeddings for inputs. embed = tf . nn . embedding_lookup ( embeddings , train_dataset ) # Compute the softmax loss, using a sample of the negative labels each time. # this is how we speed up training phase loss = tf . reduce_mean ( tf . nn . sampled_softmax_loss ( weights = softmax_weights , biases = softmax_biases , inputs = embed , labels = train_labels , num_sampled = num_sampled , num_classes = vocabulary_size )) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # This is because the embeddings are defined as a variable quantity and the # optimizer's `minimize` method will by default modify all variable quantities # that contribute to the tensor it is passed. # See docs on `tf.train.Optimizer.minimize()` for more details. optimizer = tf . train . AdagradOptimizer ( 1.0 ) . minimize ( loss ) # Compute the similarity between minibatch examples and all embeddings. # We use the cosine distance: norm = tf . sqrt ( tf . reduce_sum ( tf . square ( embeddings ), 1 , keep_dims = True )) normalized_embeddings = embeddings / norm valid_embeddings = tf . nn . embedding_lookup ( normalized_embeddings , valid_dataset ) similarity = tf . matmul ( valid_embeddings , tf . transpose ( normalized_embeddings )) Train the model num_steps = 100001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( 'Initialized' ) average_loss = 0 for step in tnrange ( num_steps ): batch_data , batch_labels = generate_batch ( batch_size , num_skips , skip_window ) feed_dict = { train_dataset : batch_data , train_labels : batch_labels } _ , l = session . run ([ optimizer , loss ], feed_dict = feed_dict ) average_loss += l if step % 2000 == 0 : if step > 0 : average_loss = average_loss / 2000 # The average loss is an estimate of the loss over the last 2000 batches. print ( 'Average loss at step %d : %f ' % ( step , average_loss )) average_loss = 0 # note that this is expensive (~20% slowdown if computed every 500 steps) if step % 10000 == 0 : sim = similarity . eval () for i in range ( valid_size ): valid_word = reverse_dictionary [ valid_examples [ i ]] top_k = 8 # number of nearest neighbors nearest = ( - sim [ i , :]) . argsort ()[ 1 : top_k + 1 ] log = 'Nearest to %s :' % valid_word for k in range ( top_k ): close_word = reverse_dictionary [ nearest [ k ]] log = ' %s %s ,' % ( log , close_word ) print ( log ) final_embeddings = normalized_embeddings . eval () Initialized var element = $('#0a58ece1-4988-4935-bf03-a674b2795354'); {\"model_id\": \"479bac87e8864793a2657306231c3b2a\", \"version_major\": 2, \"version_minor\": 0} Average loss at step 0: 7.942154 Nearest to up: refit, airmen, unexplored, scharnhorst, histones, envelopes, wanna, wick, Nearest to many: herbivorous, kazimierz, surgeries, juliette, merovingian, christadelphians, experimentation, strauss, Nearest to people: chicken, zulu, glaucus, temporarily, groundbreaking, mapuche, varnish, vinod, Nearest to some: platelets, mauritania, anzus, soaemias, plankton, orcs, cegep, danzig, Nearest to was: blotter, continuously, gulls, lineages, turbines, cardano, honky, gcb, Nearest to and: haller, potion, rickshaw, fares, soldier, mariam, sponsor, irs, Nearest to in: wrench, atwood, boys, fermat, uhf, midrash, hallucinogens, deflate, Nearest to be: infertility, olaf, faramir, dxf, latino, clem, aia, cation, Nearest to all: stripe, abbreviation, nationalised, maoi, hermann, three, lara, jay, Nearest to than: antiderivatives, deduce, brainiac, wry, propel, requested, selangor, transposed, Nearest to system: rmi, devine, elite, deism, transgressions, bows, primer, undoubtedly, Nearest to were: ange, oh, widest, fidel, kristallnacht, predicates, coprocessor, surnames, Nearest to see: astounding, equilateral, concubines, syllables, lackluster, transoxiana, kasparov, smooth, Nearest to that: figurative, cluster, wendell, horch, cards, prinz, ellesmere, maximilian, Nearest to s: endosperm, saberhagen, pipelined, recantation, calcium, infrastructural, manchuria, spears, Nearest to will: suppressive, rotations, predefined, dessau, confederation, hovered, radiohead, apicomplexa, Average loss at step 2000: 4.364343 Average loss at step 4000: 3.857699 Average loss at step 6000: 3.785119 Average loss at step 8000: 3.686496 Average loss at step 10000: 3.616513 Nearest to up: scharnhorst, refit, arabs, concubines, zeeland, airmen, calibers, diem, Nearest to many: some, kazimierz, herbivorous, timber, poorly, revitalize, psychoanalyst, studies, Nearest to people: ach, la, semester, several, glaucus, gediminas, mechanism, yupik, Nearest to some: many, overbearing, firmer, oracles, dissonant, limes, soaemias, cpu, Nearest to was: is, has, had, were, by, been, be, are, Nearest to and: or, s, but, scriptores, who, of, in, alans, Nearest to in: on, at, of, from, with, between, by, during, Nearest to be: have, was, is, do, receiving, latino, subclass, chisel, Nearest to all: marketed, maoi, abbreviation, stripe, protectors, nationalised, lara, chadic, Nearest to than: haer, deduce, caucus, omniglot, casings, propel, selangor, softer, Nearest to system: rmi, devine, elite, primer, hcl, try, deism, bows, Nearest to were: are, was, tiu, have, adware, ragged, popularizer, macrobiotic, Nearest to see: william, concubines, astounding, transoxiana, but, bullough, pigeon, artistic, Nearest to that: which, ellesmere, she, it, pectoral, also, who, breaches, Nearest to s: and, vu, his, the, was, carlist, tanoana, chulainn, Nearest to will: may, would, jews, barbarism, predefined, could, confederation, receiving, Average loss at step 12000: 3.606183 Average loss at step 14000: 3.572916 Average loss at step 16000: 3.410224 Average loss at step 18000: 3.453891 Average loss at step 20000: 3.539739 Nearest to up: unvoiced, scharnhorst, zeeland, him, refit, arabs, cesium, wick, Nearest to many: some, these, several, other, kazimierz, studies, all, psychoanalyst, Nearest to people: languages, gediminas, countries, glaucus, eplf, several, those, temporarily, Nearest to some: many, these, their, olav, overbearing, his, its, most, Nearest to was: is, has, were, had, became, be, chisel, been, Nearest to and: or, but, at, which, from, in, however, for, Nearest to in: at, on, during, from, for, by, with, and, Nearest to be: have, been, was, were, campaigner, by, receiving, combination, Nearest to all: these, marketed, maoi, marshals, many, lara, escalation, agate, Nearest to than: or, deduce, much, trough, haer, brainiac, tug, spalding, Nearest to system: hcl, rmi, elite, devine, oleg, bows, henotheism, primer, Nearest to were: are, was, had, tiu, have, be, by, is, Nearest to see: atzma, diaconate, transoxiana, syllables, showcasing, lackluster, bullough, rabbinical, Nearest to that: which, but, breaches, because, it, ietf, this, ellesmere, Nearest to s: pu, purr, tanoana, forum, traveller, predicated, plasmodium, integrator, Nearest to will: would, may, can, could, should, to, cannot, geographic, Average loss at step 22000: 3.502537 Average loss at step 24000: 3.485267 Average loss at step 26000: 3.483680 Average loss at step 28000: 3.478223 Average loss at step 30000: 3.503296 Nearest to up: him, begin, scharnhorst, them, unvoiced, refit, zeeland, calibers, Nearest to many: some, several, these, their, its, all, various, the, Nearest to people: countries, those, languages, dhea, temporarily, ach, vlsi, eplf, Nearest to some: many, these, several, the, their, olav, limes, this, Nearest to was: is, were, had, has, became, been, when, by, Nearest to and: or, who, in, but, from, of, rehearsal, tsunamis, Nearest to in: during, at, from, of, on, since, between, and, Nearest to be: have, is, been, are, were, mustelids, aldiss, impurity, Nearest to all: these, lara, some, many, inhabiting, several, maoi, marketed, Nearest to than: much, or, no, deduce, trough, spalding, tug, haer, Nearest to system: hcl, group, devine, francisco, oleg, rmi, raccoons, master, Nearest to were: are, was, have, had, is, tiu, dowager, been, Nearest to see: diaconate, include, atzma, showcasing, slider, transoxiana, rent, syllables, Nearest to that: which, this, however, but, what, where, kilometre, if, Nearest to s: his, forum, isbn, her, ancestral, insulated, sucking, and, Nearest to will: can, would, could, may, should, must, cannot, to, Average loss at step 32000: 3.500015 Average loss at step 34000: 3.495233 Average loss at step 36000: 3.458392 Average loss at step 38000: 3.301554 Average loss at step 40000: 3.431673 Nearest to up: out, him, them, unvoiced, back, arabs, down, begin, Nearest to many: some, several, these, various, those, their, certain, such, Nearest to people: languages, countries, those, quintessential, baum, serif, dhea, areas, Nearest to some: many, these, any, several, olav, their, those, most, Nearest to was: is, had, were, became, has, severing, being, been, Nearest to and: or, but, while, cynical, subsystem, where, mus, in, Nearest to in: from, of, during, on, for, and, bubbled, at, Nearest to be: been, have, were, are, is, continue, was, shotguns, Nearest to all: lara, these, both, two, many, travellers, lobster, citizen, Nearest to than: or, much, no, spalding, deduce, even, brainiac, significance, Nearest to system: systems, code, crusading, group, bows, smuts, hcl, completions, Nearest to were: are, have, was, tiu, had, be, been, being, Nearest to see: include, pigeon, syllables, algardi, diaconate, theroux, slider, lada, Nearest to that: which, however, what, this, because, where, it, oro, Nearest to s: his, forum, levine, fl, fender, isbn, tyr, empowerment, Nearest to will: would, can, could, may, should, must, might, cannot, Average loss at step 42000: 3.433565 Average loss at step 44000: 3.451384 Average loss at step 46000: 3.450792 Average loss at step 48000: 3.354962 Average loss at step 50000: 3.388338 Nearest to up: out, them, down, him, disproven, off, back, unvoiced, Nearest to many: some, several, these, various, those, most, such, other, Nearest to people: countries, languages, roots, vlsi, men, gediminas, those, scholars, Nearest to some: many, these, several, olav, most, their, any, the, Nearest to was: is, has, were, became, had, been, being, be, Nearest to and: but, or, in, while, of, from, whose, eurasia, Nearest to in: from, during, of, since, on, and, by, within, Nearest to be: have, been, were, was, is, become, being, are, Nearest to all: both, these, many, lara, writs, every, citizen, diffuses, Nearest to than: or, much, even, bam, deduce, while, spalding, significance, Nearest to system: systems, crusading, bows, code, hcl, renderings, smuts, group, Nearest to were: are, was, have, tiu, be, had, those, been, Nearest to see: include, pigeon, diaconate, algardi, theroux, bloomington, showcasing, pwnage, Nearest to that: which, however, what, often, uv, this, where, honoring, Nearest to s: whose, isbn, romanian, his, cleaner, adheres, fender, predicated, Nearest to will: would, could, can, may, must, should, shall, might, Average loss at step 52000: 3.437301 Average loss at step 54000: 3.427909 Average loss at step 56000: 3.438053 Average loss at step 58000: 3.394049 Average loss at step 60000: 3.394188 Nearest to up: out, them, down, him, off, back, disproven, replace, Nearest to many: some, several, these, various, all, most, such, other, Nearest to people: those, countries, men, scholars, roots, gediminas, others, vlsi, Nearest to some: many, several, these, olav, most, any, each, this, Nearest to was: is, had, became, has, were, been, be, although, Nearest to and: or, but, including, than, owning, shutting, with, sagan, Nearest to in: during, within, including, of, at, since, throughout, anew, Nearest to be: been, have, are, is, was, were, refer, become, Nearest to all: many, these, both, rediscovery, some, timers, menial, various, Nearest to than: or, much, but, spalding, and, deduce, no, far, Nearest to system: systems, code, crusading, group, hcl, software, depth, smuts, Nearest to were: are, was, had, have, tiu, those, haired, be, Nearest to see: include, diaconate, but, can, bloomington, pwnage, according, algardi, Nearest to that: which, however, this, what, where, it, often, ellesmere, Nearest to s: whose, infrastructural, isbn, tanoana, arbenz, flybys, vladimir, geographic, Nearest to will: would, can, could, may, must, should, might, cannot, Average loss at step 62000: 3.243753 Average loss at step 64000: 3.259946 Average loss at step 66000: 3.404854 Average loss at step 68000: 3.393782 Average loss at step 70000: 3.359670 Nearest to up: out, them, down, off, back, him, disproven, arabs, Nearest to many: some, several, these, various, all, most, olav, numerous, Nearest to people: men, those, countries, scholars, children, peoples, languages, historians, Nearest to some: many, several, these, olav, various, all, those, any, Nearest to was: is, were, has, had, became, severing, been, when, Nearest to and: or, while, but, which, like, stockade, than, unclassified, Nearest to in: within, during, on, throughout, through, from, for, at, Nearest to be: been, were, is, have, being, become, are, fully, Nearest to all: many, some, both, every, any, montag, various, these, Nearest to than: or, no, spalding, much, while, but, and, resign, Nearest to system: systems, crusading, bred, depth, group, hcl, smuts, code, Nearest to were: are, was, have, tiu, had, be, those, been, Nearest to see: include, diaconate, orators, list, algardi, according, but, fatality, Nearest to that: which, however, what, this, but, where, because, also, Nearest to s: pu, infrastructural, isbn, whose, chulainn, codifying, sequestered, purr, Nearest to will: would, could, may, can, should, must, might, shall, Average loss at step 72000: 3.372420 Average loss at step 74000: 3.350309 Average loss at step 76000: 3.322349 Average loss at step 78000: 3.355263 Average loss at step 80000: 3.377625 Nearest to up: out, off, down, them, him, disproven, back, arabs, Nearest to many: several, some, various, these, most, numerous, those, both, Nearest to people: men, children, countries, those, words, members, peoples, scholars, Nearest to some: many, several, various, these, olav, most, both, any, Nearest to was: is, were, has, had, became, been, being, when, Nearest to and: or, but, including, than, while, globalsecurity, rehearsal, lances, Nearest to in: during, within, on, until, at, since, after, through, Nearest to be: been, have, become, refer, being, were, is, proceed, Nearest to all: both, every, various, any, many, some, these, aguilera, Nearest to than: or, much, but, while, and, spalding, resign, even, Nearest to system: systems, crusading, smuts, software, code, familias, nonpartisan, game, Nearest to were: are, was, had, have, those, tiu, being, be, Nearest to see: include, according, diaconate, but, orators, includes, list, eth, Nearest to that: which, however, where, ellesmere, markham, what, this, thus, Nearest to s: pu, isbn, whose, chulainn, mondeo, infrastructural, electorates, adheres, Nearest to will: would, could, can, may, should, must, might, cannot, Average loss at step 82000: 3.407648 Average loss at step 84000: 3.411925 Average loss at step 86000: 3.389199 Average loss at step 88000: 3.359499 Average loss at step 90000: 3.365950 Nearest to up: out, off, down, them, back, him, disproven, arabs, Nearest to many: some, several, various, these, all, numerous, most, those, Nearest to people: children, men, women, persons, religions, god, screenwriters, countries, Nearest to some: many, several, these, all, any, various, those, most, Nearest to was: is, became, had, were, has, been, being, be, Nearest to and: or, but, while, emory, consistory, including, however, who, Nearest to in: within, during, around, of, throughout, under, between, near, Nearest to be: have, been, become, is, produce, was, refer, simulate, Nearest to all: many, both, some, every, various, several, each, any, Nearest to than: or, much, spalding, no, even, resign, showered, considerably, Nearest to system: systems, crusading, process, program, group, mee, unit, brockovich, Nearest to were: are, was, had, have, while, tiu, those, been, Nearest to see: list, diaconate, include, includes, references, refer, external, kliper, Nearest to that: which, however, what, but, markham, autos, where, ellesmere, Nearest to s: whose, isbn, infrastructural, his, pu, tyr, vladimir, mondeo, Nearest to will: would, could, can, may, must, should, might, cannot, Average loss at step 92000: 3.398944 Average loss at step 94000: 3.258866 Average loss at step 96000: 3.356221 Average loss at step 98000: 3.242518 Average loss at step 100000: 3.359612 Nearest to up: out, off, down, him, back, them, arabs, begin, Nearest to many: several, some, these, various, numerous, those, all, few, Nearest to people: persons, men, children, someone, women, countries, players, scholars, Nearest to some: many, several, these, any, various, olav, all, certain, Nearest to was: is, became, had, were, has, although, when, been, Nearest to and: or, but, like, while, including, when, than, who, Nearest to in: within, during, throughout, of, at, from, on, with, Nearest to be: been, have, is, refer, become, are, were, produce, Nearest to all: many, various, any, every, these, some, both, several, Nearest to than: or, spalding, much, and, even, showered, while, omniglot, Nearest to system: systems, crusading, process, program, software, familias, hcl, stewardship, Nearest to were: are, was, have, tiu, these, those, had, be, Nearest to see: diaconate, includes, references, list, include, links, refer, external, Nearest to that: which, however, what, this, lodges, ellesmere, who, dicke, Nearest to s: whose, isbn, his, tyr, ancestral, infrastructural, pu, starring, Nearest to will: would, could, must, can, should, may, might, cannot, Transform embedding into 2D using t-SNE final_embeddings . shape (50000, 128) %% time num_points = 400 tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact') two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :]) Visualize result def plot ( embeddings , labels ): assert embeddings . shape [ 0 ] >= len ( labels ), 'More labels than embeddings' plt . figure ( figsize = ( 15 , 15 )) # in inches for i , label in enumerate ( labels ): x , y = embeddings [ i , :] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ), xytext = ( 5 , 2 ), textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) plt . show () words = [ reverse_dictionary [ i ] for i in range ( 1 , num_points + 1 )] plot ( two_d_embeddings , words ) Todo CBOW An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset. References Original jupyter notebook from the Udacity MOOC course: Deep learning by Google . TensorFlow word2vec tutorial http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/","tags":"Deep Learning","url":"https://leemengtaiwan.github.io/find-word-semantic-by-using-word2vec-in-tensorflow.html"},{"title":"Simple Convolutional Neural Network using TensorFlow","text":"The goal here is to practice building convolutional neural networks to classify notMNIST characters using TensorFlow. As image size become bigger and bigger, it become unpractical to train fully-connected NN because there will be just too many parameters and thus the model will overfit very soon. And CNN solve this problem by weight sharing. We will start by building a CNN with two convolutional layers connected by a fully connected layer and then try also pooling layer and other thing to improve the model performance. Original jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import numpy as np import seaborn as sns import tensorflow as tf import matplotlib.pyplot as plt from six.moves import cPickle as pickle from six.moves import range from tqdm import tnrange import time # beautify graph plt . style . use ( 'ggplot' ) Load notMNIST dataset pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data Reformat into a TensorFlow-friendly shape: convolutions need the image data formatted as a cube of shape (width, height, #channels) labels as float 1-hot encodings. image_size = 28 num_labels = 10 num_channels = 1 # grayscale def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size , image_size , num_channels )) . astype ( np . float32 ) labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28, 1) (200000, 10) Validation set (10000, 28, 28, 1) (10000, 10) Test set (10000, 28, 28, 1) (10000, 10) def accuracy ( predictions , labels ): return ( 100.0 * np . sum ( np . argmax ( predictions , 1 ) == np . argmax ( labels , 1 )) / predictions . shape [ 0 ]) Helper for training visualization Let's define a function that make better visualization of our training progress. The function will draw mini-batch loss and training/validation accuracy dynamically. # dynamic showing loss and accuracy when training % matplotlib notebook def plt_dynamic ( x , y , ax , xlim = None , ylim = None , xlabel = 'X' , ylabel = 'Y' , colors = [ 'b' ], sleep_sec = 0 , figsize = None ): import time if figsize : fig . set_size_inches ( figsize [ 0 ], figsize [ 1 ], forward = True ) ax . set_xlabel ( xlabel ); ax . set_ylabel ( ylabel ) for color in colors : ax . plot ( x , y , color ) if xlim : ax . set_xlim ( xlim [ 0 ], xlim [ 1 ]) if ylim : ax . set_ylim ( ylim [ 0 ], ylim [ 1 ]) fig . canvas . draw () time . sleep ( sleep_sec ) NN with 2 convolutional layers Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes. Computation graph Although this assignment already provide good Tensorflow code to build convoluational networks, I found that I can't imagine what NN I was going to build by reading the code. So I tried to draw what we're going to build and explain some parameters used in code by comments. The convolutional network we're going to build: Figure 1 : CNN with 1 fully connected layer Something worth mentioning: We set both convoluational layers' output depth = 16. We use filters/patches of shape (5 * 5) to find features in local area of a image. The new width and height of the convoluational layer will be half of that in the previous layer because we use stride = 2 and SAME padding to 'slide' our patches. Thus 28 -> 14 -> 7. Notice that ReLU layers applied after convoluational layers are omitted for simplicity. The activations in C2 fully connected to the FC layer. For each neuron on the FC layer, there are $7 * 7 * 16 = 784$ weights ($785$ for bias ), so there are $785 * 64 = 50240$ parameters in the FC layer. For more details about CNN, I recommend CS231n . batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # When defining weights for a convoluational layer, use the notation # [filter_size, filter_size, input_depth, output_depth] layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) # in this CNN, two convoluational layers happen to have the same depth. # if we want, we can adjust them to be different like depth1, depth2 layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # because we use stride = 2 and SAME padding, our new shape of first feature map C1 # will be (image_size // 2, image_size //2). and because we use 2 convolutional layers, # the shape of second feature map C2 will be (image_size // 2 // 2, image_size // 2 // 2) # = (image_size // 4, image_size // 4). and because we have depth == 16, # the total neurons on C2 will be image_size // 4 * image_size // 4 * depth layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # this is where we set stride = 2 for both width and height and also SAME padding # the third parameters in tf.nn.conv2d is to set stride for every dimension # specified in the first parameter data's shape conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) conv = tf . nn . conv2d ( hidden , layer2_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) shape = hidden . get_shape () . as_list () # turn the C2 3D cube back to 2D matrix by shape (#data_points, #neurons) reshape = tf . reshape ( hidden , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model and visualize the result The best thing of the visualization is that it's rendered in a real-time manner. num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#5335ab72-3cac-4dfb-b118-9bc3c0a4c66f'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#9384e836-8768-4a2d-aa35-d39f5bace624'); {\"model_id\": \"255b2330df764c069bf0fa8b72e282b3\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 4.085.batch acc: 0.0%, Valid acc: 10.0%. Minibatch loss at step 100: 0.861.batch acc: 62.5%, Valid acc: 74.5%. Minibatch loss at step 200: 0.425.batch acc: 87.5%, Valid acc: 79.1%. Minibatch loss at step 300: 0.934.batch acc: 62.5%, Valid acc: 79.8%. Minibatch loss at step 400: 0.861.batch acc: 68.8%, Valid acc: 79.6%. Minibatch loss at step 500: 0.204.batch acc: 87.5%, Valid acc: 80.6%. Minibatch loss at step 600: 0.741.batch acc: 75.0%, Valid acc: 82.0%. Minibatch loss at step 700: 0.591.batch acc: 87.5%, Valid acc: 82.5%. Minibatch loss at step 800: 1.171.batch acc: 68.8%, Valid acc: 81.8%. Minibatch loss at step 900: 0.171.batch acc: 100.0%, Valid acc: 83.2%. Minibatch loss at step 1000: 0.487.batch acc: 93.8%, Valid acc: 82.6%. Test accuracy: 89.4% As shown above, mini-batch loss dropped rapidly at first 200 iterations, training and validation accuracy also improve quickly (both achieved about 80%). After 200 iterations, validation performance become stable but still improved about 5%. The test accuracy is about 89%. Problem 1 - Use pooling layers to reduce dimensionality The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation ( nn.max_pool() ) of stride 2 and kernel size 2. The reason why we're going to use pooling layer is that we can reduce spatial size thus parameters to reduce the chance of overfitting. And the advantage of pooling layer is that it require no new parameters. Let's see how much performance we can gain by using max pooling. Figure 2 : Max Pooling Build model with pooling layers Actually, what we will do is just to add pooling layers right after ReLU layers and let the convoluational layer use stride 1. In intuition, we let the convoluational layers look more 'closely' into the images, but also try to limit the number of activation and extract the important parts by pooling layers. batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#abf4bc42-97f9-4697-b4d2-85d448b134a6'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#fd119874-b3da-47da-b0ef-dad0791e299a'); {\"model_id\": \"ae81313ad6ad4b428ce63444b7f0ae73\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.485.batch acc: 18.8%, Valid acc: 13.0%. Minibatch loss at step 100: 0.886.batch acc: 75.0%, Valid acc: 73.2%. Minibatch loss at step 200: 0.417.batch acc: 87.5%, Valid acc: 79.7%. Minibatch loss at step 300: 0.759.batch acc: 81.2%, Valid acc: 80.9%. Minibatch loss at step 400: 0.835.batch acc: 75.0%, Valid acc: 80.5%. Minibatch loss at step 500: 0.366.batch acc: 87.5%, Valid acc: 82.0%. Minibatch loss at step 600: 0.630.batch acc: 75.0%, Valid acc: 83.3%. Minibatch loss at step 700: 0.605.batch acc: 81.2%, Valid acc: 83.2%. Minibatch loss at step 800: 1.031.batch acc: 68.8%, Valid acc: 83.5%. Minibatch loss at step 900: 0.240.batch acc: 93.8%, Valid acc: 84.6%. Minibatch loss at step 1000: 0.500.batch acc: 87.5%, Valid acc: 83.9%. Test accuracy: 90.7% There is some performance gain in my current iteration between model w/o pooling layer ( about 1.3% ). And it seems that after several iterations, the validation set performance is slightly better with the pooling layers. But it took about 1 minute and 30 seconds to train CNN with pooling layers, and only 30 seconds to train the one without pooling layers. I think it depends on whether you're willing to gain a little more performance by using more time to train the model. Problem 2 Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay. I will just try to add a convoluational layer here and train a little longer to see how the performance changed. Computation graph batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # CONV -> FC layer4_weights = tf . Variable ( tf . truncated_normal ( [( image_size // 8 + 1 ) * ( image_size // 8 + 1 ) * depth , num_hidden ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) # FC -> output layer5_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer5_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # [CONV -> RELU -> POOL] * 3 conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer3_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer3_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer4_weights ) + layer4_biases ) return tf . matmul ( hidden , layer5_weights ) + layer5_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 10001 step_interval = 500 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#0c31dbbc-5cbd-4681-a8c2-49977c89a979'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#4e57be3a-4112-4350-b4fb-a05242ed0d55'); {\"model_id\": \"b240065c4ffa4ca29432614b189f2e04\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 3.563.batch acc: 6.2%, Valid acc: 10.0%. Minibatch loss at step 1000: 0.502.batch acc: 87.5%, Valid acc: 84.9%. Minibatch loss at step 2000: 0.414.batch acc: 87.5%, Valid acc: 86.4%. Minibatch loss at step 3000: 0.164.batch acc: 93.8%, Valid acc: 87.0%. Minibatch loss at step 4000: 0.545.batch acc: 75.0%, Valid acc: 88.1%. Minibatch loss at step 5000: 0.898.batch acc: 75.0%, Valid acc: 88.4%. Minibatch loss at step 6000: 0.493.batch acc: 81.2%, Valid acc: 86.4%. Minibatch loss at step 7000: 0.613.batch acc: 81.2%, Valid acc: 89.3%. Minibatch loss at step 8000: 0.089.batch acc: 100.0%, Valid acc: 89.2%. Minibatch loss at step 9000: 0.280.batch acc: 93.8%, Valid acc: 89.6%. Minibatch loss at step 10000: 0.437.batch acc: 87.5%, Valid acc: 89.4%. Test accuracy: 94.6% By adding a new convoluational layer and train 10x steps, our model's performance can even boost to almost 95%! (though it take about 5 minutes to train on my pc) and I think there are still many things we can tune to make the model better, but I will stop here to move on to sequence model!","tags":"Deep Learning","url":"https://leemengtaiwan.github.io/simple-convolutional-neural-network-using-tensorflow.html"},{"title":"Regularization for Multi-layer Neural Networks in Tensorflow","text":"The goal of this assignment is to explore regularization techniques. The original notebook can be found here Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function from tqdm import tnrange import numpy as np import tensorflow as tf from six.moves import cPickle as pickle Load NotMNIST dataset First reload the data we generated in 1_notmnist.ipynb . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) X_train = save [ 'train_dataset' ] Y_train = save [ 'train_labels' ] X_valid = save [ 'valid_dataset' ] Y_valid = save [ 'valid_labels' ] X_test = save [ 'test_dataset' ] Y_test = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , Y_test . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat dataset Reformat into a shape that's more adapted to the models we're going to train: data as a flat matrix, labels as float 1-hot encodings. As I did in previous notebook, this reformat operation will be different from the operation suggested by the original notebook . image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T return dataset , labels X_train , Y_train = reformat ( X_train , Y_train ) X_valid , Y_valid = reformat ( X_valid , Y_valid ) X_test , Y_test = reformat ( X_test , Y_test ) print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , X_test . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (784, 10000) Using Accuracy as Default Metric Because as we explored before, there exist no unbalanced problem in the dataset, so accuracy alone will be sufficient for evaluating performance of our model on the classification task. def accuracy ( predictions , labels ): return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) 3-layer NN as base model In order to test the effect with/without regularization, we will use a little more complex neural network with 2 hidden layers as our base model. And we will be using ReLU as our activation function. Hyper parameters # hyper parameters learning_rate = 1e-2 lamba = 1e-3 keep_prob = 0.5 batch_size = 128 num_steps = 501 n0 = image_size * image_size # input size n1 = 1024 # first hidden layer n2 = 512 # second hidden layer n3 = 256 # third hidden layer n4 = num_labels # output size Build model # build a model which let us able to choose different optimzation mechnism def model ( lamba = 0 , learning_rate = learning_rate , keep_prob = 1 , learning_decay = False , batch_size = batch_size , num_steps = num_steps , n1 = n1 , n2 = n2 , n3 = n3 ): print ( \"\"\" Train 3-layer NN with following settings: Regularization lambda: {} Learning rate: {} learning_decay: {} keep_prob: {} Batch_size: {} Number of steps: {} n1, n2, n3: {}, {}, {}\"\"\" . format ( lamba , learning_rate , learning_decay , keep_prob , batch_size , num_steps , n1 , n2 , n3 )) # construct computation graph graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training X = tf . placeholder ( tf . float32 , shape = ( n0 , batch_size )) Y = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) global_step = tf . Variable ( 0 ) # use all valid/test set tf_X_valid = tf . constant ( X_valid ) tf_X_test = tf . constant ( X_test ) # initialize weights, biases # notice that we have two hidden # layers so we now have W1, b1, W2, b2, W3, b3 W1 = tf . Variable ( tf . truncated_normal ([ n1 , n0 ], stddev = np . sqrt ( 2.0 / n0 ))) W2 = tf . Variable ( tf . truncated_normal ([ n2 , n1 ], stddev = np . sqrt ( 2.0 / n1 ))) W3 = tf . Variable ( tf . truncated_normal ([ n3 , n2 ], stddev = np . sqrt ( 2.0 / n2 ))) W4 = tf . Variable ( tf . truncated_normal ([ n4 , n3 ], stddev = np . sqrt ( 2.0 / n3 ))) b1 = tf . Variable ( tf . zeros ([ n1 , 1 ])) b2 = tf . Variable ( tf . zeros ([ n2 , 1 ])) b3 = tf . Variable ( tf . zeros ([ n3 , 1 ])) b4 = tf . Variable ( tf . zeros ([ n4 , 1 ])) # training computation Z1 = tf . matmul ( W1 , X ) + b1 A1 = tf . nn . relu ( Z1 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z1 ), keep_prob ) Z2 = tf . matmul ( W2 , A1 ) + b2 A2 = tf . nn . relu ( Z2 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z2 ), keep_prob ) Z3 = tf . matmul ( W3 , A2 ) + b3 A3 = tf . nn . relu ( Z3 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z3 ), keep_prob ) Z4 = tf . matmul ( W4 , A3 ) + b4 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( Y ), logits = tf . transpose ( Z4 ))) if lamba : loss += lamba * \\ ( tf . nn . l2_loss ( W1 ) + tf . nn . l2_loss ( W2 ) + tf . nn . l2_loss ( W3 ) + tf . nn . l2_loss ( W4 )) # optimizer if learning_decay : learning_rate = tf . train . exponential_decay ( 0.5 , global_step , 5000 , 0.80 , staircase = True ) optimizer = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss , global_step = global_step ) else : optimizer = ( tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss )) # valid / test prediction Y_pred = tf . nn . softmax ( Z4 , dim = 0 ) Y_vaild_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_valid ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) Y_test_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_test ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) # define training with tf . Session ( graph = graph ) as sess : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # generate randomized mini-batches from training data offset = ( step * batch_size ) % ( Y_train . shape [ 1 ] - batch_size ) batch_X = X_train [:, offset :( offset + batch_size )] batch_Y = Y_train [:, offset :( offset + batch_size )] # train model _ , l , batch_Y_pred = sess . run ( [ optimizer , loss , Y_pred ], feed_dict = { X : batch_X , Y : batch_Y }) if ( step % 200 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( batch_Y_pred , batch_Y ), accuracy ( Y_vaild_pred . eval (), Y_valid ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( Y_test_pred . eval (), Y_test ))) Train model without regularization model ( learning_rate = 0.5 , num_steps = 1601 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.5 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 1601 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#48767639-b1f4-49ff-9d55-71837005e7fd'); {\"model_id\": \"95a1e075513f4d02a8579c4fcd5b8509\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.374. batch acc: 14.1%, Valid acc: 28.4%. Minibatch loss at step 200: 0.600. batch acc: 82.0%, Valid acc: 84.9%. Minibatch loss at step 400: 0.429. batch acc: 89.8%, Valid acc: 85.8%. Minibatch loss at step 600: 0.372. batch acc: 87.5%, Valid acc: 85.7%. Minibatch loss at step 800: 0.454. batch acc: 89.1%, Valid acc: 87.7%. Minibatch loss at step 1000: 0.374. batch acc: 87.5%, Valid acc: 88.1%. Minibatch loss at step 1200: 0.251. batch acc: 91.4%, Valid acc: 88.8%. Minibatch loss at step 1400: 0.397. batch acc: 89.8%, Valid acc: 89.0%. Minibatch loss at step 1600: 0.470. batch acc: 82.0%, Valid acc: 88.9%. Test acc: 94.2% L2 regularization Introduce and tune L2 regularization for the models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t) . The right amount of regularization should improve your validation / test accuracy. # for lamda in [1 / 10 ** i for i in list(np.arange(1, 4))]: # model(lamba=lamda) model ( lamba = 0.1 , learning_rate = 0.01 ) Train 3-layer NN with following settings: Regularization lambda: 0.1 Optimizer: sgd Learning rate: 0.01 Batch_size: 128 Number of steps: 501 n1, n2: 512, 256 Initialized var element = $('#9d14226a-c5a9-49cd-8c26-88a2dc3b138a'); {\"model_id\": \"6b6986da296646b9aac2266326edcf21\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 22969.777. batch acc: 9.4%, Valid acc: 19.3%. Minibatch loss at step 200: 13876.185. batch acc: 74.2%, Valid acc: 75.2%. Minibatch loss at step 400: 9266.566. batch acc: 78.1%, Valid acc: 74.3%. Test acc: 81.4% Case of overfitting Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens? model ( num_steps = 10 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#bc4e6cf3-ac77-481e-be4e-7e07cb737f9d'); {\"model_id\": \"312b63e26f364cec95540b452b4ec95c\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.442. batch acc: 8.6%, Valid acc: 11.4%. Test acc: 20.7% Dropout Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training. What happens to our extreme overfitting case? model ( num_steps = 10 , keep_prob = 0.5 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 0.5 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#4405a286-14eb-45c5-b87c-9c7acfccde97'); {\"model_id\": \"f5a5be47b2a14f488d9d302df5d5988d\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.784. batch acc: 7.0%, Valid acc: 10.0%. Test acc: 17.3% Boost performance by using Multi-layer NN Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1% . One avenue you can explore is to add multiple layers. Another one is to use learning rate decay: global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step, ...) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) model ( learning_decay = True , num_steps = 1501 , lamba = 0 , keep_prob = 1 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: True keep_prob: 1 Batch_size: 128 Number of steps: 1501 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#f2fccad2-0290-4ee8-b867-197dc966ac68'); {\"model_id\": \"ee2b75cf5f6542f99a17c5ad02bc49fd\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.395. batch acc: 12.5%, Valid acc: 37.0%. Minibatch loss at step 200: 0.589. batch acc: 82.0%, Valid acc: 84.7%. Minibatch loss at step 400: 0.409. batch acc: 89.1%, Valid acc: 86.2%. Minibatch loss at step 600: 0.396. batch acc: 88.3%, Valid acc: 86.5%. Minibatch loss at step 800: 0.435. batch acc: 88.3%, Valid acc: 87.6%. Minibatch loss at step 1000: 0.407. batch acc: 85.2%, Valid acc: 88.5%. Minibatch loss at step 1200: 0.262. batch acc: 91.4%, Valid acc: 88.9%. Minibatch loss at step 1400: 0.411. batch acc: 87.5%, Valid acc: 88.8%. Test acc: 94.3%","tags":"Deep Learning","url":"https://leemengtaiwan.github.io/regularization-for-multi-layer-neural-networks-in-tensorflow.html"},{"title":"Using TensorFlow to Train a Shallow NN with Stochastic Gradient Descent","text":"The goal here is to progressively train deeper and more accurate models using TensorFlow. We will first load the notMNIST dataset which we have done data cleaning. For the classification problem, we will first train two logistic regression models use simple gradient descent, stochastic gradient descent (SGD) respectively for optimization to see the difference between these optimizers. Finally, train a Neural Network with one-hidden layer using ReLU activation units to see whether we can boost our model's performance further. Previously in 1_notmnist.ipynb , we created a pickle with formatted datasets for training, development and testing on the notMNIST dataset . This post is modified from the jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import os import numpy as np import tensorflow as tf from six.moves import cPickle as pickle from six.moves import range Load notMNIST dataset This time we will use the dataset which has been normalized and randomized before to omit the data preprocessing step. Tips: Release memory after loading big-size dataset using del . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) print ( 'Dataset size: {:.1f} MB' . format ( os . stat ( pickle_file ) . st_size / 2 ** 20 )) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Dataset size: 658.8 MB Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data for easier training Reformat both pixels(features) and labels that's more adapted to the models we're going to train: features(pixels) as a flat matrix with shape = (#total pixels, #instances) Figure 1 : Flattened features labels as float 1-hot encodings with shape = (#type of labels, #instances) Figure 2 : Flattened labels Tips: Notice that we use different shape of matrix with the original TensorFlow example nookbook because I think it's easier to understand how matrix multiplication work by imagining each training/test instance as a column vector. But in response to this change, we have to modify several code in order to make it works! Transpose logits and labels when calling tf.nn.softmax_cross_entropy_with_logits Set dim = 0 when using tf.nn.softmax Set axis = 0 when using np.argmax to compute accuracy One-hot encode labels by compare the label with the 0-9 array and transform True/False array as float array use astype(np.float32) image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T # key point1 return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (10, 10000) Logistic regression with gradient descent For logistic regression, we use the formula $WX + b = Y'$ to do the computation. W is of shape (10, 784), X is of shape (784, m) and Y' is of shape (10, m) where $m$ is the number of training instances/images. After compute the probabilities of 10 classes stored in Y', we will use built-in tf.nn.softmax_cross_entropy_with_logits to compute cross-entropy between Y' and Y(train_labels) as cost. We will first instruct Tensorflow how to do all the computation and make it run the optimization several times. Build the Tensorflow computation graph We're first going to train a multinomial logistic regression using simple gradient descent. TensorFlow works like this: First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below: with graph.as_default(): ... Then you can run the operations on this graph as many times as you want by calling session.run() , providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below: with tf.Session(graph=graph) as session: ... Let's load all the data into TensorFlow and build the computation graph corresponding to our training: # With gradient descent training, even this much data is prohibitive. # Subset the training data for faster turnaround. train_subset = 10000 graph = tf . Graph () # when we want to create multiple graphs in the same script, # use this to encapsulate each graph and run session right after graph definition with graph . as_default (): # Input data. # Load the training, validation and test data into constants that are # attached to the graph. tf_train_dataset = tf . constant ( train_dataset [:, : train_subset ]) tf_train_labels = tf . constant ( train_labels [:, : train_subset ]) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # These are the parameters that we are going to be training. The weight # matrix will be initialized using random values following a (truncated) # normal distribution. The biases get initialized to zero. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. # We multiply the inputs with the weight matrix, and add biases. We compute # the softmax and cross-entropy (it's one operation in TensorFlow, because # it's very common, and it can be optimized). We take the average of this # cross-entropy across all training examples: that's our loss. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. # We are going to find the minimum of this loss using gradient descent. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. # These are not part of training, but merely here so that we can report # accuracy figures as we train. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Tips: As we saw before, logits = tf.matmul(weights, tf_train_dataset) + biases is equivalent to the logistic regression formula $Y' = WX + b$ Transpose y_hat and y to fit in softmax_cross_entropy_with_logits Gradient descent by iterating computation graph Now we can tell TensorFlow to run this computation and iterate. Here we will use tqdm library to help us easily visualize the progress and the time used in the iterations. Tips: Use np.argmax(predictions, axis=0) to transfrom one-hot encoded labels back to singe number for every data points. Use .eval() to get the predictions for test/validation set from tqdm import tnrange num_steps = 801 def accuracy ( predictions , labels ): \"\"\"For every (logit/Z, y) pair, get the (predicted label, label) and count the occurence where predicted label == label and divide by the total number of data points. \"\"\" return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) # Calculate the correct predictions # correct_prediction = tf.equal(tf.argmax(predictions), tf.argmax(labels)) # # Calculate accuracy on the test set # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) return accruacy with tf . Session ( graph = graph ) as session : # This is a one-time operation which ensures the parameters get initialized as # we described in the graph: random weights for the matrix, zeros for the # biases. tf . global_variables_initializer () . run () print ( 'Initialized' ) for step in tnrange ( num_steps ): # Run the computations. We tell .run() that we want to run the optimizer, # and get the loss value and the training predictions returned as numpy # arrays. _ , l , predictions = session . run ([ optimizer , loss , train_prediction ]) if ( step % 100 == 0 ): print ( 'Cost at step {} : {:.3f} . Training acc: {:.1f} %, Validation acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , train_labels [:, : train_subset ]), accuracy ( valid_prediction . eval (), valid_labels ), \">\" )) # Calling .eval() on valid_prediction is basically like calling run(), but # just to get that one numpy array. Note that it recomputes all its graph # dependencies. print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#a10c5ba4-9227-4d9e-91c5-1f89815219c9'); {\"model_id\": \"a776469c0f394bab94bc900cf03f469d\", \"version_major\": 2, \"version_minor\": 0} Cost at step 0: 20.057. Training acc: 6.4%, Validation acc: 10.0%. Cost at step 100: 2.326. Training acc: 70.9%, Validation acc: 70.7%. Cost at step 200: 1.868. Training acc: 73.9%, Validation acc: 73.4%. Cost at step 300: 1.611. Training acc: 75.4%, Validation acc: 74.5%. Cost at step 400: 1.436. Training acc: 76.4%, Validation acc: 74.8%. Cost at step 500: 1.306. Training acc: 77.1%, Validation acc: 75.1%. Cost at step 600: 1.207. Training acc: 77.8%, Validation acc: 75.5%. Cost at step 700: 1.127. Training acc: 78.5%, Validation acc: 75.6%. Cost at step 800: 1.062. Training acc: 79.2%, Validation acc: 75.9%. Test acc: 82.8% Logistic regression with SGD Or more precisely, mini-batch approach. From the result above, we can see it cost about 20 seconds (on my computer) to iterate 10,000 training instances by simple gradient descent. Let's now switch to stochastic gradient descent training instead, which is much faster. The graph will be similar, except that instead of holding all the training data into a constant node, we create a Placeholder node which will be fed actual data at every call of session.run() . Tips: The difference between SGD and gradient descent is that the former don't use whole training set to compute gradient descent, instead just use a 'mini-batch' of it and assume the corresponding gradient descent is the way to optimize. So we will keep using GradientDescentOptimizer but with a different loss computed from a smaller sub-training set. Figure 3 : SGD vs Gradient Descent Build computation graph batch_size = 128 graph = tf . Graph () with graph . as_default (): # Input data. For the training data, we use a placeholder that will be fed # at run time with a training minibatch. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Iterate using SGD num_steps = 3001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # Pick an offset within the training data, which has been randomized. # Note: we could use better randomization across epochs. offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) # Generate a minibatch. batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] # Prepare a dictionary telling the session where to feed the minibatch. # The key of the dictionary is the placeholder node of the graph to be fed, # and the value is the numpy array to feed to it. feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#8b60a1c0-525e-4496-ae97-bcccce98ef65'); {\"model_id\": \"2d2a36e34f9843b5a38a9fe105281093\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 20.939. batch acc: 6.2%, Valid acc: 9.7%. Minibatch loss at step 500: 2.546. batch acc: 70.3%, Valid acc: 75.1%. Minibatch loss at step 1000: 1.520. batch acc: 74.2%, Valid acc: 76.3%. Minibatch loss at step 1500: 1.441. batch acc: 76.6%, Valid acc: 77.8%. Minibatch loss at step 2000: 1.135. batch acc: 79.7%, Valid acc: 77.1%. Minibatch loss at step 2500: 1.225. batch acc: 72.7%, Valid acc: 78.8%. Minibatch loss at step 3000: 0.932. batch acc: 76.6%, Valid acc: 79.4%. Test acc: 86.9% It took only about 3 seconds in my computer to finish the optimization using SGD (which took gradient descent about 20 seconds) and got a even slightly better result. The key of SGD is take randomized samples / mini-batches and feed that into the model every iteration (thus the feed_dict term). 2-layer NN with ReLU units Instead all just linear combination of features, we want to introduce non-linearlity in our logistic regression. By turning the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes, we should be able to improve validation / test accuracy. A 2-layer NN (1-hidden layer NN) look like this: Figure 4 : 1 hidden-layer NN A ReLU activation unit look like this: Figure 5 : ReLU Build compuation graph In this part, use the notation $X$ in replace of dataset . The weights and biases of the hidden layer are denoted as $W1$ and $b1$, and the weights and biases of the output layer are denoted as $W2$ and $b2$. Thus the pre-activation output(logits) of output layer is computed as $ logits = W2 * ReLU(W1 * X + b1) + b2 $ batch_size = 128 num_hidden_unit = 1024 graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) # use all valid/test set tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # initialize weights, biases # notice that we have a new hidden layer so we now have W1, b1, W2, b2 W1 = tf . Variable ( tf . truncated_normal ([ num_hidden_unit , image_size * image_size ])) b1 = tf . Variable ( tf . zeros ([ num_hidden_unit , 1 ])) W2 = tf . Variable ( tf . truncated_normal ([ num_labels , num_hidden_unit ])) b2 = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # training computation logits = tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_train_dataset ) + b1 )) + b2 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # optimizer optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # valid / test prediction - y_hat train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_valid_dataset ) + b1 )) + b2 , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_test_dataset ) + b1 )) + b2 , dim = 0 ) Run the iterations num_steps = 3001 with tf . Session ( graph = graph ) as session : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) # take steps to optimize for step in tnrange ( num_steps ): # generate randomized mini-batches offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#c4505474-0292-4cd0-8510-ed717993aac0'); {\"model_id\": \"f379ee5b4ed04eb18e5ee8d6b1e69a17\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 409.203. batch acc: 4.7%, Valid acc: 30.5%. Minibatch loss at step 500: 12.319. batch acc: 75.8%, Valid acc: 80.7%. Minibatch loss at step 1000: 12.638. batch acc: 74.2%, Valid acc: 80.8%. Minibatch loss at step 1500: 7.635. batch acc: 77.3%, Valid acc: 81.2%. Minibatch loss at step 2000: 7.322. batch acc: 80.5%, Valid acc: 81.4%. Minibatch loss at step 2500: 10.451. batch acc: 76.6%, Valid acc: 80.1%. Minibatch loss at step 3000: 3.914. batch acc: 83.6%, Valid acc: 82.7%. Test acc: 88.7% Summary Because we use a more complex model(1 hidden-layer NN), it take a little longer to train, but we're able to gain more performance from logistic regression even with the same hyper-parameter settings (learning rate = 0.5, batch_size=128). Better performance may be gained by tuning hyper parameters of the 2 layer NN. Also notice that by using mini-batch / SGD, we can save lots of time training models and even get a better result.","tags":"Deep Learning","url":"https://leemengtaiwan.github.io/using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent.html"},{"title":"Simple Image Recognition using NotMNIST dataset","text":"Today we're going to do some simple image recogintion using NotMNIST dataset. But before creating model for prediction, it's more important to explore, clean and normalize our dataset in order to make the learning go smoother when we actually build predictive models. I motified the notebook from Udacity's online Deep learning course and the objective of this assignment is to learn about simple data curation practices , and familiarize you with some of the data we'll be reusing later. This notebook uses the notMNIST dataset to be used with python experiments. This dataset is designed to look like the classic MNIST dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST. This notebook is mainly foucsing on data preprocessing rather than building models. Workflow Download / load raw notMNIST dataset Drop unreadable images and save the remaining images Combine all images and divide it into testing/validation/test set Shuffle / Randomize the dataset Remove duplicate images appear both in train/test or train/validation set Build simple model for image recognition using different size of training data After finishing this notebook, we learn Use matplotlib to read images, transform them to ndarray and render. Identify whether there exist unbalanced problem for the labels of classification . Understand why it's important to have both valid and test set. Identify the importance of randomizing data for better efficieny when training sequentially. Identify duplicate images between training/test set. Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import matplotlib.pyplot as plt import numpy as np import os import sys import tarfile from IPython.display import display from PIL import Image from scipy import ndimage from sklearn.linear_model import LogisticRegression from six.moves.urllib.request import urlretrieve from six.moves import cPickle as pickle # Config the matplotlib backend as plotting inline in IPython % matplotlib inline Dataset Download compressed dataset if the dataset is not available yet First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labeled examples. Given these sizes, it should be possible to train models quickly on any machine. url = 'https://commondatastorage.googleapis.com/books1000/' last_percent_reported = None data_root = './datasets' # Change me to store data elsewhere def download_progress_hook ( count , blockSize , totalSize ): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int ( count * blockSize * 100 / totalSize ) if last_percent_reported != percent : if percent % 5 == 0 : sys . stdout . write ( \" %s%% \" % percent ) sys . stdout . flush () else : sys . stdout . write ( \".\" ) sys . stdout . flush () last_percent_reported = percent def maybe_download ( filename , expected_bytes , force = False ): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os . path . join ( data_root , filename ) if force or not os . path . exists ( dest_filename ): print ( 'Attempting to download:' , filename ) filename , _ = urlretrieve ( url + filename , dest_filename , reporthook = download_progress_hook ) print ( ' \\n Download Complete!' ) statinfo = os . stat ( dest_filename ) if statinfo . st_size == expected_bytes : print ( 'Found and verified' , dest_filename ) else : raise Exception ( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?' ) return dest_filename train_filename = maybe_download ( 'notMNIST_large.tar.gz' , 247336696 ) test_filename = maybe_download ( 'notMNIST_small.tar.gz' , 8458043 ) Found and verified ./datasets/notMNIST_large.tar.gz Found and verified ./datasets/notMNIST_small.tar.gz Extract the dataset into folders by characters Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labeled A through J. num_classes = 10 np . random . seed ( 133 ) def maybe_extract ( filename , force = False ): root = os . path . splitext ( os . path . splitext ( filename )[ 0 ])[ 0 ] # remove .tar.gz if os . path . isdir ( root ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping extraction of %s .' % ( root , filename )) else : print ( 'Extracting data for %s . This may take a while. Please wait.' % root ) tar = tarfile . open ( filename ) sys . stdout . flush () tar . extractall ( data_root ) tar . close () data_folders = [ os . path . join ( root , d ) for d in sorted ( os . listdir ( root )) if os . path . isdir ( os . path . join ( root , d ))] if len ( data_folders ) != num_classes : raise Exception ( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes , len ( data_folders ))) print ( data_folders ) return data_folders train_folders = maybe_extract ( train_filename ) test_folders = maybe_extract ( test_filename ) ./datasets/notMNIST_large already present - Skipping extraction of ./datasets/notMNIST_large.tar.gz. ['./datasets/notMNIST_large/A', './datasets/notMNIST_large/B', './datasets/notMNIST_large/C', './datasets/notMNIST_large/D', './datasets/notMNIST_large/E', './datasets/notMNIST_large/F', './datasets/notMNIST_large/G', './datasets/notMNIST_large/H', './datasets/notMNIST_large/I', './datasets/notMNIST_large/J'] ./datasets/notMNIST_small already present - Skipping extraction of ./datasets/notMNIST_small.tar.gz. ['./datasets/notMNIST_small/A', './datasets/notMNIST_small/B', './datasets/notMNIST_small/C', './datasets/notMNIST_small/D', './datasets/notMNIST_small/E', './datasets/notMNIST_small/F', './datasets/notMNIST_small/G', './datasets/notMNIST_small/H', './datasets/notMNIST_small/I', './datasets/notMNIST_small/J'] Problem 1 - Sample some images in dataset and render them Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display. For each character folder in dataset, randomly choose one picture in it and render them horizontally Keypoints: use os.listdir() to get list of file in a folder use mpl.image.imread to read in image as ndarray, and use plt.imshow to render ndarray as images ! ls ./datasets/notMNIST_small/ A B.pickle D E.pickle G H.pickle J A.pickle C D.pickle F G.pickle I J.pickle B C.pickle E F.pickle H I.pickle characters = 'abcdefghij' . upper () # sub folders to choose images from image_per_folder = 4 # number of images to show for each folder BASE_PATH = './datasets/notMNIST_small/' list_of_images = [] for _ in range ( image_per_folder ): for char in characters : char_folder = BASE_PATH + char + '/' images = os . listdir ( char_folder ) image_file_name = images [ np . random . randint ( len ( images ))] list_of_images . append ( char_folder + image_file_name ) def showImagesHorizontally ( list_of_files ): from matplotlib.pyplot import figure , imshow , axis from matplotlib.image import imread number_of_files = len ( list_of_files ) num_char = len ( characters ) for row in range ( int ( number_of_files / num_char )): fig = figure ( figsize = ( 15 , 5 )) for i in range ( num_char ): a = fig . add_subplot ( 1 , num_char , i + 1 ) image = imread ( list_of_files [ row * num_char + i ]) imshow ( image , cmap = 'gray' ) axis ( 'off' ) showImagesHorizontally ( list_of_images ) Data curation drop unreadable images normalization pickle the normalized data by characters / folders Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size. We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. A few images might not be readable, we'll just skip them. Debugging In case of following error occur: ImportError : Could not import the Python Imaging Library ( PIL ) required to load image files . Follow these steps: pip install pillow replace from IPython.display import display , Image to from IPython.display import display from PIL import Image Keypoints: Normalize image using image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth where pixel_depth == 255 . References https://stackoverflow.com/questions/41124353/importerror-could-not-import-the-python-imaging-library-pil-required-to-load image_size = 28 # Pixel width and height. pixel_depth = 255.0 # Number of levels per pixel. def load_letter ( folder , min_num_images ): \"\"\"Load the data for a single letter label.\"\"\" image_files = os . listdir ( folder ) dataset = np . ndarray ( shape = ( len ( image_files ), image_size , image_size ), dtype = np . float32 ) print ( folder ) num_images = 0 for image in image_files : image_file = os . path . join ( folder , image ) try : image_data = ( ndimage . imread ( image_file ) . astype ( float ) - pixel_depth / 2 ) / pixel_depth if image_data . shape != ( image_size , image_size ): raise Exception ( 'Unexpected image shape: %s ' % str ( image_data . shape )) dataset [ num_images , :, :] = image_data num_images = num_images + 1 except IOError as e : print ( 'Could not read:' , image_file , ':' , e , '- it \\' s ok, skipping.' ) dataset = dataset [ 0 : num_images , :, :] if num_images < min_num_images : raise Exception ( 'Many fewer images than expected: %d < %d ' % ( num_images , min_num_images )) print ( 'Full dataset tensor:' , dataset . shape ) print ( 'Mean:' , np . mean ( dataset )) print ( 'Standard deviation:' , np . std ( dataset )) return dataset def maybe_pickle ( data_folders , min_num_images_per_class , force = False ): dataset_names = [] for folder in data_folders : set_filename = folder + '.pickle' dataset_names . append ( set_filename ) if os . path . exists ( set_filename ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping pickling.' % set_filename ) else : print ( 'Pickling %s .' % set_filename ) dataset = load_letter ( folder , min_num_images_per_class ) try : with open ( set_filename , 'wb' ) as f : pickle . dump ( dataset , f , pickle . HIGHEST_PROTOCOL ) except Exception as e : print ( 'Unable to save data to' , set_filename , ':' , e ) return dataset_names train_datasets = maybe_pickle ( train_folders , 45000 ) test_datasets = maybe_pickle ( test_folders , 1800 ) ./datasets/notMNIST_large/A.pickle already present - Skipping pickling. ./datasets/notMNIST_large/B.pickle already present - Skipping pickling. ./datasets/notMNIST_large/C.pickle already present - Skipping pickling. ./datasets/notMNIST_large/D.pickle already present - Skipping pickling. ./datasets/notMNIST_large/E.pickle already present - Skipping pickling. ./datasets/notMNIST_large/F.pickle already present - Skipping pickling. ./datasets/notMNIST_large/G.pickle already present - Skipping pickling. ./datasets/notMNIST_large/H.pickle already present - Skipping pickling. ./datasets/notMNIST_large/I.pickle already present - Skipping pickling. ./datasets/notMNIST_large/J.pickle already present - Skipping pickling. ./datasets/notMNIST_small/A.pickle already present - Skipping pickling. ./datasets/notMNIST_small/B.pickle already present - Skipping pickling. ./datasets/notMNIST_small/C.pickle already present - Skipping pickling. ./datasets/notMNIST_small/D.pickle already present - Skipping pickling. ./datasets/notMNIST_small/E.pickle already present - Skipping pickling. ./datasets/notMNIST_small/F.pickle already present - Skipping pickling. ./datasets/notMNIST_small/G.pickle already present - Skipping pickling. ./datasets/notMNIST_small/H.pickle already present - Skipping pickling. ./datasets/notMNIST_small/I.pickle already present - Skipping pickling. ./datasets/notMNIST_small/J.pickle already present - Skipping pickling. Problem 2 Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot. Load test data and show normalized images for each pickled dataset notMNIST_small only in order to prevent memory-insufficient problem when load all train data Keypoints: use pickle.load(open(file_path, 'rb')) to load a pickle create a figure obj and use fig.add_subplot(1, len(images), i+1) to add subplot for each image render ndarray represent images using matplotlob.pyplot.imshow(image) images = [] for file_path in test_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) images . append ( data [ 0 , :, :]) from matplotlib.pyplot import figure , imshow , axis fig = figure ( figsize = ( 15 , 5 )) for i , image in enumerate ( images ): a = fig . add_subplot ( 1 , len ( images ), i + 1 ) imshow ( image , cmap = 'gray' ) axis ( 'off' ) Number of samples in ./datasets/notMNIST_small/A.pickle: 1872 Number of samples in ./datasets/notMNIST_small/B.pickle: 1873 Number of samples in ./datasets/notMNIST_small/C.pickle: 1873 Number of samples in ./datasets/notMNIST_small/D.pickle: 1873 Number of samples in ./datasets/notMNIST_small/E.pickle: 1873 Number of samples in ./datasets/notMNIST_small/F.pickle: 1872 Number of samples in ./datasets/notMNIST_small/G.pickle: 1872 Number of samples in ./datasets/notMNIST_small/H.pickle: 1872 Number of samples in ./datasets/notMNIST_small/I.pickle: 1872 Number of samples in ./datasets/notMNIST_small/J.pickle: 1872 Problem 3 We expect the data to be balanced across classes. By problem 2 above, we already see the data is balanced in test set, verify data is balanced in train data as well for file_path in train_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) Number of samples in ./datasets/notMNIST_large/A.pickle: 52909 Number of samples in ./datasets/notMNIST_large/B.pickle: 52911 Number of samples in ./datasets/notMNIST_large/C.pickle: 52912 Number of samples in ./datasets/notMNIST_large/D.pickle: 52911 Number of samples in ./datasets/notMNIST_large/E.pickle: 52912 Number of samples in ./datasets/notMNIST_large/F.pickle: 52912 Number of samples in ./datasets/notMNIST_large/G.pickle: 52912 Number of samples in ./datasets/notMNIST_large/H.pickle: 52912 Number of samples in ./datasets/notMNIST_large/I.pickle: 52912 Number of samples in ./datasets/notMNIST_large/J.pickle: 52911 Merge seperate character dataset together (for all training/test/valid) Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune train_size as needed. The labels will be stored into a separate array of integers 0 through 9. Also create a validation dataset for hyperparameter tuning. Keypoints: Why we need vaild/test set? Our goal is to make sure the model we train can generalize to brand-new data. If we only divide the whole dataset into training/test set and try to get the best model by tuning parameters on test set, chances are that we actully give the model 'hints' on training set by our eyes. Because eventually, the best model will use the parameters incorporating our knowledge about training dataset, and when the very brand-new data comes, our model can't actually generalize to it and make wrong predictions. So we have to use vaild/dev set to tune our model, and only test the performance on test set to simulate model's 'real-world' performance after deploying it. def make_arrays ( nb_rows , img_size ): if nb_rows : dataset = np . ndarray (( nb_rows , img_size , img_size ), dtype = np . float32 ) labels = np . ndarray ( nb_rows , dtype = np . int32 ) else : dataset , labels = None , None return dataset , labels def merge_datasets ( pickle_files , train_size , valid_size = 0 ): num_classes = len ( pickle_files ) valid_dataset , valid_labels = make_arrays ( valid_size , image_size ) train_dataset , train_labels = make_arrays ( train_size , image_size ) vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v , start_t = 0 , 0 end_v , end_t = vsize_per_class , tsize_per_class end_l = vsize_per_class + tsize_per_class for label , pickle_file in enumerate ( pickle_files ): try : with open ( pickle_file , 'rb' ) as f : letter_set = pickle . load ( f ) # let's shuffle the letters to have random validation and training set np . random . shuffle ( letter_set ) if valid_dataset is not None : valid_letter = letter_set [: vsize_per_class , :, :] valid_dataset [ start_v : end_v , :, :] = valid_letter valid_labels [ start_v : end_v ] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set [ vsize_per_class : end_l , :, :] train_dataset [ start_t : end_t , :, :] = train_letter train_labels [ start_t : end_t ] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e : print ( 'Unable to process data from' , pickle_file , ':' , e ) raise return valid_dataset , valid_labels , train_dataset , train_labels train_size = 200000 valid_size = 10000 test_size = 10000 valid_dataset , valid_labels , train_dataset , train_labels = merge_datasets ( train_datasets , train_size , valid_size ) _ , _ , test_dataset , test_labels = merge_datasets ( test_datasets , test_size ) print ( 'Training:' , train_dataset . shape , train_labels . shape ) print ( 'Validation:' , valid_dataset . shape , valid_labels . shape ) print ( 'Testing:' , test_dataset . shape , test_labels . shape ) Training: (200000, 28, 28) (200000,) Validation: (10000, 28, 28) (10000,) Testing: (10000, 28, 28) (10000,) Randomize data Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match. The purpose of randomizing data is to perserve the assumption that we get the data randomly as predict phase. We don't want to train models sequantially on training instances like AAA ... BBB ... CCC. Keypoints: train set and test set should apply the same shuffle shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] Although we do only one randomization here, we may do it multiple times when we are going to use the same dataset many times. def randomize ( dataset , labels ): permutation = np . random . permutation ( labels . shape [ 0 ]) shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] return shuffled_dataset , shuffled_labels train_dataset , train_labels = randomize ( train_dataset , train_labels ) test_dataset , test_labels = randomize ( test_dataset , test_labels ) valid_dataset , valid_labels = randomize ( valid_dataset , valid_labels ) Problem 4 - Sanity check after shuffling dataset Convince yourself that the data is still good after shuffling! Randomly sample data instances to make sure X is corresponding to y for both training and test set after shuffling. Keypoints: label is start from 0 to 9 for A to J lookup_labels = { k : v for ( k , v ) in zip ( np . arange ( 10 ), 'ABCDEFGHIJ' )} def sanity_check ( X , y , s = None ): print ( s ) m1 , m2 = X . shape [ 0 ], y . shape [ 0 ] assert ( m1 == m2 ) # randomly choose 10 images to check label indices = np . random . randint ( 0 , m1 , size = 10 ) fig = plt . figure ( figsize = ( 15 , 5 )) for i , idx in enumerate ( indices ): fig . add_subplot ( 1 , len ( indices ), i + 1 ) plt . imshow ( X [ idx , :, :], cmap = 'gray' ) plt . title ( lookup_labels [ y [ idx ]]) plt . axis ( 'off' ) sanity_check ( train_dataset , train_labels , 'Training dataset:' ) Training dataset: sanity_check ( test_dataset , test_labels , 'Test dataset:' ) Test dataset: Serialize dataset for later usage Everything looks good, finally, let's save the data for later reuse: pickle_file = os . path . join ( data_root , 'notMNIST.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : valid_dataset , 'valid_labels' : valid_labels , 'test_dataset' : test_dataset , 'test_labels' : test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 690800503 Problem 5 - Remove overlapping samples in test/valid set By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples. Optional questions: What about near duplicates between datasets? (images that are almost identical) Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments. Find duplicate images in test set I used Cosine Similarity to measure whether image A in training set is identical to those image B in valid/test set. As formula suggested, I reshape every image in training set into column vectors, and do the same for images in valid/test set and compute similarity between the vectors. Although we can compute cosine similarity for every (train image vector, test image vector) explicitly, it's better to use vectorization to speed up computation since that we have 200,000 training images and 10,000 valid/test images. (Although it still take about 10 minutes to run in my computer) Keypoints: Vectorize both training/test images and compute cosine similarity using cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y)) . The output matrix cosine_sim will be shape (m1, m2) where m1 is the number of training images and m2 the number of test images. cosine_matrix(i, j) mean the cosine similarity between training image #i and test image #j. %% time def get_duplicate_data(source_dataset, target_dataset, threshold=1, num_duplicate_to_show=0): X = source_dataset.reshape(source_dataset.shape[0], -1) Y = target_dataset.reshape(target_dataset.shape[0], -1) assert(X.shape[1] == Y.shape[1]) dim = X.shape[1] cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y)) assert(cosine_sim.shape == (X.shape[0], Y.shape[0])) # for each image in training set, find corresponding duplicate in test/valid set dup_target_indices = [] show_duplicate_counter = 0 for source_idx in range(cosine_sim.shape[0]): dup_indices = list(np.where(cosine_sim[source_idx, :] >= threshold)[0]) # render duplicate images when is available. may omit if visual output is not required if dup_indices and num_duplicate_to_show and (show_duplicate_counter < num_duplicate_to_show): # show only non-redudent duplicate images for i in dup_indices: if i in dup_target_indices: dup_indices.remove(i) if not dup_indices: continue if len(dup_indices) == 1: fig = plt.figure(figsize=(3, 15)) fig.add_subplot(1, len(dup_indices) + 1, 1) plt.imshow(source_dataset[source_idx, :, :], cmap='gray') plt.title('Source: ' + str(source_idx)) plt.axis('off') for i, target_idx in enumerate(dup_indices): fig.add_subplot(1, len(dup_indices) + 1, i + 2) plt.imshow(target_dataset[target_idx, :, :], cmap='gray') plt.title('Target: ' + str(target_idx)) plt.axis('off') show_duplicate_counter += 1 dup_target_indices.extend(dup_indices) return list(set(dup_target_indices)) dup_indices_test = get_duplicate_data(train_dataset, test_dataset, num_duplicate_to_show=5) print('Number of duplicates in test dataset: {}'.format(len(dup_indices_test))) Number of duplicates in test dataset: 1768 CPU times: user 5min 11s, sys: 4min 17s, total: 9min 29s Wall time: 7min 40s Duplicate images in validation set %% time dup_indices_valid = get_duplicate_data(train_dataset, valid_dataset, num_duplicate_to_show=5) print('Number of duplicates in validation dataset: {}'.format(len(dup_indices_valid))) Number of duplicates in validation dataset: 1507 CPU times: user 7min 52s, sys: 5min 7s, total: 12min 59s Wall time: 8min 57s Serialize sanitized dataset for later model performance comparison Remove duplicates in test/valid set Save dataset Keypoints: Do the same operation to both X and y print ( \"Number of duplicate images in test set: {} \" . format ( len ( dup_indices_test ))) print ( \"Number of duplicate images in valid set: {} \" . format ( len ( dup_indices_valid ))) Number of duplicate images in test set: 1768 Number of duplicate images in valid set: 1507 non_duplicate_indices = [ i for i in range ( test_dataset . shape [ 0 ]) if not i in dup_indices_test ] sanitized_test_dataset = test_dataset [ non_duplicate_indices , :, :] sanitized_test_labels = test_labels [ non_duplicate_indices ] non_duplicate_indices = [ i for i in range ( valid_dataset . shape [ 0 ]) if not i in dup_indices_valid ] sanitized_valid_dataset = valid_dataset [ non_duplicate_indices , :, :] sanitized_valid_labels = valid_labels [ non_duplicate_indices ] pickle_file = os . path . join ( data_root , 'notMNIST_sanitized.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : sanitized_valid_dataset , 'valid_labels' : sanitized_valid_labels , 'test_dataset' : sanitized_test_dataset , 'test_labels' : sanitized_test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 680517003 Problem 6 - Build naive classifier using logistic regression Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it. Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model. Optional question: train an off-the-shelf model on all the data! from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score plt . style . use ( 'ggplot' ) np . random . seed ( 42 ) train_sizes = [ 100 , 1000 , 50000 , 100000 , 200000 ] # train models using different size of training set test_scores , test_scores_sanitized = [[] for _ in range ( 2 )] for train_size in train_sizes : # random choose #train_size of training instances indices = np . random . randint ( 0 , train_dataset . shape [ 0 ], train_size ) # reshape images to (train_size, dim * dim) for easier processing X = train_dataset [ indices , :, :] \\ . reshape ( - 1 , train_dataset . shape [ 1 ] * train_dataset . shape [ 2 ]) y = train_labels [ indices ] # train model clf = ( LogisticRegression ( random_state = 10 , solver = 'lbfgs' , multi_class = 'multinomial' ) . fit ( X , y )) # test on original test set and the sanitized one y_pred = clf . predict ( test_dataset . reshape ( test_dataset . shape [ 0 ], - 1 )) y_pred_sanitized = clf . predict ( sanitized_test_dataset . reshape ( sanitized_test_dataset . shape [ 0 ], - 1 )) test_score = accuracy_score ( y_pred , test_labels ) test_score_sanitized = accuracy_score ( y_pred_sanitized , sanitized_test_labels ) test_scores . append ( test_score ) test_scores_sanitized . append ( test_score_sanitized ) # print(classification_report(test_labels, y_pred)) # print(accuracy_score(test_labels, y_pred)) plt . figure ( figsize = ( 7 , 7 )) plt . xlabel ( 'Training size' , fontsize = 20 ) plt . ylabel ( 'Accuracy' , fontsize = 20 ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) for x , y in zip ( train_sizes , test_scores ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) for x , y in zip ( train_sizes , test_scores_sanitized ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) plt . scatter ( train_sizes , test_scores , label = 'Test score' , color = 'blue' ); plt . scatter ( train_sizes , test_scores_sanitized , label = 'Test score(Sanitized)' , color = 'red' ); plt . legend ( loc = 4 ) plt . title ( 'Test set Accuracy vs Training size' , fontsize = 25 ); As show above, there are seveal things worth mentioning: Our model became better on classifiying labels in test set by using more data in training phase. By training a simple logistic regression model on all available training data, we can expect to get about 90% accuracy. There is a performance gap (2~3% on accuracy) between test set with duplicate images and the one without. Depends on our application, choose the one which best fit our use case. Notice that we just want to have a off-the-shelf model quickly, so we don't even tune hyper-parameters using validation set. We may be able to further improve model's predictive performance by tuning hyper-parameters or using Neural Network in another notebook.","tags":"Machine learning","url":"https://leemengtaiwan.github.io/simple-image-recognition-using-notmnist-dataset.html"},{"title":"Purpose of this blog","text":"第一篇文章做一點 blog 的簡介，打算把自己在學 data science 還有 machine learning 過程中寫的筆記還有在 MOOC 上課的 code (主要是 jupyter notebook) 記錄下來方便自己以後搜尋。 雖然目前為止我都將 jupyter notebook render 成 HTML 然後存到 Evernote 搜尋, 但是如果該 notebook 一直更新的話就變得很不實際.. </br> 這次利用 python 的 pelican 將 jupyter notebook 轉成靜態網頁, 讓我可以不斷更新 notebooks 而且也希望 code 可以幫助到其他也在做 data science / machine learning 的人當作一些參考。 blog 的走向目前看來應該會是中英夾雜 .. This is the first post of my blog. </br> I decided to record my learning path toward data scientist / machine learning engineer and make it easier for my future self to review what I have learnt. This blog will include codes from the courses on MOOC like Coursera, Udacity and also some pet projects. It would also be wonderful if these code and thought can help someone who want to become a data scientist.","tags":"Miscellaneous","url":"https://leemengtaiwan.github.io/purpose-of-this-blog.html"}]}