<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 進擊的 BERT：運用自然語言處理的巨人之力</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="介紹目前自然語言處理領域中非常熱門的語言代表模型 BERT 以及遷移學習的運作方式。本文將透過一個假新聞分類問題，以 Pytorch 向讀者展示如何將強大的語言代表模型運用到自己有興趣的 NLP 任務之上，從而飛得更快更遠。" />
        <meta name="keywords" content="自然語言處理, NLP, Pytorch" />
        <meta name="tags" content="自然語言處理" />
        <meta name="tags" content="NLP" />
        <meta name="tags" content="Pytorch" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/attack_on_bert.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" />
        <meta property="og:title" content="進擊的 BERT：運用自然語言處理的巨人之力" />
        <meta property="og:description" content="介紹目前自然語言處理領域中非常熱門的語言代表模型 BERT 以及遷移學習的運作方式。本文將透過一個假新聞分類問題，以 Pytorch 向讀者展示如何將強大的語言代表模型運用到自己有興趣的 NLP 任務之上，從而飛得更快更遠。" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <!--for responsive embed youtube video-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/embed_youtube.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="進擊的 BERT：運用自然語言處理的巨人之力">進擊的 BERT：運用自然語言處理的巨人之力</a><ul><li><a class="toc-href" href="#BERT：理解上下文的語言代表模型" title="BERT：理解上下文的語言代表模型">BERT：理解上下文的語言代表模型</a><ul><li><a class="toc-href" href="#Load-chinese-BERT" title="Load chinese BERT">Load chinese BERT</a></li><li><a class="toc-href" href="#Sentence-finetune-model" title="Sentence finetune model">Sentence finetune model</a></li><li><a class="toc-href" href="#Compare-custom-loss-to-predefined-loss" title="Compare custom loss to predefined loss">Compare custom loss to predefined loss</a></li><li><a class="toc-href" href="#Attentionmap-&amp;-BertViz" title="Attentionmap &amp; BertViz">Attentionmap &amp; BertViz</a></li><li><a class="toc-href" href="#Download-raw-data-and-sample" title="Download raw data and sample">Download raw data and sample</a></li><li><a class="toc-href" href="#Load-tsv-as-pytorch-dataset" title="Load tsv as pytorch dataset">Load tsv as pytorch dataset</a></li><li><a class="toc-href" href="#直接預測" title="直接預測">直接預測</a></li><li><a class="toc-href" href="#Loss-function-&amp;-Optimizer" title="Loss function &amp; Optimizer">Loss function &amp; Optimizer</a></li><li><a class="toc-href" href="#Finetune-Train-the-network" title="Finetune Train the network">Finetune Train the network</a></li><li><a class="toc-href" href="#Feature-extraction" title="Feature extraction">Feature extraction</a></li><li><a class="toc-href" href="#產生-test-結果" title="產生 test 結果">產生 test 結果</a></li></ul></li><li><a class="toc-href" href="#Conclusion_1" title="Conclusion">Conclusion</a><ul><li><a class="toc-href" href="#Some-further-resources" title="Some further resources">Some further resources</a></li></ul></li><li><a class="toc-href" href="#Part-III:-Fine-tuning-a-pre-trained-model_2" title="Part III: Fine-tuning a pre-trained model">Part III: Fine-tuning a pre-trained model</a><ul><li><a class="toc-href" href="#The-Transformer" title="The Transformer">The Transformer</a><ul><li><a class="toc-href" href="#A-Transformer-Encoder-Layer" title="A Transformer Encoder Layer">A Transformer Encoder Layer</a></li><li><a class="toc-href" href="#Multi-headed-Attention" title="Multi-headed Attention">Multi-headed Attention</a><ul><li><a class="toc-href" href="#Scaled-dot-product-attention" title="Scaled dot product attention">Scaled dot product attention</a></li><li><a class="toc-href" href="#The-Multi-head-part" title="The Multi-head part">The Multi-head part</a></li><li><a class="toc-href" href="#Multi-Headed-Attention-is-easy-now-in-PyTorch!!" title="Multi-Headed Attention is easy now in PyTorch!!">Multi-Headed Attention is easy now in PyTorch!!</a></li></ul></li><li><a class="toc-href" href="#Positional-embeddings_1" title="Positional embeddings">Positional embeddings</a></li></ul></li><li><a class="toc-href" href="#BERT_1" title="BERT">BERT</a><ul><li><a class="toc-href" href="#Tokenization-in-BERT" title="Tokenization in BERT">Tokenization in BERT</a></li><li><a class="toc-href" href="#Bert-tokenizer-/-vectorizer" title="Bert tokenizer / vectorizer">Bert tokenizer / vectorizer</a></li><li><a class="toc-href" href="#FineTuneClassifier" title="FineTuneClassifier">FineTuneClassifier</a></li><li><a class="toc-href" href="#ConfusionMatrix" title="ConfusionMatrix">ConfusionMatrix</a></li><li><a class="toc-href" href="#Trainer" title="Trainer">Trainer</a></li><li><a class="toc-href" href="#Evaluator" title="Evaluator">Evaluator</a></li><li><a class="toc-href" href="#fit()" title="fit()">fit()</a></li><li><a class="toc-href" href="#Train" title="Train">Train</a></li></ul></li></ul></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/attack_on_bert.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                            <a href="https://leemeng.tw/tag/nlp.html" rel="tag">NLP</a>
                            <a href="https://leemeng.tw/tag/pytorch.html" rel="tag">Pytorch</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" title="">
                            進擊的 BERT：運用自然語言處理的巨人之力
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-07-08 (Mon)</li>
                        <li class="page-view">
                            0 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>

    <div id="social-wrap" style="cursor: pointer">
        <a class="open-popup" title="訂閱最新文章">
            <i class="im im-newspaper-o" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" target="_blank" title="分享到 Facebook">
            <i class="im im-facebook" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html&title=%E9%80%B2%E6%93%8A%E7%9A%84%20BERT%EF%BC%9A%E9%81%8B%E7%94%A8%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%B7%A8%E4%BA%BA%E4%B9%8B%E5%8A%9B&summary=%E4%BB%8B%E7%B4%B9%E7%9B%AE%E5%89%8D%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E9%A0%98%E5%9F%9F%E4%B8%AD%E9%9D%9E%E5%B8%B8%E7%86%B1%E9%96%80%E7%9A%84%E8%AA%9E%E8%A8%80%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B%20BERT%20%E4%BB%A5%E5%8F%8A%E9%81%B7%E7%A7%BB%E5%AD%B8%E7%BF%92%E7%9A%84%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F%E3%80%82%E6%9C%AC%E6%96%87%E5%B0%87%E9%80%8F%E9%81%8E%E4%B8%80%E5%80%8B%E5%81%87%E6%96%B0%E8%81%9E%E5%88%86%E9%A1%9E%E5%95%8F%E9%A1%8C%EF%BC%8C%E4%BB%A5%20Pytorch%20%E5%90%91%E8%AE%80%E8%80%85%E5%B1%95%E7%A4%BA%E5%A6%82%E4%BD%95%E5%B0%87%E5%BC%B7%E5%A4%A7%E7%9A%84%E8%AA%9E%E8%A8%80%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B%E9%81%8B%E7%94%A8%E5%88%B0%E8%87%AA%E5%B7%B1%E6%9C%89%E8%88%88%E8%B6%A3%E7%9A%84%20NLP%20%E4%BB%BB%E5%8B%99%E4%B9%8B%E4%B8%8A%EF%BC%8C%E5%BE%9E%E8%80%8C%E9%A3%9B%E5%BE%97%E6%9B%B4%E5%BF%AB%E6%9B%B4%E9%81%A0%E3%80%82&source=https%3A//leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" target="_blank" title="分享到 LinkedIn">
            <i class="im im-linkedin" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://twitter.com/intent/tweet?text=%E9%80%B2%E6%93%8A%E7%9A%84%20BERT%EF%BC%9A%E9%81%8B%E7%94%A8%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%B7%A8%E4%BA%BA%E4%B9%8B%E5%8A%9B&url=https%3A//leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html&hashtags=zi-ran-yu-yan-chu-li,nlp,pytorch" target="_blank" title="分享到 Twitter">
            <i class="im im-twitter" aria-hidden="true"></i>
        </a>
    </div>


    <!--custom images with icon shown on left nav-->

</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這篇文章帶你了解並實際運用現在 NLP 領域的巨人之力：BERT 模型。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果你還有印象，在<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">自然語言處理與深度學習入門指南</a>裡我使用了 LSTM 以及 Google 的語言代表模型 <a href="https://github.com/google-research/bert">BERT</a> 來分類中文假新聞。說來有趣，因為 BERT 本身的強大，我不費吹灰之力就在<a href="https://www.kaggle.com/c/fake-news-pair-classification-challenge/leaderboard">該 Kaggle 競賽</a>達到 85 % 的正確率，距離第一名 3 %，總排名前 30 %。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/nlp-kaggle-intro/kaggle-final-result.png"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>雖說如此，使用 BERT 一直不是那麼直觀的事情。最近適逢 <a href="https://pytorch.org/hub">PyTorch Hub</a> 上架 <a href="https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/">BERT</a>，李宏毅教授的<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html">機器學習課程</a>也推出了 <a href="https://github.com/openai/gpt-2">BERT 的教學影片</a>，我認為現在正是了解並<strong>實際運用</strong> BERT 的最佳時機！</p>
<p>閱讀完這篇文章，你也能跟我一樣運用強大的 BERT，而且是以最直覺、簡單的方式。你會了解目前 NLP 領域非常熱門的<a href="https://docs.google.com/presentation/d/1DJI1yX4U5IgApGwavt0AmOCLWwso7ou1Un93sMuAWmA/edit?usp=sharing">遷移學習（Transfer Learning）</a>技術，並實際運用這股巨人之力來解決你自己有興趣的自然語言任務。我在文中也會提供一些有趣的研究及應用 ，讓你之後可以進一步探索這個變化快速的 NLP 世界。</p>
<p>我們等等會簡單回顧 BERT 裡的一些重要概念，但如果你完全不熟 NLP 或是壓根子沒聽過什麼是 BERT，我強力建議你之後找時間（或是現在！）觀看李宏毅教授說明 <a href="https://allennlp.org/elmo">ELMo</a>、BERT 以及 <a href="https://github.com/openai/gpt-2">GPT</a> 等模型的影片，淺顯易懂：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="resp-container">
<iframe allow="accelerometer; 
                            autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" class="resp-iframe" frameborder="0" src="https://www.youtube-nocookie.com/embed/UYPa347-DdE">
</iframe>
</div>
<center>
                        李宏毅教授講解目前 NLP 領域的最新研究是如何讓機器讀懂文字的（我超愛這截圖）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BERT：理解上下文的語言代表模型">BERT：理解上下文的語言代表模型<a class="anchor-link" href="#BERT：理解上下文的語言代表模型">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>一個簡單 convention，等等文中會穿插使用的：</p>
<ul>
<li>代表</li>
<li>representation</li>
<li>repr.</li>
<li>repr. 向量</li>
</ul>
<p>指的都是一個可以用來<strong>代表</strong>某詞彙（在某個語境下）的多維連續向量（continuous vector）。</p>
<p>現在在 NLP 圈混的，應該沒有人會說自己不曉得 Transformer 的<a href="https://arxiv.org/abs/1706.03762">經典論文 Attention Is All You Need</a> 以及其知名的<a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Encoder-Decoder-%E6%A8%A1%E5%9E%8B-+-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6">自注意力機制（Self-attention mechanism）</a>。<a href="https://arxiv.org/abs/1810.04805">BERT</a> 全名為 <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers，是 Google 以無監督的方式利用大量無標註文本「煉成」的<strong>語言模型</strong>，其架構為 Transformer 中的 Encoder。</p>
<p>我在<a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html">淺談神經機器翻譯 &amp; 用 Transformer 英翻中</a>一文已經鉅細靡遺地解說過所有 Transformer 的相關概念，這邊就不再贅述。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/bert/bert-intro.jpg"/>
</center>
<center>
                        BERT 其實就是 Transformer 中的 Encoder，只是很多層
                        （<a href="https://youtu.be/UYPa347-DdE?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>BERT 是一種語言模型，而<a href="https://youtu.be/iWea12EAu6U">語言模型（<strong>L</strong>anguage <strong>M</strong>odel, LM）</a>做的事情就是在給定一些詞彙的前提下， 去估計下一個詞彙出現的機率分佈。在<a href="https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html">讓 AI 寫點金庸</a>裡的 LSTM 也是一個語言模型 ，只是跟 BERT 差了很多個數量級。</p>
<p>為何會想要訓練一個語言模型（LM）？因為：</p>
<ul>
<li>不像 <a href="http://www.image-net.org/">ImageNet</a> 還要找人標注數據，要訓練 LM 的話網路上的所有文本都是你的潛在資料集，數據無限大（BERT 預訓練使用的數據集共有 33 <strong>億</strong>個字，其中包含維基百科及 <a href="https://arxiv.org/abs/1506.06724">BooksCorpus</a>）</li>
<li>厲害的 LM 能夠學會語法結構、解讀語義甚至<a href="http://ckip.iis.sinica.edu.tw/project/coreference/">指代消解</a>。透過特徵擷取或是 fine-tuning 能更有效率地訓練下游任務並提升其表現</li>
</ul>
<p>這就是近來 NLP 領域非常流行的兩階段做法：</p>
<ul>
<li>先以無監督方式預訓練一個巨大的 LM</li>
<li>再將該 LM 拿來做特徵擷取或是 fine-tuning 下游有標註數據的任務</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/bert/lm-equation.jpg"/>
</center>
<center>
                        給定前 t 個在字典裡的詞彙，語言模型要去估計第 t + 1 個詞彙的機率分佈 P
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>當然天下沒有白吃的午餐。</p>
<p>要訓練好一個有 1.1 億參數的 12 層 <strong>BERT-BASE</strong> 得用 16 個 <a href="https://cloudplatform.googleblog.com/2018/06/Cloud-TPU-now-offers-preemptible-pricing-and-global-availability.html">TPU chips</a> 跑上整整 4 天，<a href="https://medium.com/syncedreview/the-staggering-cost-of-training-sota-ai-models-e329e80fa82">花費 500 鎂</a>；24 層的 <strong>BERT-LARGE</strong> 則有 3.4 億個參數，得用 64 個 TPU chips（約 7000 鎂）訓練。喔對了，別忘了多次實驗得把這些成本乘上幾倍。</p>
<p>值得慶幸的是作者們有釋出訓練好的模型。因此只要使用 <a href="https://github.com/google-research/bert">TensorFlow</a> 或是 <a href="https://github.com/huggingface/pytorch-pretrained-BERT">PyTorch</a> 將已訓練好的 BERT 載入，就能省去預訓練的所有成本，並馬上使用強大的 BERT。</p>
<p>讓我們簡單瞭解一下 BERT 是怎麼訓練出來的。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/bert/bert-pretrain-tasks.jpg"/>
</center>
<center>
                        BERT 在預訓練時需要完成的兩個任務
                        （<a href="https://youtu.be/UYPa347-DdE?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4" target="_blank">圖片來源</a>）
                        
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Google 在預訓練 BERT 時讓它<strong>同時</strong>進行兩個任務：</p>
<ul>
<li>克漏字填空（<a href="https://journals.sagepub.com/doi/abs/10.1177/107769905303000401">1953 年被提出的 Cloze task</a>）</li>
<li>判斷第 2 個句子本來是否跟第 1 個句子相接（<strong>N</strong>ext <strong>S</strong>entence <strong>P</strong>rediction, NSP）</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>對上通天文下知地理的鄉民們來說，要完成這兩個任務簡單到爆。只要稍微看一下<strong>前後文</strong>就能知道左邊克漏字任務的 <code>[MASK]</code> 裡頭該填 <code>退了</code>；而 <code>醒醒吧</code> 後面接 <code>你沒有妹妹</code> 也十分合情合理。</p>
<p>多說無益，這是一篇 BERT 實用文章，因此讓我們馬上載入 <a href="https://pytorch.org/hub">PyTorch Hub</a> 上的 <a href="https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/">BERT 模型</a>實際體驗看看。首先我們需要安裝一些簡單的函式庫：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip install tqdm boto3 requests regex -q
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們接著把中文 BERT 使用的 tokenizer 載入：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">clear_output</span>

<span class="n">GITHUB_REPO</span> <span class="o">=</span> <span class="s2">"huggingface/pytorch-pretrained-BERT"</span> <span class="c1"># 感謝 HuggingFace 團隊造福後人</span>
<span class="n">PRETRAINED_MODEL_NAME</span> <span class="o">=</span> <span class="s2">"bert-base-chinese"</span>  <span class="c1"># 指定繁簡中文 BERT-BASE 預訓練模型</span>

<span class="c1"># 取得此預訓練模型所使用的 tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">GITHUB_REPO</span><span class="p">,</span> <span class="s1">'bertTokenizer'</span><span class="p">,</span> <span class="n">PRETRAINED_MODEL_NAME</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>

<span class="c1"># 顯示字典資訊</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"字典大小："</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="c1"># tokenizer.vocab</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>字典大小： 21128
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>沒記錯的話，英文 BERT 的字典大小大約是 3 萬左右。我們可以瞧瞧中文 BERT 字典裡頭紀錄的一些 tokens 以及對應的索引：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_tokens</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">random_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">random_tokens</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">{0:20}{1:15}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">"token"</span><span class="p">,</span> <span class="s2">"index"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">25</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">random_tokens</span><span class="p">,</span> <span class="n">random_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">{0:15}{1:10}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">id</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>token               index          
-------------------------
抒                    2830
##ein               11858
##薯                 19018
result              13170
149                  9491
evolution           12691
mwc                 12184
##刃                 14202
kelly               11394
##换                 15997
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>BERT 使用當初 <a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html">Google NMT</a> 提出的 <a href="https://arxiv.org/abs/1609.08144">WordPiece Tokenization</a> ，將本來的 words 拆成更小粒度的 wordpieces，有效改善 <a href="https://en.wiktionary.org/wiki/OOV">OOV</a> 問題。中文的話大致上就像是 character-level tokenization，而有 <code>##</code> 前綴的 tokens 即為 wordpieces。</p>
<p>ㄅㄆㄇㄈ當然也是有被收錄的：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">647</span><span class="p">,</span> <span class="mi">657</span><span class="p">))</span>
<span class="n">some_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">t</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">some_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>('ㄅ', 647)
('ㄆ', 648)
('ㄇ', 649)
('ㄉ', 650)
('ㄋ', 651)
('ㄌ', 652)
('ㄍ', 653)
('ㄎ', 654)
('ㄏ', 655)
('ㄒ', 656)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外你也可以在 <a href="https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/hubconfs/bert_hubconf.py">Hugging Face 團隊的 repo </a> 看到所有可供使用的 BERT 預訓練模型。截至目前為止有以下模型可供使用：</p>
<ul>
<li>bert-base-chinese</li>
<li>bert-base-uncased</li>
<li>bert-base-cased</li>
<li>bert-base-german-cased</li>
<li>bert-base-multilingual-uncased</li>
<li>bert-base-multilingual-cased</li>
<li>bert-large-cased</li>
<li>bert-large-uncased</li>
<li>bert-large-uncased-whole-word-masking</li>
<li>bert-large-cased-whole-word-masking</li>
</ul>
<p>這些模型的主要差別在於預訓練時用的文本語言以及層數有所不同。常被拿來應用與研究的是英文的 <code>bert-base-cased</code>，但為了方便了解 BERT 運作，本文使用包含繁體與簡體中文的預訓練模型。</p>
<p>讓我們實際拿個中文句子來做斷詞看看：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 利用中文 BERT 的 tokenizer 將中文句子做 tokenization</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。"</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="s1">'...'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ids</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="s1">'...'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。
['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...
[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>除了一般的 wordpieces 以外，BERT 裡頭總共有 5 個特殊 tokens 各司其職：</p>
<ul>
<li><code>[CLS]</code>：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.</li>
<li><code>[SEP]</code>：兩個句子會合併成一個序列，中間插入這個 token 以做區隔</li>
<li><code>[UNK]</code>：沒出現在字典裡頭的字會被這個 token 取代</li>
<li><code>[PAD]</code>：padding 遮罩，將長度不一的序列補齊方便做 batch 運算</li>
<li><code>[MASK]</code>：未知遮罩，僅在預訓練階段會用到</li>
</ul>
<p>padding 遮罩在之前的 <a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3%E9%81%AE%E7%BD%A9%E5%9C%A8%E6%B3%A8%E6%84%8F%E5%87%BD%E5%BC%8F%E4%B8%AD%E7%9A%84%E6%95%88%E6%9E%9C">Transformer</a> 文章有詳細介紹，而 <code>[MASK]</code> token 一般在 fine-tuning 或是 feature extraction 時不會用到，這邊只是為了展示預訓練階段的克漏字任務才使用的。</p>
<p>現在馬上讓我們看看給定上面有 <code>[MASK]</code> 的句子，BERT 會填入什麼字：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">"""這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測"""</span>

<span class="c1"># 除了 tokens 以外我們還需要辨別句子的 segment ids</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ids</span><span class="p">])</span>  <span class="c1"># (1, seq_len)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">)</span>  <span class="c1"># (1, seq_len)</span>
<span class="n">maskedLM_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">GITHUB_REPO</span><span class="p">,</span> 
                                <span class="s1">'bertForMaskedLM'</span><span class="p">,</span> 
                                <span class="n">PRETRAINED_MODEL_NAME</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>

<span class="c1"># 使用 masked LM 估計 [MASK] 位置所代表的實際 token </span>
<span class="n">maskedLM_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">maskedLM_model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">)</span>
    <span class="c1"># (1, seq_len, num_hidden_units)</span>

<span class="c1"># 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來</span>
<span class="n">masked_index</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">probs</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">masked_index</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
<span class="n">predicted_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"輸入句子    ："</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="s1">'...'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">predicted_tokens</span><span class="p">,</span> <span class="n">probs</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">tokens</span><span class="p">[</span><span class="n">masked_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Top </span><span class="si">{}</span><span class="s2"> (</span><span class="si">{:2}</span><span class="s2">%)：</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">),</span> <span class="n">tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">]),</span> <span class="s1">'...'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>輸入句子    ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...
--------------------------------------------------
Top 1 (67%)：['[CLS]', '等', '到', '潮', '水', '來', '了', '，', '就', '知'] ...
Top 2 (25%)：['[CLS]', '等', '到', '潮', '水', '濕', '了', '，', '就', '知'] ...
Top 3 ( 2%)：['[CLS]', '等', '到', '潮', '水', '過', '了', '，', '就', '知'] ...
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Google 在訓練中文 BERT 鐵定沒看<a href="https://term.ptt.cc/">批踢踢</a>，還無法抓到鄉民們想要的那個 <code>退</code> 字。不過以語言模型的角度來看預測結果已經挺好的了。BERT 利用關注 <code>潮水</code> 這兩個字，從 2 萬多個 wordpieces 的字典裡頭估計該 <code>[MASK]</code> token 為 <code>來</code>，也還說的過去。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/bert/bert-attention.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是 <a href="https://github.com/jessevig/bertviz">BertViz</a> 視覺化 BERT 注意力的結果，我等等會列出安裝步驟讓你自己玩玩。值得一提的是，這是第 8 層的 <a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Multi-head-attention%EF%BC%9A%E4%BD%A0%E7%9C%8B%E4%BD%A0%E7%9A%84%EF%BC%8C%E6%88%91%E7%9C%8B%E6%88%91%E7%9A%84">Multi-head attention</a> 裡頭某一個 head 的注意力結果。</p>
<p>學會做克漏字讓 BERT 可以更好去 model 在不同語境下每個詞彙該有的 repr.，而 NSP 任務則能幫助 BERT model 兩個句子之間的關係，這在<a href="https://zh.wikipedia.org/wiki/%E5%95%8F%E7%AD%94%E7%B3%BB%E7%B5%B1">問答系統 QA</a>、<a href="http://nlpprogress.com/english/natural_language_inference.html">自然語言推論 NLI </a>或是任何包含兩個句子的分類任務都很有幫助。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這樣的 repr. 就是近年超級熱門的 <a href="https://youtu.be/S-CspeZ8FHc">contextual word representation</a> 概念，跟以往沒有蘊含上下文資訊的 <a href="https://youtu.be/8rXD5-xhemo">Word2Vec、GloVe</a> 等無語境的詞嵌入向量有很大的差異。用學術一點的說法就是：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        Contextual word repr. 讓同 word type 的 word token 在不同語境下有不同的表示方式；而傳統的 word repr. 無論上下文，都會讓同 word type 的 word token 的 repr. 相同。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>直覺上 contextual word representation 比較能反映人類語言的真實情況，畢竟同個詞彙在不同情境下的意涵相異是再正常不過的事情了。讓我再舉個具體例子：</p>
<div class="highlight"><pre><span></span>情境 1：

胖虎叫大雄去買漫畫，回來慢了就打他。

情境 2：

妹妹說胖虎是「胖子」，他聽了很不開心。
</pre></div>
<p>很明顯地，在這兩個情境裡頭「他」所代表的語義不同。如果仍使用沒蘊含上下文資訊的詞向量，機器就會很難正確「解讀」這兩個句子所蘊含的語義了。</p>
<p>現在讓我們跟隨<a href="https://colab.research.google.com/drive/1g2nhY9vZG-PLC3w3dcHGqwsHBAXnD9EY">這個 Colab 筆記本</a>來使用剛剛看到的 <a href="https://github.com/jessevig/bertviz">BertViz</a>，看看 BERT 會怎麼處理這兩個情境：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 安裝 BertViz</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="o">!</span><span class="nb">test</span> -d bertviz_repo <span class="o">||</span> git clone https://github.com/jessevig/bertviz bertviz_repo
<span class="k">if</span> <span class="ow">not</span> <span class="s1">'bertviz_repo'</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">path</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">'bertviz_repo'</span><span class="p">]</span>

<span class="c1"># import packages</span>
<span class="kn">from</span> <span class="nn">bertviz.pytorch_pretrained_bert</span> <span class="k">import</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertTokenizer</span>
<span class="kn">from</span> <span class="nn">bertviz.head_view_bert</span> <span class="k">import</span> <span class="n">show</span>

<span class="c1"># 在 jupyter notebook 裡頭顯示 visualzation 的 helper</span>
<span class="k">def</span> <span class="nf">call_html</span><span class="p">():</span>
  <span class="kn">import</span> <span class="nn">IPython</span>
  <span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="s1">'''</span>
<span class="s1">        &lt;script src="/static/components/requirejs/require.js"&gt;&lt;/script&gt;</span>
<span class="s1">        &lt;script&gt;</span>
<span class="s1">          requirejs.config({</span>
<span class="s1">            paths: {</span>
<span class="s1">              base: '/static/base',</span>
<span class="s1">              "d3": "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min",</span>
<span class="s1">              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',</span>
<span class="s1">            },</span>
<span class="s1">          });</span>
<span class="s1">        &lt;/script&gt;</span>
<span class="s1">        '''</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Setup 以後就能非常輕鬆地將 BERT 內部的注意力機制視覺化出來：</p>
<div class="highlight"><pre><span></span><span class="c1"># 記得我們是使用中文 BERT</span>
<span class="n">bert_version</span> <span class="o">=</span> <span class="s1">'bert-base-chinese'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_version</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_version</span><span class="p">)</span>

<span class="c1"># 情境 1 的句子</span>
<span class="n">sentence_a</span> <span class="o">=</span> <span class="s2">"胖虎叫大雄去買漫畫，"</span>
<span class="n">sentence_b</span> <span class="o">=</span> <span class="s2">"回來慢了就打他。"</span>
<span class="n">call_html</span><span class="p">()</span>
<span class="n">show</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">sentence_a</span><span class="p">,</span> <span class="n">sentence_b</span><span class="p">)</span>

<span class="c1"># 注意：執行這段程式碼以後只會顯示下圖左側的結果。</span>
<span class="c1"># 為了方便你比較，我把情境 2 的結果也同時附上</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/bert/bert-coreference.jpg"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是 BERT 裡第 9 層 Encoder block 的其中一個 head 的注意力結果。</p>
<p>圖中的線條代表該 head 在更新「他」（左側）的 repr. 時關注其他詞彙（右側）的注意力程度。越粗代表關注權重（attention weights）越高。很明顯地這個 head 具有一定的<a href="https://youtu.be/i19m4GzBhfc">指代消解（Coreference Resolution）</a>能力，能正確地找出「他」所指代的對象。</p>
<p>指代消解可不是一項簡單任務，但 BERT 透過自注意力機制、深度雙向語言模型以及大量的訓練文本達到這樣的水準，是一件令人雀躍的事情。</p>
<p>當然 BERT 並不是第一個嘗試產生 contextual word repr. 的語言模型。在它之前最知名的例子有剛剛提到的 <a href="https://allennlp.org/elmo">ELMo</a> 以及 <a href="https://github.com/openai/gpt-2">GPT</a>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/bert/bert_elmo_gpt.jpg"/>
</center>
<center>
                        ELMo、GPT 以及 BERT 都透過訓練語言模型來獲得 contextual word representation
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ELMo 利用兩層雙向 LSTM  做語言模型並將中間得到的隱狀態向量串接當作每個詞彙的 contextual word representation；GPT 則是使用 Transformer 的 Decoder 來訓練一個中規中矩，從左到右的語言模型。</p>
<p>BERT 跟它們的差異在於利用 <strong>M</strong>asked <strong>L</strong>anguage <strong>M</strong>odel（MLM，即克漏字的文雅說法）的概念訓練一個<strong>雙向</strong>的語言模型，使得其輸出的每個 token 的 repr. 都同時蘊含了前後文資訊。</p>
<p>跟以往模型相比，BERT 能更好地處理自然語言，在著名的問答任務 <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD2.0</a> 也有卓越表現：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/bert/squad2.jpg"/>
</center>
<center>
                        SQuAD 2.0 目前排行榜的前 5 名有 4 個有使用 BERT
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-chinese-BERT">Load chinese BERT<a class="anchor-link" href="#Load-chinese-BERT">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>李宏毅：BERT 和 GPT-2 都是使用 word piece (例如: fragment 可以拆成 frag + ment 兩個 pieces ，一個 word 也可以獨自形成一個 word piece) ，word piece 可以由蒐集大量的資料找出常出現的 pattern 取得</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">clear_output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tqdm boto3 requests regex -q
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">github_repo</span> <span class="o">=</span> <span class="s2">"huggingface/pytorch-pretrained-BERT"</span>
<span class="n">PRETRAINED_MODEL_NAME</span> <span class="o">=</span> <span class="s2">"bert-base-chinese"</span>

<span class="c1"># bert_tokenizer = torch.hub.load(github_repo, 'bertTokenizer', PRETRAINED_MODEL_NAME, do_basic_tokenize=False)</span>
<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">github_repo</span><span class="p">,</span> <span class="s1">'bertTokenizer'</span><span class="p">,</span> <span class="n">PRETRAINED_MODEL_NAME</span><span class="p">)</span>
<span class="c1"># TODO: True / False</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"[CLS] 東京好熱， [SEP] 台灣也是嗎？ [SEP] [PAD]"</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>['[CLS]', '東', '京', '好', '熱', '，', '[SEP]', '台', '灣', '也', '是', '嗎', '？', '[SEP]', '[PAD]']
[101, 3346, 776, 1962, 4229, 8024, 102, 1378, 4124, 738, 3221, 1621, 8043, 102, 0]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sentence-finetune-model">Sentence finetune model<a class="anchor-link" href="#Sentence-finetune-model">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
<span class="n">tokens_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ids</span><span class="p">])</span>
<span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">segments_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masks_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([1, 15])
torch.Size([1, 15])
torch.Size([1, 15])
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load bertForSequenceClassification</span>
<span class="n">finetune_task</span> <span class="o">=</span> <span class="s2">"bertForSequenceClassification"</span>
<span class="n">num_labels</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">github_repo</span><span class="p">,</span> <span class="n">finetune_task</span><span class="p">,</span> <span class="n">PRETRAINED_MODEL_NAME</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_labels</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Predict the sequence classification logits</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">tokens_tensors</span><span class="p">,</span> 
                        <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">,</span> 
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks_tensors</span><span class="p">)</span>

<span class="n">clear_output</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[-0.5932, -0.7918,  0.1374]])
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compare-custom-loss-to-predefined-loss">Compare custom loss to predefined loss<a class="anchor-link" href="#Compare-custom-loss-to-predefined-loss">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tmp_criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">tmp_loss</span> <span class="o">=</span> <span class="n">tmp_criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">tmp_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.5586)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks_tensors</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.5586, grad_fn=&lt;NllLossBackward&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># train on GPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="n">classifier</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>device: cuda:0
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): BertLayerNorm()
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1)
  (classifier): Linear(in_features=768, out_features=3, bias=True)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Attentionmap-&amp;-BertViz">Attentionmap &amp; BertViz<a class="anchor-link" href="#Attentionmap-&amp;-BertViz">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># bert_tokenizer.save_vocabulary("vocab.txt")</span>
<span class="c1"># classifier.bert.config.to_json_file("bert_config.json")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Download-raw-data-and-sample">Download raw data and sample<a class="anchor-link" href="#Download-raw-data-and-sample">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: 看能不能使用 torch utils 直接下載 kaggle dataset</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"train.tsv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe table table-striped table-responsive">
<thead>
<tr style="text-align: right;">
<th></th>
<th>text_a</th>
<th>text_b</th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>2018年社保改革新政策：这四大情况可提前支取养老金</td>
<td>2018年社保改革新政策，这三大养老金调整方式须知晓！</td>
<td>unrelated</td>
</tr>
<tr>
<th>1</th>
<td>注射胰岛素会上瘾？5种情况必须使用胰岛素治疗！</td>
<td>谣言粉碎机｜吃粗粮能降糖？胰岛素注射液必须放冰箱？</td>
<td>unrelated</td>
</tr>
<tr>
<th>2</th>
<td>三高了？不如试试用凉水泡它喝，坚持一个月，效果惊到你！</td>
<td>凉水喝了拉肚子还致癌？关于&ldquo;凉水&rdquo;你该知道的3个真相</td>
<td>unrelated</td>
</tr>
<tr>
<th>3</th>
<td>罕见的水果吃过5种以上是土豪，吃过10种以上就是贵族！</td>
<td>这些水果吃过3种是土豪，吃过5种是贵族，你吃过几种呢？</td>
<td>agreed</td>
</tr>
<tr>
<th>4</th>
<td>最管用的&ldquo;天然胰岛素&rdquo;，糖尿病的救星终于出现！血糖猛降不是梦</td>
<td>&ldquo;天然胰岛素&rdquo;，糖尿病&ldquo;死对头&rdquo;，每天吃两口，血糖不升反降！</td>
<td>agreed</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(23871, 3)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-tsv-as-pytorch-dataset">Load tsv as pytorch dataset<a class="anchor-link" href="#Load-tsv-as-pytorch-dataset">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>input: (text_a, text_b, label)</li>
<li>output: <ul>
<li>tokens_tensor: <code>[CLS] T1 T2 [SEP] T3 T4 [SEP] [PAD]</code></li>
<li>segments_tensors</li>
<li>mask_tensors</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">input_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install pysnooper -q
<span class="kn">import</span> <span class="nn">pysnooper</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="c1"># The convention in BERT is:</span>
  <span class="c1"># (a) For sequence pairs:</span>
  <span class="c1">#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]</span>
  <span class="c1">#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1</span>
  <span class="c1"># (b) For single sequences:</span>
  <span class="c1">#  tokens:   [CLS] the dog is hairy . [SEP]</span>
  <span class="c1">#  type_ids: 0     0   0   0  0     0 0</span>
  <span class="c1">#</span>
  <span class="c1"># Where "type_ids" are used to indicate whether this is the first</span>
  <span class="c1"># sequence or the second sequence. The embedding vectors for `type=0` and</span>
  <span class="c1"># `type=1` were learned during pre-training and are added to the wordpiece</span>
  <span class="c1"># embedding vector (and position vector). This is not *strictly* necessary</span>
  <span class="c1"># since the [SEP] token unambiguously separates the sequences, but it makes</span>
  <span class="c1"># it easier for the model to learn the concept of sequences.</span>
  <span class="c1">#</span>
  <span class="c1"># For classification tasks, the first vector (corresponding to [CLS]) is</span>
  <span class="c1"># used as the "sentence vector". Note that this only makes sense because</span>
  <span class="c1"># the entire model is fine-tuned.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pad_sequence</span>
 
    
<span class="k">class</span> <span class="nc">FakeNewsDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="c1"># 讀取原始 tsv 檔並 setup 一些參數</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"train"</span><span class="p">,</span> <span class="s2">"dev"</span><span class="p">,</span> <span class="s2">"test"</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">mode</span> <span class="o">+</span> <span class="s2">".tsv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    
    <span class="c1"># 回傳一筆訓練 / 驗證 / 測試數據</span>
<span class="c1">#     @pysnooper.snoop()</span>
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">text_a</span><span class="p">,</span> <span class="n">text_b</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]</span>
        <span class="n">word_pieces</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"[CLS]"</span><span class="p">]</span>
        <span class="n">tokens_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_a</span><span class="p">)</span>
        <span class="n">word_pieces</span> <span class="o">+=</span> <span class="n">tokens_a</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"[SEP]"</span><span class="p">]</span>
        <span class="n">len_a</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        
        <span class="c1"># 第二個句子的 BERT tokens</span>
        <span class="n">tokens_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_b</span><span class="p">)</span>
        <span class="n">word_pieces</span> <span class="o">+=</span> <span class="n">tokens_b</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"[SEP]"</span><span class="p">]</span>
        <span class="n">len_b</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span> <span class="o">-</span> <span class="n">len_a</span>
        
        <span class="c1"># 將整個 token 序列轉換成索引序列</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        
        <span class="c1"># 將 label 也轉換成索引</span>
        <span class="n">label_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        
        <span class="n">segments_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">len_a</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">len_b</span><span class="p">,</span> 
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label_id</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">len</span>


<span class="k">def</span> <span class="nf">create_mini_batch</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
    <span class="n">tokens_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">label_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">])</span>
    
    <span class="n">tokens_tensors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> 
                                  <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">segments_tensors</span><span class="p">,</span> 
                                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> 
                                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">masks_tensors</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">tokens_tensors</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">label_ids</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">[</span><span class="s1">'text_a'</span><span class="p">,</span> <span class="s1">'text_b'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">text_a</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">text_b</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">input_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"[CLS]"</span><span class="p">]</span> <span class="o">+</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_a</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"[SEP]"</span><span class="p">]</span> <span class="o">+</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_b</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">'[SEP]'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">FakeNewsDataset</span><span class="p">(</span><span class="s2">"train"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">bert_tokenizer</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">create_mini_batch</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
    
    <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">label_ids</span> <span class="o">=</span> \
        <span class="n">data</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"batch </span><span class="si">{i}</span><span class="s2">:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"tokens_tensors:"</span><span class="p">,</span> <span class="n">tokens_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"segments_tensors:"</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"masks_tensors:"</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"label_ids:"</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c1"># input &gt; token &gt; segment &gt; position</span>
    <span class="n">input_words</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">'[PAD]'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">masks_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_words</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_words</span><span class="p">,</span> <span class="n">tokens_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">segments_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">masks_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'   '</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>  <span class="s1">'  '</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    
    <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>batch 0:
tokens_tensors: torch.Size([64, 63])
segments_tensors: torch.Size([64, 63])
masks_tensors: torch.Size([64, 63])
label_ids: torch.Size([64])
------------------------------
[CLS] 101     0    1
注 3800     0    1
射 2198     0    1
胰 5536     0    1
岛 2270     0    1
素 5162     0    1
会 833     0    1
上 677     0    1
瘾 4614     0    1
？ 8043     0    1
5 126     0    1
种 4905     0    1
情 2658     0    1
况 1105     0    1
必 2553     0    1
须 7557     0    1
使 886     0    1
用 4500     0    1
胰 5536     0    1
岛 2270     0    1
素 5162     0    1
治 3780     0    1
疗 4545     0    1
！ 8013     0    1
[SEP] 102     0    1
谣 6469     1    1
言 6241     1    1
粉 5106     1    1
碎 4810     1    1
机 3322     1    1
｜ 8078     1    1
吃 1391     1    1
粗 5110     1    1
粮 5117     1    1
能 5543     1    1
降 7360     1    1
糖 5131     1    1
？ 8043     1    1
胰 5536     1    1
岛 2270     1    1
素 5162     1    1
注 3800     1    1
射 2198     1    1
液 3890     1    1
必 2553     1    1
须 7557     1    1
放 3123     1    1
冰 1102     1    1
箱 5056     1    1
？ 8043     1    1
[SEP] 102     1    1
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
&lt;pad&gt; 0     0    0
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="直接預測">直接預測<a class="anchor-link" href="#直接預測">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_dev</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"dev.tsv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
<span class="n">df_dev</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(3206, 3)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_dev</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>unrelated    2187
agreed        934
disagreed      85
Name: label, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_dev</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s2">"unrelated"</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>0.682158452900811</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dev</span> <span class="o">=</span> <span class="n">FakeNewsDataset</span><span class="p">(</span><span class="s2">"dev"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">bert_tokenizer</span><span class="p">)</span>
<span class="n">devloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">create_mini_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">devloader</span><span class="p">:</span>
        <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> \
        <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Accuracy of the network on dev set: </span><span class="si">%d</span><span class="s1"> </span><span class="si">%%</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span>
    <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy of the network on dev set: 67 %
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels</span><span class="p">,</span> <span class="n">predicted</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(tensor([1, 0, 0, 2, 2, 0], device='cuda:0'),
 tensor([0, 0, 0, 2, 0, 0], device='cuda:0'))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Loss-function-&amp;-Optimizer">Loss function &amp; Optimizer<a class="anchor-link" href="#Loss-function-&amp;-Optimizer">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)</span>
<span class="c1"># print(f"Model has {num_params} parameters") </span>

<span class="c1"># model.to('cuda:0')</span>
<span class="c1"># loss = torch.nn.NLLLoss()</span>
<span class="c1"># loss = loss.to('cuda:0')</span>

<span class="c1"># learnable_params = [p for p in model.parameters() if p.requires_grad]</span>
<span class="c1"># optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">learnable_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learnable_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"# params: {sum(p.numel() for p in learnable_params)}"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre># params: 102269955
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Finetune-Train-the-network">Finetune Train the network<a class="anchor-link" href="#Finetune-Train-the-network">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Outputs:
    if <code>labels</code> is not <code>None</code>:
        Outputs the CrossEntropy classification loss of the output with the labels.
    if <code>labels</code> is <code>None</code>:
        Outputs the classification logits of shape [batch_size, num_labels].</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_on_dev</span><span class="p">():</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">devloader</span><span class="p">:</span>
            <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> \
            <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

            <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Accuracy of the network on dev set: </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span>
        <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="k">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="k">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/content/drive'</span><span class="p">,</span> <span class="n">force_remount</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Mounted at /content/drive
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s1">'/content/drive/My Drive/latest_checkpoint.pth'</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">):</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"recovered latest checkpoints."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>1 epoch: 50 minutes</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>
EPOCHS = 30

for epoch in tqdm(range(EPOCHS)):
    
    
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        
        tokens_tensors, segments_tensors, \
        masks_tensors, labels = [t.to(device) for t in data]

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
#         logits = classifier(input_ids=tokens_tensors, 
#                             token_type_ids=segments_tensors, 
#                             attention_mask=masks_tensors)
#         loss = criterion(logits, labels)
#         loss.backward()
        loss = classifier(input_ids=tokens_tensors, 
                            token_type_ids=segments_tensors, 
                            attention_mask=masks_tensors, labels=labels)
        loss.backward()
        optimizer.step()
        

        # print statistics
        running_loss += loss.item()
        
        if i % 100 == 99:    # print every 100 mini-batches
            print('[epoch %d, steps %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss))
            running_loss = 0.0
    
    
    evaluate_on_dev()
    torch.save(classifier.state_dict(), 
               f"/content/drive/My Drive/checkpoint_epoch_{epoch + 1}.pth")
    torch.save(classifier.state_dict(), MODEL_PATH)

print('Finished Training')
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 1, steps   100] loss: 47.437
[epoch 1, steps   200] loss: 38.050
[epoch 1, steps   300] loss: 36.146
Accuracy of the network on dev set: 83.91 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 10%|█         | 1/10 [05:26&lt;48:59, 326.59s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 2, steps   100] loss: 31.601
[epoch 2, steps   200] loss: 29.002
[epoch 2, steps   300] loss: 27.861
Accuracy of the network on dev set: 85.50 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 20%|██        | 2/10 [10:53&lt;43:32, 326.54s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 3, steps   100] loss: 25.822
[epoch 3, steps   200] loss: 22.838
[epoch 3, steps   300] loss: 26.275
Accuracy of the network on dev set: 83.81 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 30%|███       | 3/10 [16:19&lt;38:06, 326.66s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 4, steps   100] loss: 24.291
[epoch 4, steps   200] loss: 22.369
[epoch 4, steps   300] loss: 21.653
Accuracy of the network on dev set: 85.75 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 40%|████      | 4/10 [21:46&lt;32:39, 326.59s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 5, steps   100] loss: 19.547
[epoch 5, steps   200] loss: 15.473
[epoch 5, steps   300] loss: 16.879
Accuracy of the network on dev set: 83.06 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 50%|█████     | 5/10 [27:12&lt;27:12, 326.55s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 6, steps   100] loss: 16.239
[epoch 6, steps   200] loss: 12.886
[epoch 6, steps   300] loss: 13.843
Accuracy of the network on dev set: 82.41 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 60%|██████    | 6/10 [32:39&lt;21:45, 326.49s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 7, steps   100] loss: 15.607
[epoch 7, steps   200] loss: 17.340
[epoch 7, steps   300] loss: 19.988
Accuracy of the network on dev set: 86.03 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 70%|███████   | 7/10 [38:05&lt;16:19, 326.44s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 8, steps   100] loss: 13.632
[epoch 8, steps   200] loss: 13.587
[epoch 8, steps   300] loss: 13.003
Accuracy of the network on dev set: 86.28 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 80%|████████  | 8/10 [43:31&lt;10:52, 326.46s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 9, steps   100] loss: 11.371
[epoch 9, steps   200] loss: 10.259
[epoch 9, steps   300] loss: 11.037
Accuracy of the network on dev set: 85.25 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre> 90%|█████████ | 9/10 [48:58&lt;05:26, 326.62s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[epoch 10, steps   100] loss: 9.708
[epoch 10, steps   200] loss: 9.456
[epoch 10, steps   300] loss: 10.046
Accuracy of the network on dev set: 85.75 %
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|██████████| 10/10 [54:26&lt;00:00, 326.76s/it]</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Finished Training
CPU times: user 31min 2s, sys: 22min 52s, total: 53min 54s
Wall time: 54min 26s
</pre>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_stream output_stderr output_text">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>(23871, 3)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
        <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> \
        <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Accuracy of the network on train set: </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span>
    <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy of the network on train set: 97.64 %
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># sdf</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Feature-extraction">Feature extraction<a class="anchor-link" href="#Feature-extraction">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>1 epoch 20 minutes</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># for p in classifier.bert.parameters():</span>
<span class="c1">#     p.requires_grad = False</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learnable_params = [p for p in classifier.parameters() if p.requires_grad]</span>
<span class="c1"># print(f"# params: {sum(p.numel() for p in learnable_params)}")</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># %%time</span>
<span class="c1"># EPOCHS = 10</span>

<span class="c1"># for epoch in tqdm(range(EPOCHS)):</span>
    
    
<span class="c1">#     running_loss = 0.0</span>
<span class="c1">#     for i, data in enumerate(trainloader, 0):</span>
        
<span class="c1">#         tokens_tensors, segments_tensors, \</span>
<span class="c1">#         masks_tensors, labels = [t.to(device) for t in data]</span>

<span class="c1">#         # zero the parameter gradients</span>
<span class="c1">#         optimizer.zero_grad()</span>

<span class="c1">#         # forward + backward + optimize</span>
<span class="c1">#         logits = classifier(input_ids=tokens_tensors, </span>
<span class="c1">#                             token_type_ids=segments_tensors, </span>
<span class="c1">#                             attention_mask=masks_tensors)</span>
<span class="c1">#         loss = criterion(logits, labels)</span>
<span class="c1">#         loss.backward()</span>
<span class="c1">#         optimizer.step()</span>

<span class="c1">#         # print statistics</span>
<span class="c1">#         running_loss += loss.item()</span>
        
<span class="c1">#         if i % 100 == 99:    # print every 100 mini-batches</span>
<span class="c1">#             print('[epoch %d, steps %5d] loss: %.3f' %</span>
<span class="c1">#                   (epoch + 1, i + 1, running_loss))</span>
<span class="c1">#             running_loss = 0.0</span>
    
    
<span class="c1">#     evaluate_on_dev()</span>
<span class="c1">#     torch.save(classifier.state_dict(), './checkpoint.pth')</span>

<span class="c1"># print('Finished Training')</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># correct = 0</span>
<span class="c1"># total = 0</span>
<span class="c1"># with torch.no_grad():</span>
<span class="c1">#     for data in trainloader:</span>
<span class="c1">#         tokens_tensors, segments_tensors, \</span>
<span class="c1">#         masks_tensors, labels = [t.to(device) for t in data]</span>
        
<span class="c1">#         logits = classifier(tokens_tensors, segments_tensors, masks_tensors)</span>
<span class="c1">#         _, predicted = torch.max(logits.data, 1)</span>
<span class="c1">#         total += labels.size(0)</span>
<span class="c1">#         correct += (predicted == labels).sum().item()</span>

<span class="c1"># print('Accuracy of the network on train set: %d %%' % (</span>
<span class="c1">#     100 * correct / total))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: demo 加　script 版本</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="產生-test-結果">產生 test 結果<a class="anchor-link" href="#產生-test-結果">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pad_sequence</span>
 
    
<span class="k">class</span> <span class="nc">FakeNewsDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="c1"># 讀取原始 tsv 檔並 setup 一些參數</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"train"</span><span class="p">,</span> <span class="s2">"dev"</span><span class="p">,</span> <span class="s2">"test"</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">mode</span> <span class="o">+</span> <span class="s2">".tsv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'agreed'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">'disagreed'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'unrelated'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span> <span class="c1"># TEMP</span>

            
<span class="c1">#         self.labels = sorted(self.df.label.unique())</span>
<span class="c1">#         self.label_map = {label: i for i, label in enumerate(self.labels)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    
    <span class="c1"># 回傳一筆訓練 / 驗證 / 測試數據</span>
<span class="c1">#     @pysnooper.snoop()</span>
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"test"</span><span class="p">:</span>
            <span class="n">text_a</span><span class="p">,</span> <span class="n">text_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span>
            <span class="n">label_tensor</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_a</span><span class="p">,</span> <span class="n">text_b</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span>
            <span class="c1"># 將 label 也轉換成索引</span>
            <span class="n">label_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
            <span class="n">label_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label_id</span><span class="p">)</span>
            
        
        <span class="c1"># 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]</span>
        <span class="n">word_pieces</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"[CLS]"</span><span class="p">]</span>
        <span class="n">tokens_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_a</span><span class="p">)</span>
        <span class="n">word_pieces</span> <span class="o">+=</span> <span class="n">tokens_a</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"[SEP]"</span><span class="p">]</span>
        <span class="n">len_a</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        
        <span class="c1"># 第二個句子的 BERT tokens</span>
        <span class="n">tokens_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text_b</span><span class="p">)</span>
        <span class="n">word_pieces</span> <span class="o">+=</span> <span class="n">tokens_b</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"[SEP]"</span><span class="p">]</span>
        <span class="n">len_b</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span> <span class="o">-</span> <span class="n">len_a</span>
        
        <span class="c1"># 將整個 token 序列轉換成索引序列</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        
        <span class="n">segments_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">len_a</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">len_b</span><span class="p">,</span> 
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensor</span><span class="p">,</span> <span class="n">label_tensor</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">len</span>


<span class="k">def</span> <span class="nf">create_mini_batch</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
    <span class="n">tokens_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="n">tokens_tensors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> 
                                  <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">segments_tensors</span><span class="p">,</span> 
                                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> 
                                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">masks_tensors</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">tokens_tensors</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">,</span> <span class="n">label_ids</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dev.label_map</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>{'agreed': 0, 'disagreed': 1, 'unrelated': 2}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test.df.title2_zh.apply(lambda x: 1 if type(x) == float else 0 ).sum()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">FakeNewsDataset</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">bert_tokenizer</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">create_mini_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict_test</span><span class="p">():</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
            <span class="n">tokens_tensors</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">masks_tensors</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">tokens_tensors</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">,</span> <span class="n">masks_tensors</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">predictions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                
                <span class="n">predictions</span> <span class="o">=</span> <span class="n">predicted</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predicted</span><span class="p">))</span>

<span class="c1">#     print('Accuracy of the network on dev set: %.2f %%' % (</span>
<span class="c1">#         100 * correct / total))</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_test</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([80126])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">test</span><span class="o">.</span><span class="n">label_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">index_map</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>{0: 'agreed', 1: 'disagreed', 2: 'unrelated'}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"result"</span><span class="p">:</span> <span class="n">predictions</span><span class="o">.</span><span class="n">tolist</span><span class="p">()})</span>
<span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
<span class="n">result</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">index_map</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span class="n">result</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe table table-striped table-responsive">
<thead>
<tr style="text-align: right;">
<th></th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>unrelated</td>
</tr>
<tr>
<th>1</th>
<td>unrelated</td>
</tr>
<tr>
<th>2</th>
<td>unrelated</td>
</tr>
<tr>
<th>3</th>
<td>unrelated</td>
</tr>
<tr>
<th>4</th>
<td>unrelated</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span><span class="nb">test</span> -d test.csv.zip <span class="o">||</span> wget https://s3-ap-northeast-1.amazonaws.com/smartnews-dmp-tmp/meng/test.csv.zip
<span class="o">!</span>unzip test.csv.zip
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>--2019-07-07 14:53:09--  https://s3-ap-northeast-1.amazonaws.com/smartnews-dmp-tmp/meng/test.csv.zip
Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.4.142
Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.4.142|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6906894 (6.6M) [application/zip]
Saving to: &lsquo;test.csv.zip&rsquo;

test.csv.zip        100%[===================&gt;]   6.59M  --.-KB/s    in 0.1s    

2019-07-07 14:53:09 (48.4 MB/s) - &lsquo;test.csv.zip&rsquo; saved [6906894/6906894]

Archive:  test.csv.zip
  inflating: test.csv                
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"test.csv"</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">df_test</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe table table-striped table-responsive">
<thead>
<tr style="text-align: right;">
<th></th>
<th>id</th>
<th>tid1</th>
<th>tid2</th>
<th>title1_zh</th>
<th>title2_zh</th>
<th>title1_en</th>
<th>title2_en</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>321187</td>
<td>167562</td>
<td>59521</td>
<td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>
<td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>
<td>egypt 's presidential election failed to win m...</td>
<td>Lyon! Lyon officials have denied that Felipe F...</td>
</tr>
<tr>
<th>1</th>
<td>321190</td>
<td>167564</td>
<td>91315</td>
<td>萨达姆被捕后告诫美国的一句话，发人深思</td>
<td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>
<td>A message from Saddam Hussein after he was cap...</td>
<td>The Top 10 Americans believe that the Lizard M...</td>
</tr>
<tr>
<th>2</th>
<td>321189</td>
<td>167563</td>
<td>167564</td>
<td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>
<td>萨达姆被捕后告诫美国的一句话，发人深思</td>
<td>Will the United States wage war on Iraq withou...</td>
<td>A message from Saddam Hussein after he was cap...</td>
</tr>
<tr>
<th>3</th>
<td>321193</td>
<td>167564</td>
<td>160994</td>
<td>萨达姆被捕后告诫美国的一句话，发人深思</td>
<td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>
<td>A message from Saddam Hussein after he was cap...</td>
<td>The hanging Saddam is a surrogate? This man's ...</td>
</tr>
<tr>
<th>4</th>
<td>321191</td>
<td>167564</td>
<td>15084</td>
<td>萨达姆被捕后告诫美国的一句话，发人深思</td>
<td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>
<td>A message from Saddam Hussein after he was cap...</td>
<td>Chinese loquat loquat plaster in America? Pure...</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">final_result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">'id'</span><span class="p">],</span> <span class="n">result</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">'label'</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">final_result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Id'</span><span class="p">,</span> <span class="s1">'Category'</span><span class="p">]</span>
<span class="n">final_result</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe table table-striped table-responsive">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Id</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>321187</td>
<td>unrelated</td>
</tr>
<tr>
<th>1</th>
<td>321190</td>
<td>unrelated</td>
</tr>
<tr>
<th>2</th>
<td>321189</td>
<td>unrelated</td>
</tr>
<tr>
<th>3</th>
<td>321193</td>
<td>unrelated</td>
</tr>
<tr>
<th>4</th>
<td>321191</td>
<td>unrelated</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">final_result</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">'bert_epoch10_bsize64_pytorch_10prec_num_sample_23871_train.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that this is a <em>massive</em> gain over our CNN baseline and also improves over our ELMo contextual embeddings for this dataset.  BERT has been shown high-performance results across many datasets, and integrating it into unstructured prediction problems is quite simple, as we saw in this section.</p>
<h2 id="Conclusion_1">Conclusion<a class="anchor-link" href="#Conclusion">&para;</a></h2><p>In this section we investigated the Transformer model architecture, particularly in the context of pretraining LMs.  We discussed some of the model details and we looked at how BERT extends the GPT approach from OpenAI.  We then built our own fine-tuned classifier using the Hugging Face PyTorch library to create and re-load the BERT model and add our own layers on top.</p>
<h3 id="Some-further-resources">Some further resources<a class="anchor-link" href="#Some-further-resources">&para;</a></h3><p>We have only scratched the surface of the exciting way that transfer learning is transforming NLP.</p>
<ul>
<li><strong>Transformer Architecture</strong><ul>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>: mentioned previously, but so good it deserves mentioning again</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>:  good tutorial on how the Transformer works</li>
</ul>
</li>
<li><p>A really nice blogpost on transfer learning from Sebastian Ruder (<a href="http://ruder.io/nlp-imagenet/">http://ruder.io/nlp-imagenet/</a>)</p>
</li>
<li><p><strong>Transfer Learning</strong></p>
<ul>
<li>A <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit">fantastic tutorial at NAACL this year</a> which is both thorough and introductory.  It covers a lot of material including how to probe pretrain models to try and figure out what they are up to</li>
<li>A nice colab from the Google BERT devs showing using BERT from TF-Hub (<a href="https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb">https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb</a>)</li>
</ul>
</li>
<li><p><strong>Model Intepretation and Probing</strong></p>
<ul>
<li>Jesse Vig's Blog post analyzing the different heads of BERT based<ul>
<li>Part I: <a href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77">https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77</a></li>
<li>Part II: <a href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1">https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1</a></li>
<li>And a colab that drills into the <a href="https://colab.research.google.com/drive/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB">Q and K vectors during multi-head attention here</a>: </li>
</ul>
</li>
<li><a href="https://github.com/clarkkev/attention-analysis">Kevin Clark's Jupyter Notebooks</a> for <a href="https://arxiv.org/abs/1906.04341">What Does BERT Look At? An Analysis of BERT's Attention, Clark et al., 2019</a></li>
<li><a href="https://github.com/TalLinzen/rnn_agreement">Tal Linzen's code</a> for <a href="https://arxiv.org/abs/1611.01368">Assessing the ability of LSTMs to learn syntax-sensitive dependencies, Linzen et al., 2016</a></li>
<li><a href="https://github.com/yoavg/bert-syntax">Yoav Goldberg's code</a> assessing syntactic abilities of BERT</li>
<li><a href="https://github.com/nelson-liu/contextual-repr-analysis">Nelson Liu's code</a> for <a href="https://homes.cs.washington.edu/~nfliu/papers/liu+gardner+belinkov+peters+smith.naacl2019.pdf">Linguistic Knowledge and Transferability of Contextual Representations, Liu et al., 2019</a></li>
</ul>
</li>
<li><p><strong>More about Neural NLP</strong></p>
<ul>
<li>Get right into the source material.  Some papers that are helpful to understand deep learning in NLP (<a href="https://github.com/dpressel/lit">https://github.com/dpressel/lit</a>)</li>
</ul>
</li>
<li><p><strong>Get Hacking</strong></p>
<ul>
<li>Implementations of most of what we talked about today in TensorFlow and PyTorch (<a href="https://github.com/dpressel/baseline">https://github.com/dpressel/baseline</a>)</li>
</ul>
</li>
</ul>
<p>There is also an end-to-end example using the Baseline API above to train a GPT-like LM using the code above in PyTorch:</p>
<p><a href="https://github.com/dpressel/baseline/blob/master/api-examples/pretrain-transformer-lm.py">https://github.com/dpressel/baseline/blob/master/api-examples/pretrain-transformer-lm.py</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Part-III:-Fine-tuning-a-pre-trained-model_2">Part III: Fine-tuning a pre-trained model<a class="anchor-link" href="#Part-III:-Fine-tuning-a-pre-trained-model">&para;</a></h1><p>In the last section, we looked at using a biLM networks layers as embeddings for our classification model.  In that approach, we maintain the exact same model architecture as before, but just switching our word embeddings out for context embeddings (or, more commonly, using them in concert).</p>
<p>The paper <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding
by Generative Pre-Training</a> (Radford et al 2018) explored a different approach, much more similar to what is typically done in computer vision.  In fine-tuning, we reuse the network architecture and simply replace the head.  We dont use any model specific architecture anymore, just a final layer.  There is an accompanying blog post <a href="https://openai.com/blog/language-unsupervised/">here</a>.  The image below is borrowed from that blog post</p>
<p><img alt="alt text" src="https://openai.com/content/images/2018/06/zero-shot-transfer@2x.png"/></p>
<p>As we can see from the images, these models can rapidly improve our downstream performance with very limited fine-tuning supervision.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Transformer">The Transformer<a class="anchor-link" href="#The-Transformer">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The original Transformer is an all-attention encoder-decoder model first introduced in <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need, Vaswani et al., 2017</a>.  It is described at a high-level in <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">this Google AI post</a>.
Here is an image of the model architecture for a Transformer:</p>
<p><img alt="Transformer Architecture" src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png"/></p>
<p>The reference implementation from Google is the <a href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor">tensor2tensor repository</a>.  There is a lot going on in that codebase, which some people may find hard to follow.</p>
<p>We are going to go through each component in a hands-on manner, which will hopefully give you a visual feel of what is happening.</p>
<p>If you want to understand Transformers better, there is a terrific blog post called <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer, Rush, 2018</a> where you can see how to code up a Transformer from scratch to do Neural Machine Translation (NMT) while following along with the paper.</p>
<p>In versions used in practice, there are slight differences from the actual image, most notably, that layer norm is performed first.  Also, in a causal LM pre-training setting, as in the case of GPT, we have no need for the decoder, which simplifies our architecture substantially, leaving only a masked self-attention in the encoder (this prevents us from seeing the future as we predict).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-Transformer-Encoder-Layer">A Transformer Encoder Layer<a class="anchor-link" href="#A-Transformer-Encoder-Layer">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is code adapted from <a href="https://github.com/dpressel/baseline">Baseline</a> that implements a Transformer block used in a GPT-like architecture (pictured above).  We are going to take a closer look at these blocks, so lets think of this as the high-level overview.  The input to this class is a <code>torch.Tensor</code> of shape <code>BxT</code>.  The first sub-component in a Transformer block is the Multi-Headed Attention.  The second is the "FFN" shown in the image -- an MLP layer followed by a linear projection back to the original size.  We encapsulate these transformations in an <code>nn.Sequential</code>.  Notice that each sub-layer is also a residual connection.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">pdrop</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">activation_type</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param num_heads (`int`): the number of heads for self-attention</span>
<span class="sd">        :param d_model (`int`): The model dimension size</span>
<span class="sd">        :param pdrop (`float`): The dropout probability</span>
<span class="sd">        :param scale (`bool`): Whether we are doing scaled dot-product attention</span>
<span class="sd">        :param activation_type: What activation type to use</span>
<span class="sd">        :param d_ff: The feed forward layer size</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span> <span class="k">if</span> <span class="n">d_ff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">pdrop</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">),</span>
                                 <span class="n">pytorch_activation</span><span class="p">(</span><span class="n">activation_type</span><span class="p">),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">pdrop</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param x: the inputs</span>
<span class="sd">        :param mask: a mask for the inputs</span>
<span class="sd">        :return: the encoder output</span>
<span class="sd">        """</span>
        <span class="c1"># Builtin Attention mask</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multi-headed-Attention">Multi-headed Attention<a class="anchor-link" href="#Multi-headed-Attention">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Multi-headed attention is one of the key innovations of the Transformer.  The idea was to allow each attention head to learn different relations.</p>
<p><img alt="MHA" src="https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png"/></p>
<h4 id="Scaled-dot-product-attention">Scaled dot product attention<a class="anchor-link" href="#Scaled-dot-product-attention">&para;</a></h4><p>Here is a picture of the operations involved in scaled dot product attention.</p>
<p><img alt="MHA Architecture" src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png"/></p>
<p><code>Q</code>, <code>K</code> and <code>V</code> are low-order projections of the input.  For Encoder-Decoders, the <code>Q</code> is a query vector in the decoder, and <code>K</code> and <code>V</code> are representations of the Encoder.  A dot product of the encoder keys and the query vector determines a set of weights that are applied against the <code>V</code> (again, also a representation of the encoder values).  In the case of the encoder, these are all drawn from the same input.  Basic dot product attention was actually introduced in <a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation, Luong et al., 2014</a>, but in the the Transformer paper, the authors made a strong case that the basic dot product attention benefits from scaling.</p>
<p>This is implemented (again adapted from Baseline), as follows:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762</span>

<span class="sd">    We apply the query to the keys to recieve our weights via softmax, which are then applied</span>
<span class="sd">    for each value, but in a series of efficient matrix operations.  In the case of self-attention,</span>
<span class="sd">    the key, query and values are all low order projections of the same input.</span>

<span class="sd">    :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D</span>
<span class="sd">    :param key: a set of keys from encoder or self</span>
<span class="sd">    :param value: a set of values from encoder or self</span>
<span class="sd">    :param mask: masking (for destination) to prevent seeing what we shouldnt</span>
<span class="sd">    :param dropout: apply dropout operator post-attention (this is not a float)</span>
<span class="sd">    :return: A tensor that is (BxHxTxT)</span>

<span class="sd">    """</span>
    <span class="c1"># (., H, T, T) = (., H, T, D) x (., H, D, T)</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">weights</span>
</pre></div>
<h4 id="The-Multi-head-part">The Multi-head part<a class="anchor-link" href="#The-Multi-head-part">&para;</a></h4><p>Each of the attention operations above that we apply is going to learn some weighted representation of our input -- what are we paying attention to?  There are lots of things that might be useful!   We might want to attend to the next word for language modeling.  To remember what we said, we might want to learn something like which pronouns refer to which nouns that we saw in previous tokens (this is called anaphora resolution and is a subset of coreference resolution).  We might hope that it picks up parse dependencies, that could help us with tasks that benefit from syntax.  Remember that each of our <code>Q</code>, <code>K</code> and <code>V</code> are low-order projections of our input.  What if we had many low-order projections and used each to learn different weightings?  This  is exactly what multi-head attention is.  Each "head" does the operation above and learns something meaningful (or at least, we hope it does!).</p>
<p>Here is some code that implements multi-headed attention using our function above:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Multi-headed attention from https://arxiv.org/abs/1706.03762 via http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>

<span class="sd">    Multi-headed attention provides multiple looks of low-order projections K, Q and V using an attention function</span>
<span class="sd">    (specifically `scaled_dot_product_attention` in the paper.  This allows multiple relationships to be illuminated</span>
<span class="sd">    via attention on different positional and representational information from each head.</span>

<span class="sd">    The number of heads `h` times the low-order projection dim `d_k` is equal to `d_model` (which is asserted upfront).</span>
<span class="sd">    This means that each weight matrix can be simply represented as a linear transformation from `d_model` to `d_model`,</span>
<span class="sd">    and partitioned into heads after the fact.</span>

<span class="sd">    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the</span>
<span class="sd">    sub-sequent `FFN` sub-layer.</span>

<span class="sd">    There are 3 uses of multi-head attention in the Transformer.</span>
<span class="sd">    For encoder-decoder layers, the queries come from the previous decoder layer, and the memory keys come from</span>
<span class="sd">    the encoder.  For encoder layers, the K, Q and V all come from the output of the previous layer of the encoder.</span>
<span class="sd">    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using</span>
<span class="sd">    future values</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">"""Constructor for multi-headed attention</span>

<span class="sd">        :param h: The number of heads</span>
<span class="sd">        :param d_model: The model hidden size</span>
<span class="sd">        :param dropout (``float``): The amount of dropout to use</span>
<span class="sd">        :param attn_fn: A function to apply attention, defaults to SDP</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_O</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_fn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span> <span class="k">if</span> <span class="n">scale</span> <span class="k">else</span> <span class="n">dot_product_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""Low-order projections of query, key and value into multiple heads, then attention application and dropout</span>

<span class="sd">        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D</span>
<span class="sd">        :param key: a set of keys from encoder or self</span>
<span class="sd">        :param value: a set of values from encoder or self</span>
<span class="sd">        :param mask: masking (for destination) to prevent seeing what we shouldnt</span>
<span class="sd">        :return: Multi-head attention output, result of attention application to sequence (B, T, d_model)</span>
<span class="sd">        """</span>
        <span class="n">batchsz</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># (B, H, T, D)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_Q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batchsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_K</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batchsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_V</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batchsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_fn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> \
            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batchsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_O</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<p>We are going to take a look at how multi-headed attention works visually. To do this, we are going to use the <a href="https://github.com/jessevig/bertviz">viz-bert codebase</a> from Jesse Vig.  The accompanying paper is <a href="https://arxiv.org/pdf/1906.05714.pdf">A Multiscale Visualization of Attention in the Transformer Model, Vig, 2019</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import sys</span>

<span class="c1"># !test -d bertviz_repo &amp;&amp; echo "FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo"</span>
<span class="c1"># # !rm -r bertviz_repo # Uncomment if you need a clean pull from repo</span>
<span class="c1"># !test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo</span>
<span class="c1"># if not 'bertviz_repo' in sys.path:</span>
<span class="c1">#   sys.path += ['bertviz_repo']</span>
<span class="c1"># !pip install regex</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from bertviz import attention, visualization</span>
<span class="c1"># from bertviz.pytorch_pretrained_bert import BertModel as VizBertModel</span>
<span class="c1"># from bertviz.pytorch_pretrained_bert import BertTokenizer as VizBertTokenizer</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># %%javascript</span>
<span class="c1"># require.config({</span>
<span class="c1">#   paths: {</span>
<span class="c1">#       d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'</span>
<span class="c1">#   }</span>
<span class="c1"># });</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>def call_html():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
<script>
          requirejs.config({
            paths: {
              base: '/static/base',
              "d3": "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min",
              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',
            },
          });
        </script>
        '''))</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># model = VizBertModel.from_pretrained('bert-base-uncased')</span>
<span class="c1"># tokenizer = VizBertTokenizer.from_pretrained('bert-base-uncased')</span>
<span class="c1"># sentence_a = "The dog crossed the road ."</span>
<span class="c1"># sentence_b = "The owner came out and put him on a leash ."</span>
<span class="c1"># attention_visualizer = visualization.AttentionVisualizer(model, tokenizer)</span>
<span class="c1"># tokens_a, tokens_b, attn = attention_visualizer.get_viz_data(sentence_a, sentence_b)</span>
<span class="c1"># call_html()</span>
<span class="c1"># attention.show(tokens_a, tokens_b, attn)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Try playing around with <code>sentence_a</code> and <code>sentence_b</code>.  You can select and unselect different attention heads, as well as the layer that you are visualizing.  There is a lot going on here.  <a href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77">This blog post</a>  by Jesse Vig, the author of the software we are using to render the attention heads above, discusses how BERT attention heads learn various types of attention.  <a href="https://arxiv.org/abs/1906.04341">Clark et al 2019 have a paper</a> that also delves into what learns, particular in the context of our linguistic notions of syntax</p>
<p>It turns out BERT learns a lot of stuff:</p>
<ul>
<li><p><strong>next/previous/identical word tracking</strong></p>
</li>
<li><p><strong>stuff that correlates closely to linguistic notions of syntax</strong>:</p>
<ul>
<li>BERT attention heads learn something like coreference</li>
<li>BERT attention heads learn some approximation of dependency parsing.  Different attention heads learn different dependency/governor relationships</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Multi-Headed-Attention-is-easy-now-in-PyTorch!!">Multi-Headed Attention is easy now in PyTorch!!<a class="anchor-link" href="#Multi-Headed-Attention-is-easy-now-in-PyTorch!!">&para;</a></h4><p>This operation is now built into PyTorch.  There is a caveat that only scaled-dot product attention is supported.  The code above does not use that module since it supports both scaled and unscaled attention.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Positional-embeddings_1">Positional embeddings<a class="anchor-link" href="#Positional-embeddings">&para;</a></h3><p>To eliminate auto-regressive (RNN) models from the transformer, positional embeddings need to be created and added to the word embeddings.  Otherwise, during attention there would be no way to account for word position. There are several ways to support positional embeddings.</p>
<p>The first way is very simple -- you just need to create a <code>nn.Embedding</code> that you give your offsets for each token.  Embedding representations will be learned for each position, but you can only learn up to the number of positions you have seen.</p>
<p>Another way, used in the original Transformer is to embed a bunch of sinusoids with different frequencies that are a function of the position:</p>
<p>$$PE_{(pos,2i)}=sin(pos/10000^{2i}/dmodel)$$
$$PE_{(pos,2i+1)}=cos(pos/10000^{2i}/dmodel)$$</p>
<p>where $pos$ is the position and $i$ is the dimension corresponding to a sinusoid. The wavelengths form a geometric progression from $2\pi$ to $10000\times2\pi$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BERT_1">BERT<a class="anchor-link" href="#BERT">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this section of the tutorial, we are going to fine-tune BERT <a href="https://arxiv.org/abs/1810.04805">Devlin et al 2018</a>, a transformer architecture that replaces the causal LM objective with 2 new objectives:</p>
<ol>
<li><strong>Masking out words</strong> with some probability, predict the missing words (MLM objective)</li>
</ol>
<p><img alt="MLM" src="https://2.bp.blogspot.com/-pNxcHHXNZg0/W9iv3evVyOI/AAAAAAAADfA/KTSvKXNzzL0W8ry28PPl7nYI1CG_5WuvwCLcBGAs/s1600/f1.png"/></p>
<ol>
<li>Given 2 adjacent sentences, <strong>predict if the second sentence follows the first</strong> (NSP objective)</li>
</ol>
<p><img alt="NSP" src="https://4.bp.blogspot.com/-K_7yu3kjF18/W9iv-R-MnyI/AAAAAAAADfE/xUwR_G1iTY0vq9X-Z3LnW5t4NLS9BQzdgCLcBGAs/s1600/f2.png"/></p>
<p>From an architecture diagram, <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">this blog post announcing BERT</a> notes the differences:</p>
<p><img alt="BERT vs GPT and ELMo" src="https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s1600/image3.png"/></p>
<p>Our model will simply build on the existing model architecture with a single transformation layer to the output number of classes.  BERT is <a href="https://github.com/google-research/bert">open source</a> but the code is in TensorFlow, and since this tutorial is written in PyTorch, we need a different solution.  We will use the <a href="https://github.com/huggingface/pytorch-pretrained-BERT">Hugging Face Transformer codebase</a> as our API -- it can read in the original Google-trained weights.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pip install pytorch-pretrained-bert</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import numpy as np</span>
<span class="c1"># import torch</span>
<span class="c1"># import torch.nn as nn</span>
<span class="c1"># import torch.nn.functional as F</span>
<span class="c1"># import io</span>
<span class="c1"># import os</span>
<span class="c1"># import re</span>
<span class="c1"># import codecs</span>
<span class="c1"># from collections import Counter</span>
<span class="c1"># from torch.utils.data import DataLoader, TensorDataset</span>
<span class="c1"># from pytorch_pretrained_bert.tokenization import BertTokenizer</span>
<span class="c1"># from pytorch_pretrained_bert.modeling import BertModel</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenization-in-BERT">Tokenization in BERT<a class="anchor-link" href="#Tokenization-in-BERT">&para;</a></h3><p>In the last sequence, we talked about how ELMo biLMs can limit their parameters while accounting for unseen words using character-compositional word embeddings.  This technique is very powerful, but its also slow.  It is common in NMT to use some sort of sub-word encoding that limits the vocabulary size, but allows us to not have unattested words.  The <code>tensor2tensor</code> codebase, for example, creates an invertible encoding for words into sub-tokens with a limited vocabulary.  The tokenizer is built from a corpus upfront and stored in a file, and then can be used to encode text.</p>
<p>There are 4 phases in this algorithm described in the tensor2tensor codebase:</p>
<pre><code>1. Tokenize into a list of tokens.  Each token is a unicode string of either
  all alphanumeric characters or all non-alphanumeric characters.  We drop
  tokens consisting of a single space that are between two alphanumeric
  tokens.
2. Escape each token.  This escapes away special and out-of-vocabulary
  characters, and makes sure that each token ends with an underscore, and
  has no other underscores.
3. Represent each escaped token as a the concatenation of a list of subtokens
  from the limited vocabulary.  Subtoken selection is done greedily from
  beginning to end.  That is, we construct the list in order, always picking
  the longest subtoken in our vocabulary that matches a prefix of the
  remaining portion of the encoded token.
4. Concatenate these lists.  This concatenation is invertible due to the
  fact that the trailing underscores indicate when one list is finished.



</code></pre>
<p>We can access Google's trained BERT Tokenizer via the Hugging Face API</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bert-tokenizer-/-vectorizer">Bert tokenizer / vectorizer<a class="anchor-link" href="#Bert-tokenizer-/-vectorizer">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our model this time around is very simple.  It has an output linear layer that comes from pooled output from BERT</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def whitespace_tokenizer(words):</span>
<span class="c1">#     return words.split() </span>

<span class="c1"># def sst2_tokenizer(words):</span>
<span class="c1">#     REPLACE = { "'s": " 's ",</span>
<span class="c1">#                 "'ve": " 've ",</span>
<span class="c1">#                 "n't": " n't ",</span>
<span class="c1">#                 "'re": " 're ",</span>
<span class="c1">#                 "'d": " 'd ",</span>
<span class="c1">#                 "'ll": " 'll ",</span>
<span class="c1">#                 ",": " , ",</span>
<span class="c1">#                 "!": " ! ",</span>
<span class="c1">#                 }</span>
<span class="c1">#     words = words.lower()</span>
<span class="c1">#     words = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", words)</span>
<span class="c1">#     for k, v in REPLACE.items():</span>
<span class="c1">#             words = words.replace(k, v)</span>
<span class="c1">#     return [w.strip() for w in words.split()]</span>

<span class="c1"># BERT_TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')</span>
<span class="c1"># BERT_MODEL = BertModel.from_pretrained('bert-base-uncased')</span>
<span class="c1"># def bert_tokenizer(words, pretokenizer=whitespace_tokenizer):</span>
<span class="c1">#     subwords = ['[CLS]']</span>
<span class="c1">#     for word in pretokenizer(words):</span>
<span class="c1">#         if word == '&lt;unk&gt;':</span>
<span class="c1">#             subword = '[UNK]'</span>
<span class="c1">#         else:</span>
<span class="c1">#             subword = BERT_TOKENIZER.tokenize(word)</span>
<span class="c1">#         subwords += subword</span>
<span class="c1">#     return subwords + ['[SEP]']</span>

<span class="c1"># def bert_vectorizer(sentence):</span>
<span class="c1">#     return BERT_TOKENIZER.convert_tokens_to_ids(sentence)</span>
<span class="c1">#     #return [BERT_TOKENIZER.vocab.get(subword, BERT_TOKENIZER.vocab['[PAD]']) for subword in sentence]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># words = "National Taiwan University"</span>
<span class="c1"># print("words:")</span>
<span class="c1"># print(words)</span>
<span class="c1"># print()</span>
<span class="c1"># print("BERT_TOKENIZER.tokenize(words):")</span>
<span class="c1"># print(BERT_TOKENIZER.tokenize(words))</span>
<span class="c1"># print()</span>
<span class="c1"># print("bert_tokenizer(words):")</span>
<span class="c1"># tokens = bert_tokenizer(words)</span>
<span class="c1"># print(tokens)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># print(tokens)</span>
<span class="c1"># ids = BERT_TOKENIZER.convert_tokens_to_ids(tokens)</span>
<span class="c1"># print(ids)</span>
<span class="c1"># print(BERT_TOKENIZER.convert_ids_to_tokens(ids))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="FineTuneClassifier">FineTuneClassifier<a class="anchor-link" href="#FineTuneClassifier">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class FineTuneClassifier(nn.Module):</span>

<span class="c1">#     def __init__(self, base_model, num_classes, embed_dim, hidden_units=[]):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.base_model = base_model</span>
<span class="c1">#         input_units = embed_dim</span>
<span class="c1">#         output_units = embed_dim</span>
<span class="c1">#         sequence = []</span>
<span class="c1">#         for h in hidden_units:</span>
<span class="c1">#             sequence.append(nn.Linear(input_units, h))</span>
<span class="c1">#             input_units = h</span>
<span class="c1">#             output_units = h</span>
            
<span class="c1">#         sequence.append(nn.Linear(output_units, num_classes))</span>
<span class="c1">#         self.outputs = nn.Sequential(*sequence)</span>

<span class="c1">#     def forward(self, inputs):</span>
<span class="c1">#         x, lengths = inputs</span>
        
<span class="c1">#         input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1)</span>
<span class="c1">#         input_type_ids = torch.zeros(x.shape, device=x.device, dtype=torch.long)</span>
<span class="c1">#         _, pooled = self.base_model(x, token_type_ids=input_type_ids, attention_mask=input_mask)</span>
        
<span class="c1">#         stacked = self.outputs(pooled)</span>
<span class="c1">#         return F.log_softmax(stacked, dim=-1)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All the rest of our code comes from the previous sections</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import torch</span>
<span class="c1"># import torch.nn as nn</span>
<span class="c1"># import torch.nn.functional as F</span>
<span class="c1"># from typing import List, Tuple</span>
<span class="c1"># import os</span>
<span class="c1"># import io</span>
<span class="c1"># import re</span>
<span class="c1"># import codecs</span>
<span class="c1"># import numpy as np</span>
<span class="c1"># from collections import Counter</span>
<span class="c1"># from torch.utils.data import DataLoader, TensorDataset</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ConfusionMatrix">ConfusionMatrix<a class="anchor-link" href="#ConfusionMatrix">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class ConfusionMatrix:</span>
<span class="c1">#     """Confusion matrix with metrics</span>

<span class="c1">#     This class accumulates classification output, and tracks it in a confusion matrix.</span>
<span class="c1">#     Metrics are available that use the confusion matrix</span>
<span class="c1">#     """</span>
<span class="c1">#     def __init__(self, labels):</span>
<span class="c1">#         """Constructor with input labels</span>

<span class="c1">#         :param labels: Either a dictionary (`k=int,v=str`) or an array of labels</span>
<span class="c1">#         """</span>
<span class="c1">#         if type(labels) is dict:</span>
<span class="c1">#             self.labels = []</span>
<span class="c1">#             for i in range(len(labels)):</span>
<span class="c1">#                 self.labels.append(labels[i])</span>
<span class="c1">#         else:</span>
<span class="c1">#             self.labels = labels</span>
<span class="c1">#         nc = len(self.labels)</span>
<span class="c1">#         self._cm = np.zeros((nc, nc), dtype=np.int)</span>

<span class="c1">#     def add(self, truth, guess):</span>
<span class="c1">#         """Add a single value to the confusion matrix based off `truth` and `guess`</span>

<span class="c1">#         :param truth: The real `y` value (or ground truth label)</span>
<span class="c1">#         :param guess: The guess for `y` value (or assertion)</span>
<span class="c1">#         """</span>

<span class="c1">#         self._cm[truth, guess] += 1</span>

<span class="c1">#     def __str__(self):</span>
<span class="c1">#         values = []</span>
<span class="c1">#         width = max(8, max(len(x) for x in self.labels) + 1)</span>
<span class="c1">#         for i, label in enumerate([''] + self.labels):</span>
<span class="c1">#             values += ["{:&gt;{width}}".format(label, width=width+1)]</span>
<span class="c1">#         values += ['\n']</span>
<span class="c1">#         for i, label in enumerate(self.labels):</span>
<span class="c1">#             values += ["{:&gt;{width}}".format(label, width=width+1)]</span>
<span class="c1">#             for j in range(len(self.labels)):</span>
<span class="c1">#                 values += ["{:{width}d}".format(self._cm[i, j], width=width + 1)]</span>
<span class="c1">#             values += ['\n']</span>
<span class="c1">#         values += ['\n']</span>
<span class="c1">#         return ''.join(values)</span>

<span class="c1">#     def save(self, outfile):</span>
<span class="c1">#         ordered_fieldnames = OrderedDict([("labels", None)] + [(l, None) for l in self.labels])</span>
<span class="c1">#         with open(outfile, 'w') as f:</span>
<span class="c1">#             dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)</span>
<span class="c1">#             dw.writeheader()</span>
<span class="c1">#             for index, row in enumerate(self._cm):</span>
<span class="c1">#                 row_dict = {l: row[i] for i, l in enumerate(self.labels)}</span>
<span class="c1">#                 row_dict.update({"labels": self.labels[index]})</span>
<span class="c1">#                 dw.writerow(row_dict)</span>

<span class="c1">#     def reset(self):</span>
<span class="c1">#         """Reset the matrix</span>
<span class="c1">#         """</span>
<span class="c1">#         self._cm *= 0</span>

<span class="c1">#     def get_correct(self):</span>
<span class="c1">#         """Get the diagonals of the confusion matrix</span>

<span class="c1">#         :return: (``int``) Number of correct classifications</span>
<span class="c1">#         """</span>
<span class="c1">#         return self._cm.diagonal().sum()</span>

<span class="c1">#     def get_total(self):</span>
<span class="c1">#         """Get total classifications</span>

<span class="c1">#         :return: (``int``) total classifications</span>
<span class="c1">#         """</span>
<span class="c1">#         return self._cm.sum()</span>

<span class="c1">#     def get_acc(self):</span>
<span class="c1">#         """Get the accuracy</span>

<span class="c1">#         :return: (``float``) accuracy</span>
<span class="c1">#         """</span>
<span class="c1">#         return float(self.get_correct())/self.get_total()</span>

<span class="c1">#     def get_recall(self):</span>
<span class="c1">#         """Get the recall</span>

<span class="c1">#         :return: (``float``) recall</span>
<span class="c1">#         """</span>
<span class="c1">#         total = np.sum(self._cm, axis=1)</span>
<span class="c1">#         total = (total == 0) + total</span>
<span class="c1">#         return np.diag(self._cm) / total.astype(float)</span>

<span class="c1">#     def get_support(self):</span>
<span class="c1">#         return np.sum(self._cm, axis=1)</span>

<span class="c1">#     def get_precision(self):</span>
<span class="c1">#         """Get the precision</span>
<span class="c1">#         :return: (``float``) precision</span>
<span class="c1">#         """</span>

<span class="c1">#         total = np.sum(self._cm, axis=0)</span>
<span class="c1">#         total = (total == 0) + total</span>
<span class="c1">#         return np.diag(self._cm) / total.astype(float)</span>

<span class="c1">#     def get_mean_precision(self):</span>
<span class="c1">#         """Get the mean precision across labels</span>

<span class="c1">#         :return: (``float``) mean precision</span>
<span class="c1">#         """</span>
<span class="c1">#         return np.mean(self.get_precision())</span>

<span class="c1">#     def get_weighted_precision(self):</span>
<span class="c1">#         return np.sum(self.get_precision() * self.get_support())/float(self.get_total())</span>

<span class="c1">#     def get_mean_recall(self):</span>
<span class="c1">#         """Get the mean recall across labels</span>

<span class="c1">#         :return: (``float``) mean recall</span>
<span class="c1">#         """</span>
<span class="c1">#         return np.mean(self.get_recall())</span>

<span class="c1">#     def get_weighted_recall(self):</span>
<span class="c1">#         return np.sum(self.get_recall() * self.get_support())/float(self.get_total())</span>

<span class="c1">#     def get_weighted_f(self, beta=1):</span>
<span class="c1">#         return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())</span>

<span class="c1">#     def get_macro_f(self, beta=1):</span>
<span class="c1">#         """Get the macro F_b, with adjustable beta (defaulting to F1)</span>

<span class="c1">#         :param beta: (``float``) defaults to 1 (F1)</span>
<span class="c1">#         :return: (``float``) macro F_b</span>
<span class="c1">#         """</span>
<span class="c1">#         if beta &lt; 0:</span>
<span class="c1">#             raise Exception('Beta must be greater than 0')</span>
<span class="c1">#         return np.mean(self.get_class_f(beta))</span>

<span class="c1">#     def get_class_f(self, beta=1):</span>
<span class="c1">#         p = self.get_precision()</span>
<span class="c1">#         r = self.get_recall()</span>

<span class="c1">#         b = beta*beta</span>
<span class="c1">#         d = (b * p + r)</span>
<span class="c1">#         d = (d == 0) + d</span>

<span class="c1">#         return (b + 1) * p * r / d</span>

<span class="c1">#     def get_f(self, beta=1):</span>
<span class="c1">#         """Get 2 class F_b, with adjustable beta (defaulting to F1)</span>

<span class="c1">#         :param beta: (``float``) defaults to 1 (F1)</span>
<span class="c1">#         :return: (``float``) 2-class F_b</span>
<span class="c1">#         """</span>
<span class="c1">#         p = self.get_precision()[1]</span>
<span class="c1">#         r = self.get_recall()[1]</span>
<span class="c1">#         if beta &lt; 0:</span>
<span class="c1">#             raise Exception('Beta must be greater than 0')</span>
<span class="c1">#         d = (beta*beta * p + r)</span>
<span class="c1">#         if d == 0:</span>
<span class="c1">#             return 0</span>
<span class="c1">#         return (beta*beta + 1) * p * r / d</span>

<span class="c1">#     def get_all_metrics(self):</span>
<span class="c1">#         """Make a map of metrics suitable for reporting, keyed by metric name</span>

<span class="c1">#         :return: (``dict``) Map of metrics keyed by metric names</span>
<span class="c1">#         """</span>
<span class="c1">#         metrics = {'acc': self.get_acc()}</span>
<span class="c1">#         # If 2 class, assume second class is positive AKA 1</span>
<span class="c1">#         if len(self.labels) == 2:</span>
<span class="c1">#             metrics['precision'] = self.get_precision()[1]</span>
<span class="c1">#             metrics['recall'] = self.get_recall()[1]</span>
<span class="c1">#             metrics['f1'] = self.get_f(1)</span>
<span class="c1">#         else:</span>
<span class="c1">#             metrics['mean_precision'] = self.get_mean_precision()</span>
<span class="c1">#             metrics['mean_recall'] = self.get_mean_recall()</span>
<span class="c1">#             metrics['macro_f1'] = self.get_macro_f(1)</span>
<span class="c1">#             metrics['weighted_precision'] = self.get_weighted_precision()</span>
<span class="c1">#             metrics['weighted_recall'] = self.get_weighted_recall()</span>
<span class="c1">#             metrics['weighted_f1'] = self.get_weighted_f(1)</span>
<span class="c1">#         return metrics</span>

<span class="c1">#     def add_batch(self, truth, guess):</span>
<span class="c1">#         """Add a batch of data to the confusion matrix</span>

<span class="c1">#         :param truth: The truth tensor</span>
<span class="c1">#         :param guess: The guess tensor</span>
<span class="c1">#         :return:</span>
<span class="c1">#         """</span>
<span class="c1">#         for truth_i, guess_i in zip(truth, guess):</span>
<span class="c1">#             self.add(truth_i, guess_i)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Trainer">Trainer<a class="anchor-link" href="#Trainer">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class Trainer:</span>
<span class="c1">#     def __init__(self, optimizer: torch.optim.Optimizer):</span>
<span class="c1">#         self.optimizer = optimizer</span>

<span class="c1">#     def run(self, model, labels, train, loss, batch_size): </span>
<span class="c1">#         model.train()       </span>
<span class="c1">#         train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)</span>

<span class="c1">#         cm = ConfusionMatrix(labels)</span>

<span class="c1">#         for batch in train_loader:</span>
<span class="c1">#             loss_value, y_pred, y_actual = self.update(model, loss, batch)</span>
<span class="c1">#             _, best = y_pred.max(1)</span>
<span class="c1">#             yt = y_actual.cpu().int().numpy()</span>
<span class="c1">#             yp = best.cpu().int().numpy()</span>
<span class="c1">#             cm.add_batch(yt, yp)</span>

<span class="c1">#         print(cm.get_all_metrics())</span>
<span class="c1">#         return cm</span>
    
<span class="c1">#     def update(self, model, loss, batch):</span>
<span class="c1">#         self.optimizer.zero_grad()</span>
<span class="c1">#         x, lengths, y = batch</span>
<span class="c1">#         lengths, perm_idx = lengths.sort(0, descending=True)</span>
<span class="c1">#         x_sorted = x[perm_idx]</span>
<span class="c1">#         y_sorted = y[perm_idx]</span>
<span class="c1">#         y_sorted = y_sorted.to('cuda:0')</span>
<span class="c1">#         inputs = (x_sorted.to('cuda:0'), lengths)</span>
<span class="c1">#         y_pred = model(inputs)</span>
<span class="c1">#         loss_value = loss(y_pred, y_sorted)</span>
<span class="c1">#         loss_value.backward()</span>
<span class="c1">#         self.optimizer.step()</span>
<span class="c1">#         return loss_value.item(), y_pred, y_sorted</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluator">Evaluator<a class="anchor-link" href="#Evaluator">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class Evaluator:</span>
<span class="c1">#     def __init__(self):</span>
<span class="c1">#         pass</span>

<span class="c1">#     def run(self, model, labels, dataset, batch_size=1):</span>
<span class="c1">#         model.eval()</span>
<span class="c1">#         valid_loader = DataLoader(dataset, batch_size=batch_size)</span>
<span class="c1">#         cm = ConfusionMatrix(labels)</span>
<span class="c1">#         for batch in valid_loader:</span>
<span class="c1">#             y_pred, y_actual = self.inference(model, batch)</span>
<span class="c1">#             _, best = y_pred.max(1)</span>
<span class="c1">#             yt = y_actual.cpu().int().numpy()</span>
<span class="c1">#             yp = best.cpu().int().numpy()</span>
<span class="c1">#             cm.add_batch(yt, yp)</span>
<span class="c1">#         return cm</span>

<span class="c1">#     def inference(self, model, batch):</span>
<span class="c1">#         with torch.no_grad():</span>
<span class="c1">#             x, lengths, y = batch</span>
<span class="c1">#             lengths, perm_idx = lengths.sort(0, descending=True)</span>
<span class="c1">#             x_sorted = x[perm_idx]</span>
<span class="c1">#             y_sorted = y[perm_idx]</span>
<span class="c1">#             y_sorted = y_sorted.to('cuda:0')</span>
<span class="c1">#             inputs = (x_sorted.to('cuda:0'), lengths)</span>
<span class="c1">#             y_pred = model(inputs)</span>
<span class="c1">#             return y_pred, y_sorted</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="fit()">fit()<a class="anchor-link" href="#fit()">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def fit(model, labels, optimizer, loss, epochs, batch_size, train, valid, test):</span>

<span class="c1">#     trainer = Trainer(optimizer)</span>
<span class="c1">#     evaluator = Evaluator()</span>
<span class="c1">#     best_acc = 0.0</span>
    
<span class="c1">#     for epoch in range(epochs):</span>
<span class="c1">#         print('EPOCH {}'.format(epoch + 1))</span>
<span class="c1">#         print('=================================')</span>
<span class="c1">#         print('Training Results')</span>
<span class="c1">#         cm = trainer.run(model, labels, train, loss, batch_size)</span>
<span class="c1">#         print('Validation Results')</span>
<span class="c1">#         cm = evaluator.run(model, labels, valid)</span>
<span class="c1">#         print(cm.get_all_metrics())</span>
<span class="c1">#         if cm.get_acc() &gt; best_acc:</span>
<span class="c1">#             print('New best model {:.2f}'.format(cm.get_acc()))</span>
<span class="c1">#             best_acc = cm.get_acc()</span>
<span class="c1">#             torch.save(model.state_dict(), './checkpoint.pth')</span>
<span class="c1">#     if test:</span>
<span class="c1">#         model.load_state_dict(torch.load('./checkpoint.pth'))</span>
<span class="c1">#         cm = evaluator.run(model, labels, test)</span>
<span class="c1">#         print('Final result')</span>
<span class="c1">#         print(cm.get_all_metrics())</span>
<span class="c1">#     return cm.get_acc()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train">Train<a class="anchor-link" href="#Train">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># BASE = 'trec'</span>
<span class="c1"># TRAIN = os.path.join(BASE, 'trec.nodev.utf8')</span>
<span class="c1"># VALID = os.path.join(BASE, 'trec.dev.utf8')</span>
<span class="c1"># TEST = os.path.join(BASE, 'trec.test.utf8')</span>

<span class="c1"># # lowercase=False so we can defer to BERT's tokenizer to handle</span>
<span class="c1"># r = Reader((TRAIN, VALID, TEST,), lowercase=False, vectorizer=bert_vectorizer, tokenizer=bert_tokenizer)</span>
<span class="c1"># train = r.load(TRAIN)</span>
<span class="c1"># valid = r.load(VALID)</span>
<span class="c1"># test = r.load(TEST)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># bert_small_dims = 768</span>
<span class="c1"># batch_size = 50</span>
<span class="c1"># epochs = 12</span>
<span class="c1"># model = FineTuneClassifier(BERT_MODEL, len(r.labels), bert_small_dims)</span>
<span class="c1"># num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)</span>
<span class="c1"># print(f"Model has {num_params} parameters") </span>


<span class="c1"># model.to('cuda:0')</span>
<span class="c1"># loss = torch.nn.NLLLoss()</span>
<span class="c1"># loss = loss.to('cuda:0')</span>

<span class="c1"># learnable_params = [p for p in model.parameters() if p.requires_grad]</span>
<span class="c1"># optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)</span>

<span class="c1"># fit(model, r.labels, optimizer, loss, epochs, batch_size, train, valid, test)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># train.tensors</span>
</pre></div>
</div>
</div>
</div>
</div>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="https://leemeng.tw/tag/zi-ran-yu-yan-chu-li.html" rel="tag">自然語言處理</a>
                        <a href="https://leemeng.tw/tag/nlp.html" rel="tag">NLP</a>
                        <a href="https://leemeng.tw/tag/pytorch.html" rel="tag">Pytorch</a>
                    </span>

                </p>



                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary " style="color: #FFFFFF">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__next">
                            <a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html" rel="next">
                                <span>Next Post</span>
                                淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中
                            </a>
                        </div>
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '進擊的 BERT：運用自然語言處理的巨人之力';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
                </div>

                <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>                </ul>
            </div>
        </div>

        <div class="row footer-bottom">
            <div class="col-twelve">
                <div class="copyright">
                    <span>Powered by <a href="http://getpelican.com/" target="_blank">Pelican</a></span>
                    <span>© Copyright Hola 2017</span>
                    <span>Design by <a href="https://www.styleshout.com/" target="_blank">styleshout</a></span>
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>
        </div> <!-- end footer-bottom -->
    </footer> <!-- end footer -->


    <div id="preloader">
        <div id="loader"></div>
    </div>

        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script>


</body>
</html>