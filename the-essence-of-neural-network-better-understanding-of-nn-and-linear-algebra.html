<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 神經網路的本質：直觀理解神經網路 & 線性代數</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="s" />
        <meta name="keywords" content="Manim, Python" />
        <meta name="tags" content="Manim" />
        <meta name="tags" content="Python" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="/theme/images/background/TwoLayersReLUInBetweenSolveHardTwoCurves.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html" />
        <meta property="og:title" content="神經網路的本質：直觀理解神經網路 & 線性代數" />
        <meta property="og:description" content="s" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="/theme/css/toc-nav.css">

    <!--for responsive embed youtube video-->
        <link rel="stylesheet" type="text/css" href="/theme/css/embed_youtube.css">

    <!--for prettify dark-mode result-->
        <link rel="stylesheet" type="text/css" href="/theme/css/darkmode.css">

    <link rel="stylesheet" type="text/css" href="/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="/theme/js/modernizr.js"></script>
    <script src="/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="神經網路的本質：直觀理解神經網路 &amp; 線性代數">神經網路的本質：直觀理解神經網路 &amp; 線性代數</a><ul><li><a class="toc-href" href="#一些會有幫助的背景知識" title="一些會有幫助的背景知識">一些會有幫助的背景知識</a></li><li><a class="toc-href" href="#深度學習框架操作容易，但你真的了解神經網路嗎？" title="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？</a></li><li><a class="toc-href" href="#用二元分類連結神經網路-&amp;-線性代數" title="用二元分類連結神經網路 &amp; 線性代數">用二元分類連結神經網路 &amp; 線性代數</a></li><li><a class="toc-href" href="#能夠解決二元分類的神經網路長得是什麼模樣？" title="能夠解決二元分類的神經網路長得是什麼模樣？">能夠解決二元分類的神經網路長得是什麼模樣？</a></li><li><a class="toc-href" href="#結論" title="結論">結論</a></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(/theme/images/background/TwoLayersReLUInBetweenSolveHardTwoCurves.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="/tag/manim.html" rel="tag">Manim</a>
                            <a href="/tag/python.html" rel="tag">Python</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html" title="">
                            神經網路的本質：直觀理解神經網路 & 線性代數
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-09-21 (Sat)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>



    <!--custom images with icon shown on left nav-->

</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這是篇幫助你直觀理解神經網路的科普文。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定深度學習之旅的基礎。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>（小提醒：因本文圖片與動畫皆為黑色背景，強烈推薦用左下按鈕以 Dark Mode 閱讀本文）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是個眾人對人工智慧（<strong>A</strong>rtificial <strong>I</strong>ntelligence, AI）趨之若鶩的時代。此領域近年的蓬勃發展很大一部份得歸功於<a href="https://leemeng.tw/deep-learning-resources.html">深度學習</a>以及<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神經網路</a>的研究。現行的深度學習框架（framework）也日漸成熟，讓任何人都可以使用 <a href="https://www.tensorflow.org/overview">TensorFlow</a> 或 <a href="https://pytorch.org/">PyTorch</a> 輕鬆建立神經網路，解決各式各樣的問題。</p>
<p>舉例而言，你在 30 秒內就可訓練出一個能夠辨識數字的神經網路：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="c1"># 此例使用 TensorFlow，但各大深度學習框架的實現邏輯基本上類似</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="c1"># 載入深度學習 Hello World: MNIST 數字 dataset</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># 建立一個約有 10 萬個參數的「小型」神經網路</span>
<span class="c1"># 在現在模型參數動輒上千萬、上億的年代，此神經網路不算大</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># 選擇損失函數、optimizer</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="c1"># 訓練模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 訓練後的 NN 在測試集上可得到近 98% 正確辨識率</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># 實際測試結果</span>
<span class="c1"># loss: 0.0750 - accuracy: 0.9763</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>是的，扣除註解不到 15 行就可以把讀取數據、訓練 <code>model</code> 以及推論全部搞定了。這邊秀出程式碼只是要讓你感受一下透過框架建立一個神經網路有多麽地「簡單」。事實上，這也是絕大多數線上課程以及教學文章<strong>能</strong>教你的東西。對此數字辨識應用有興趣的讀者稍後也可自行參考<a href="https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/beginner.ipynb"> TensorFlow 的 Colab 筆記本</a>。</p>
<p>我等等要秀給你看的任何一個神經網路都要比 <code>model</code> 還簡單個一萬倍，但了解並觀察這些神經網路的運作將成為你的 AI 旅程中最有趣的經驗之一。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="一些會有幫助的背景知識">一些會有幫助的背景知識<a class="anchor-link" href="#一些會有幫助的背景知識">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>本文將透過不少動畫帶你理解神經網路（<strong>N</strong>eural <strong>N</strong>etwork, 後簡稱為 NN）與<a href="https://www.youtube.com/watch?v=fNk_zzaMoSs">線性代數（Linear Algebra）</a>之間的緊密關係。不過具備些背景知識將能讓你有更深的體會：</p>
<ol>
<li>能夠讀懂文章開頭建立 NN 的 <a href="https://www.python.org/">Python</a> 程式碼</li>
<li>了解線上課程都會教的<a href="https://www.youtube.com/watch?v=Dr-WRlEFefw">超基本 NN 概念</a><ul>
<li>何謂<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（Fully Connected Layer）</a></li>
<li>何謂參數以及如何估計全連接層的參數量</li>
<li>常見的 activation functions 如 <a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">ReLU</a></li>
</ul>
</li>
<li>基本的線性代數概念如矩陣相乘、向量空間</li>
</ol>
<p>別擔心，這些是 nice-to-have，就算你沒背景知識也能直接繼續閱讀。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/00010.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        一個簡單 NN 嘗試解決二元分類的過程（線性轉換）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外因為篇幅有限，本文會把焦點放在<strong>已經訓練好</strong>的 NN，不會特別說明<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">訓練神經網路</a>的細節。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？<a class="anchor-link" href="#深度學習框架操作容易，但你真的了解神經網路嗎？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>讓我們再次回到文章開頭透過 <a href="https://www.tensorflow.org/guide/keras">Keras</a> 建立的神經網路 <code>model</code>：</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
<p>就跟你在多數深度學習文章裡頭看到的，現在要使用深度學習框架建立<strong>基本的</strong> NN 非常容易，只要當作疊疊樂一個個 layer 疊上去就好了。下圖則將此 <code>model</code> 用視覺上更容易理解的方式呈現：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="/images/manim/mnist-simple.jpg"/>
</center>
<center>
                        輸入是 28*28 = 784 維的圖片像素，輸出則是 10 個數字類別的簡單 2-layers NN
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>透過 API，我們也可以輕鬆查看整個 NN 的參數量：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               100480    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>儘管擁有 10 萬個可訓練參數，此 <code>model</code> 是個在深度學習領域裡只能被歸類在 Hello World 等級的可憐 NN。畢竟這世界很瘋狂，<a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">我們以前討論過的 BERT</a> 以及 <a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">GPT-2</a> 都是現在 NLP 界的知名語言模型，而它們可都是擁有<strong>上億</strong>參數的強大 NN。那可是此 <code>model</code> 的 100 倍大。</p>
<p>但先別管 BERT 或 GPT-2 了，就算是這個 Hello World 等級的 NN，你真的覺得你對它的運作機制有足夠的理解嗎？</p>
<p>講白點，儘管現在路上隨便拉個人都會使用 TensorFlow 或是 PyTorch 來建立神經網路，我認為許多人（包含剛入門的我）對最基本的神經網路都沒有足夠<strong>直觀</strong>的理解。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="/images/manim/three-key-components.jpg"/>
</center>
<center>
                        構成本文的關鍵三要素：矩陣運算、二元分類以及神經網路
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了讓你能夠更直觀地理解神經網路，我將透過二元分類（Binary Classification）任務說明神經網路以及線性代數之間的緊密關係。前言很長，但如果你想要了解神經網路的本質，或是想要為自己之後的 AI 之旅打下良好基礎，那我會建議你繼續往下閱讀：）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="用二元分類連結神經網路-&amp;-線性代數">用二元分類連結神經網路 &amp; 線性代數<a class="anchor-link" href="#用二元分類連結神經網路-&amp;-線性代數">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://en.wikipedia.org/wiki/Binary_classification">二元分類</a>是<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html">機器學習（Machine Learning）</a>領域裡一個十分基本的任務，其目標是把一個集合裡的所有數據點（data points）依照某種分類規則劃分成<strong>兩</strong>個族群（groups）或類別（classes）。比方我們之間看過的<a href="https://demo.leemeng.tw/">貓狗圖像辨識</a>。不過在這篇文章裡，我假設所有數據點<strong>最多只有 2 個維度（dimensions）</strong>。</p>
<p>如果你讀過之前的文章，可能會覺得這假設是在「羞辱」我們。畢竟我們已用過神經網路來：</p>
<ul>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">執行假新聞偵測（BERT）</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">生成新金庸小說（GPT-2）</a></li>
<li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html">把英文翻成中文（Transformer）</a></li>
<li><a href="https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html">生成新海誠動畫（CartoonGAN）</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>以上這些進階任務跟本文的 <strong>2 維</strong>二元分類相比要複雜許多，自然也十分有趣且具有挑戰性。如果你有興趣深入了解，稍後可以點擊連結來查看相關文章。但這篇之所以用二元分類作為目標任務是因其十分單純，所以我們能夠透過它來一窺神經網路的本質。</p>
<p>以下是一個資料集：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/ShowLinearSeparableDataPoints.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        包含兩類別（曲線）的 2 維資料集
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在此資料集裡，兩曲線分別來自不同類別，各自包含 100 個數據點 $x$。每個數據點 $x$ 也可以很自然地用 2 維的 $\left (x_{coord}, y_{coord}  \right )$ 座標來表示。你可以從右下角得知黃點的標籤 $y = 0$，藍點為 $1$。</p>
<p>另外，圖中也描繪了此向量空間中的基底向量（basis vector）：</p>
<ul>
<li>x 軸上藍色的 $\vec{i}$</li>
<li>y 軸上紅色的 $\vec{j}$ </li>
</ul>
<p>那麼要如何分類這個資料集呢？複習<a href="https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html"> AI For Everyone 裡的其中一個重要概念</a>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        目前多數的機器學習以及 AI 應用本質上都是讓電腦學會一個映射函數（Mapping Function），幫我們將輸入的數據 x 轉換到理想的輸出 y。
                        <br/>
<span style="float:right;margin-right: 1.5rem">─ Andrew Ng</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>套用相同概念，要處理這個分類任務，我們要問的問題就變成「給定所有藍點與黃點 $x = \left (x_{coord}, y_{coord}  \right )$，我們能不能找出一個函數 $f$，將這些 2 維數據 $x$ 完美地<strong>轉換</strong>到它們對應的 1 維標籤 $y$ 呢？」</p>
<p>換句話說，我們想要找出一個 $x$ 的函數 $f(x)$，使得以下式子成立：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) 
&amp; = f(\begin{bmatrix} x_{coord} \\ y_{coord} \end{bmatrix}) \\
&amp; = f(\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}) \\
&amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果我們能找到符合這個條件的 $f(x)$，就能瞬間預測出一個數據點 $x$ 離哪個類別比較近了。當然，我們有無窮多種 model $f(x)$ 的方法。但在線性代數的世界裡，我們可以用矩陣運算的形式定義 $f(x)$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) &amp; = W x + b \\
  &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} x + b \\ 
  &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
  &amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>希望你還記得國中、高中或是大學裡任何一位線性代數老師的諄諄教誨。</p>
<p>在上面的式子裡：</p>
<ul>
<li>$x$ 是一個 2 維的 column vector</li>
<li>$W$ 是一個 1 x 2 的<a href="https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%98%B5">權重矩陣（weight matrix）</a></li>
<li>$b$ 為偏差（bias），是一個純量（scalar）</li>
</ul>
<p>如果我們先暫時忽略 $b$，事實上 $f(x)$ 對輸入 $x$ 做的<strong>轉換</strong>跟多數神經網路都會使用到的<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（<strong>F</strong>ully <strong>C</strong>onnected Layer，後稱 FC）</a>的作用是完全相同的。換句話說，使用一層 FC 的 NN 基本上就是在做矩陣運算（假設激勵函式為線性）。因此這個形式的 $f(x)$ 與神經網路之間有非常美好的對應關係：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/SymbolicOneByTwoMatrixAndNN.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        1 x 2 矩陣運算與 1-Layer NN 的對應關係
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>沒錯，透過矩陣運算，我們建立了這世上最簡單的 1-Layer 神經網路！而之所以是 1-Layer，原因在於 NN 的第一層為原始數據（raw data），第二層才是我們實際新定義的神經網路（FC）。矩陣運算跟 FC 的對應關係家喻戶曉，但這應該是你第一次看到兩者共舞。</p>
<p>從<strong>線性代數</strong>的角度來看，我們是透過權重矩陣 $W$ 對 2 維的 $x$ 進行線性轉換後得到 1 維的 $y$；而以<strong>神經網路</strong>的角度檢視，我們則是將以 2 維向量表示（represent）的數據點 $x$ 透過與權重 $W$ 進行加權總和後得到新的 1 維 representation $y$。這是常有人說神經網路在做<a href="https://arxiv.org/abs/1206.5538">表徵學習（Representation Learning）</a>的原因。</p>
<p>另外值得一提的是矩陣 $W$ 裡的參數 $w_{mn}$ 實際上對應到 NN 某一層中第 $n$ 個神經元（neuron）連到其下一層中第 $m$ 個神經元的<strong>邊（edge）</strong>。</p>
<p>而就跟文章開頭看到的程式碼一樣，要使用 TensorFlow 定義這個神經網路也十分容易：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="c1"># 將 2 dim 轉成 1 dim</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> 
                                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_5 (Dense)              (None, 1)                 2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>扣除 bias 以後整個神經網路的確只有 2 個參數。為了讓你加深印象，讓我們將數字實際代入 $W$ 與 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
W = \begin{bmatrix} 1 &amp; 5  \end{bmatrix} \\ 
x = \begin{bmatrix} 3 \\ 3 \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>接著再看一次剛剛的運算過程：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/NumberOneByTwoMatrixAndNN.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這邊我刻意讓 $x_{1} = x_{2}$。你可以清楚地看到不同大小的 $w_{mn}$ 可以讓相同的輸入維度 $x_{n}$ 給與輸出值 $y_{m}$ 不同程度的影響力，這也是 $W$ 之所以被稱之為「權重」的原因。</p>
<p>你現在應該已經了解最基本的矩陣運算還有神經網路的概念，以及兩者之間的緊密關係了。更美妙的是，如果你了解<a href="https://youtu.be/fNk_zzaMoSs">線性代數的本質</a>，就會知道函數 $f(x) = W x + b = y$ 事實定義了兩個簡單轉換來將 2 維的輸入 $x$ 依序轉換成 1 維輸出 $y$：</p>
<ul>
<li>線性轉換：$W$</li>
<li>位移：$b$</li>
</ul>
<p>你可能也已經看過<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84">線性轉換（linear transformation, or linear map）</a>的數學定義：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="/images/manim/formal-linear-transformation-formula.jpg" style="mix-blend-mode: initial;"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是所有線性轉換都具備的<a href="https://en.wikipedia.org/wiki/Additive_map">可加性（additivity）</a>與<a href="https://zh.wikipedia.org/wiki/%E9%BD%90%E6%AC%A1%E5%87%BD%E6%95%B0">齊次性質（homogeneity）</a>。不過別擔心，在本文裡你不需了解這些定義也能直觀地理解線性轉換。用比較不嚴謹的說法，線性轉換會將一個向量空間（vector space）<strong>旋轉</strong>或<strong>伸縮</strong>（或是兩者皆做），但不會扭曲該空間。</p>
<p>要直觀瞭解這個概念，讓我們再次將幾個數字代入 $f(x)$ 裡頭：</p>
\begin{align}
f(x) &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
 &amp; = \begin{bmatrix} 1 &amp; 1  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + (-2) 
\end{align}<p>這次假設 $w_{11} = w_{12} = 1, b = -2$。另外別忘了我們前面說過的，這個 $f(x)$ 實際上也對應到一個 1-Layer NN。我們可以看看這個神經網路會怎麼轉換 2 維空間裡頭的 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/ApplyMatrixAsSingleLinearTransformation.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這個 NN 做的轉換非常地簡單，但是隱含不少前面提過的重要概念。</p>
<p>首先，以<strong>神經網路</strong>的角度而言，此 2 維向量空間裡頭的任一數據點 $ x = \left (x_{coord}, y_{coord}  \right )$ 都對應到左上 NN 第一層（輸入層）中的兩個神經元。權重 $W$ 則以<strong>邊</strong>的方式呈現，負責將第一層 2 神經元的值透過不同比重<strong>送去啟動</strong>（activate）下一層的神經元。此例則因 $w_{1} = w_{2} = 1$ 所以 $x_{1}$ 跟 $x_{2}$ 的值對 $y$ 的值的影響相同。接著 NN 只要再將 $y$ 加上純量 $b$ 即完成轉換。</p>
<p>以<strong>線性代數</strong>的視角來看，矩陣 $W$ 則定義了一個線性轉換，此轉換說明了原向量空間 $V_{original}$ 裡的 2 個基底向量 $\vec{i}$、$\vec{j}$ 在<strong>轉換後</strong>的空間 $V_{transformed}$ 裡頭的位置。我們透過 $W = [1, 1]$ 將 y 軸的 $\vec{j}$ 放到 $V_{transformed}$（跟 x 軸重疊的數線）裡跟 $\vec{i}$ 一樣的位置（$w_{12} = 1$）；而原 x 軸上的 $\vec{i}$ 則保持在原位（$w_{11} = 1$）。</p>
<p>為了讓轉換過後的 $\vec{j}$ 在對的位置，y 軸順時針旋轉了 90 度。你也能看到原 2 維空間裡頭的每個數據點 $x$ 都跟著 $\vec{j}$ 一起被轉換成 1 維數線（Number line）上的一個值，而不再是 2 維的座標。透過 $W$ 被轉換到一維空間以後，該數線上的每個數據點只要再被加上純量 $b$ 就完成兩步驟的轉換。</p>
<p>這邊的重點是，<strong>神經網路</strong>與<strong>線性代數</strong>雖然看似風馬牛不相及，實際上兩者做的是同件事情：利用 $f(x)$ 將輸入 $x$ 進行一系列幾何轉換後輸出 $y$。到此為止，你應該已能瞭解為何<a href="https://demo.leemeng.tw/#%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%EF%BC%9A%E4%B8%80%E5%80%8B%E6%98%A0%E5%B0%84%E5%87%BD%E6%95%B8">我曾說過神經網路是個映射函數的理由了</a>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="能夠解決二元分類的神經網路長得是什麼模樣？">能夠解決二元分類的神經網路長得是什麼模樣？<a class="anchor-link" href="#能夠解決二元分類的神經網路長得是什麼模樣？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們在上一節看到，一個 1-Layer 神經網路可以透過一個全連接層（線性轉換 $W$ + 位移 $b$）將 2 維輸入 $x$ 轉換到一維輸出 $y$。但你剛剛應該也已經注意到，轉換後的 $y$ 實在不是什麼好的分類結果：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="{filename}manim/ApplyMatrixAsSingleLinearTransformationAndShowAccuracy.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你的時間寶貴，因此我事先將本文的精華都濃縮在底下這個 1 分鐘的短片了。此影片展示了一個簡單神經網路解決二分類任務的<strong>完整過程</strong>。我等等會更仔細地說明影片內容，但現在請你馬上點擊播放鍵觀看吧！</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video controls="" muted="" playsinline="" poster="/the-essence-of-neural-network-better-understanding-of-nn-and-linear-algebra.html">
<source src="/images/manim/TwoLayersReLUInBetweenSolveHardTwoCurves.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>順帶一提，這是一個只有 9 個參數的神經網路，其規模跟現在媒體整天在報導的 A.I. 相比可說是滄海一粟。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        但這是我看過最美麗、直觀的神經網路運作。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我不知道看影片前的你對神經網路或是線性代數的理解程度，但我相信很多人都能在這短短的一分鐘內（重新）獲得些啟發。如果你願意，我強烈建議你至少再看一次影片並在需要時暫停咀嚼。裡頭有很多十分基本但重要的 NN 概念值得掌握。</p>
<p>你有再看一遍嗎？讓我幫你把影片裡隱含的重要概念一一列出：</p>
<ul>
<li>神經網路是將輸入（維度）做一連串簡單轉換，最後得到理想輸出（維度）的函式</li>
<li>最基本且常見的神經網路是「層」為單位，每一層的矩陣相乘事實上都在做線性轉換</li>
<li>線性轉換基本上就是對輸入空間的數據做旋轉、縮放、延伸等轉換</li>
<li>層跟層之間常會透過非線性轉換函式來提升神經網路整體的轉換能力</li>
<li>神經網路也常被視為是在做 representation learning，因為每一層的轉換都將該層輸入的數據轉換成更適合達到任務目標的形式</li>
<li>透過對輸入數據做一連串適當的</li>
<li>針對某些任務，特定的神經網路架構有其能力極限，我們透過學習從該架構中找出一組參數，讓神經網路做適合的轉換以完成任務</li>
</ul>
<p>很多東西你可能都知道了，人類是視覺動物，</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="結論">結論<a class="anchor-link" href="#結論">&para;</a></h2><ul>
<li>下一代</li>
<li>我們在 dl resource 說過神經網路是一連串的幾何轉換</li>
</ul>
</div>
</div>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="/tag/manim.html" rel="tag">Manim</a>
                        <a href="/tag/python.html" rel="tag">Python</a>
                    </span>

                </p>



                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary ">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__next">
                            <a href="/gpt2-language-model-generate-chinese-jing-yong-novels.html" rel="next">
                                <span>Next Post</span>
                                直觀理解 GPT-2 語言模型並生成金庸武俠小說
                            </a>
                        </div>
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>



<!-- footer
================================================== -->
<footer>
    <div class="row footer-bottom">
        <div class="col-twelve">
            <div class="go-top">
            <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
            </div>
        </div>
    </div> <!-- end footer-bottom -->
</footer> <!-- end footer -->


        <!-- Javascript
    ================================================== -->
    <script src="/theme/js/jquery-3.2.1.min.js"></script>
    <script src="/theme/js/plugins.js"></script>
    <script src="/theme/js/main.js"></script>
    <script type='text/javascript' src='/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script><!--https://darkmodejs.learn.uno/-->
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.4.0/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.2s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>


</body>
</html>