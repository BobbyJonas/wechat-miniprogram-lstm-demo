{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2018-11-09 21:00\n",
    "- title: \n",
    "- slug: \n",
    "- tags: 深度學習\n",
    "- description: \n",
    "- summary: \n",
    "- image: \n",
    "- image_credit_url: \n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400 minutes 總影片長度\n",
    "觀看人數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Deep Structure?\n",
    "\n",
    "現實世界中，我們常常要的要一個可以把 x 對應的 y 的 fn 。而給定一個 network structure，不管其是 shallow 或是 deep，理論上只要架構夠大，都能讓潛在包含的 function space 包含想要 fit 的 function 。那為何 deep 好？\n",
    "\n",
    "- （抽象化）我們在討論這章的時候，也不考慮實際上 shallow / deep 架構所構成的 function space 怎麼做 optimize / generalization 來實際找到該 target fn, 而是先假設只要包含了，最後就能找到一組參數 fit 出 target function\n",
    "- 可以更有效率地產生更多 piece-wise linear functions, 同樣 performance 下， shallow 需要的 neuron 數目是 deep 的指數倍\n",
    "- 有些在 deep 才能 fit 的 functions 你在 shallow 無法 fit\n",
    "\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 獵人比喻 swallow network 的最好/最差狀態\n",
    "\n",
    "結論：當你想要 fit 一個複雜的 fn, deep is exponentially better than shallow\n",
    "    - 以相同的 performance 或是相同的 neurons 數來比較\n",
    "    - 就像是相加 vs 相乘能力（以 ReLU 來看)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "就是從（deep structure 組成的） function space 裡頭找出 function star 的方法\n",
    "\n",
    "- （抽象化）optimization 不等於 learning\n",
    "    - 給定一個 loss function, optimization 專注在找到能讓 loss function 最小的一組參數（fit training data well）。而至於使用該參數預測測試資料有沒有 overfit 問題（learning）不是現在這章的關注重點\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 八門遁甲不是一個非常好的招式（deep structure 就算）\n",
    "\n",
    "- training stuck 不一定就代表走到 critical point, 梯度為 0（有可能 loss 很低沒啥改變，但是 grad 的值還是很大）\n",
    "- 且 training 過程，遭遇到的 gradient 是上上下下，而不是跟 loss 一樣直線下降\n",
    "- 如果確定是遇到 critical point, 可以使用 Hessian Matrix 確認是 local minimum / saddle point / local maximum\n",
    "- ReLU network 有 local minima, 要避免撞到這些盲點，選好 initial 的位置很重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
