{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2018-11-09 21:00\n",
    "- title: \n",
    "- slug: \n",
    "- tags: 深度學習\n",
    "- description: \n",
    "- summary: \n",
    "- image: \n",
    "- image_credit_url: \n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- why empirical important? we just can't see all the possiblities and came up with very general theories. experiement helps to understand DL.\n",
    "- 遠古時代 2000, dl 發展真的很快，很多 empirical result 又過時了\n",
    "- 大家很努力在搞清楚 DL 的 NN 到底內部在做什麼（畫 error surface etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Theory 10 影片\n",
    "- 400 minutes 總影片長度, 換幾篇 paper?\n",
    "- deep strucuture, optimization, generalization\n",
    "\n",
    "\n",
    "觀看人數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現實世界中，我們常常要的要一個可以把 x 對應的 y 的 fn 。而給定一個 network structure，不管其是 shallow 或是 deep，理論上只要架構夠大，都能讓潛在包含的 function space 包含想要 fit 的 function 。那為何 deep 好？\n",
    "\n",
    "- （抽象化）我們在討論這章的時候，也不考慮實際上 shallow / deep 架構所構成的 function space 怎麼做 optimize / generalization 來實際找到該 target fn, 而是先假設只要包含了，最後就能找到一組參數 fit 出 target function\n",
    "- 可以更有效率地產生更多 piece-wise linear functions, 同樣 performance 下， shallow 需要的 neuron 數目是 deep 的指數倍\n",
    "- 有些在 deep 才能 fit 的 functions 你在 shallow 無法 fit\n",
    "\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 獵人比喻 swallow network 的最好/最差狀態\n",
    "\n",
    "結論：當你想要 fit 一個複雜的 fn, deep is exponentially better than shallow\n",
    "    - 以相同的 performance 或是相同的 neurons 數來比較\n",
    "    - 就像是相加 vs 相乘能力（以 ReLU 來看)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是從（deep structure 組成的） function space 裡頭找出 function star 的方法\n",
    "\n",
    "- （抽象化）optimization 不等於 learning\n",
    "    - 給定一個 loss function, optimization 專注在找到能讓 loss function 最小的一組參數（fit training data well）。而至於使用該參數預測測試資料有沒有 overfit 問題（learning）不是現在這章的關注重點\n",
    "    - 反過來說，有時候找到 global minima 不是最重要的，而且反而容易 overfitting. local minimal 可能就足夠\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 八門遁甲不是一個非常好的招式（deep structure 就算）\n",
    "\n",
    "- training stuck 不一定就代表走到 critical point, 梯度為 0（有可能 loss 很低沒啥改變，但是 grad 的值還是很大）\n",
    "- 且 training 過程，遭遇到的 gradient 是上上下下，而不是跟 loss 一樣直線下降\n",
    "- 如果確定是遇到 critical point, 可以使用 Hessian Matrix 確認是 local minimum / saddle point / local maximum\n",
    "- ReLU network / non-linear netowrk 有 local minima, 要避免撞到這些盲點，選好 initial 的位置很重要；資料可能也有影響\n",
    "- 而 empirical 結果以及一些猜測顯示，我們訓練的 network structure 越大，越不容易遇到 local minima, 而且 local minima 在 loss 越低的時候越容易出現，local minima 的結果可能已經跟 global minima 一樣好。只是未來需要更多研究  \n",
    "- 長相非常不一樣的 networks 可以表現得非常像。因此不同 optimization strategy （adam, SGD）走到 solution 的方式的差異（最終流域、走的方向不同）可能比我們想像大，不止是速度的差別而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 就算你的 training stuck，也不代表你遇到 criticial point (要檢查 grad）；就算玉到 critical point, 也不代表是 local minima（要檢查 hessian matrix）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在 ML 的觀念，當兩個 model 的 training error 差不多的時候，選 capacity 小的，比較不會 overfitting（但這不一定 apply 到 DL）\n",
    "- DL 有非常高的 capacity, 但卻沒有我們想像中的那麼容易 overfitting，仿佛自帶 regularization, 還有很多謎在裡頭\n",
    "    - 例子：MNIST training error 0，但 test error 為零的時候，在一般的 ML 你都會想要減少 model capacity，但在 DL 裡頭，你再增加 model 複雜度還反而降低 test error，學得更好 ..\n",
    "- 但同時我們也看到 NN 可以硬記所有資料，就算你給他 random label，他也能在 training 時學到 100%\n",
    "- 儘管我們能透過 validation set 來選擇兩個在 training set 都表現完美的 network, 有一些額外的 indicators 的話，可以讓我們有更多選擇一個更能 generalize 的 nn 的 guideline，並在未來訓練模型的時候就塞一些東西進去（比方說 regularization term, 然後這也成為一個研究領域），來讓 NN 的 indicators 符合我們想要的樣子\n",
    "    - sensitivity 的 empirical result 讓我們看到 generalization gap 跟 jacob 是成正比的。這代表我們可以在沒有正確 label 的情況下，針對一筆 test data 算 sensitivity，來預測 network 能答對的機率。如果答對的希望渺茫，我們能交給人工判斷，在實務上是一個非常好的應用\n",
    "    - sharpness：直覺上找到的 local minima 附近的 error surface 越平滑，越能 generalize\n",
    "        - training / test set 的分佈有差，但是如果是平滑的 local minima 其跟 test set 分配的差距可能比較小\n",
    "        - batch_size 小的 train 出來的 NN 的 generalization 能力比較強（雖然在 training 的時候都能表現非常好）\n",
    "            - 有假說是，比較大的 batch_size 讓我們比較容易跑到本來不容易跑到的 shape local minima\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph & Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
