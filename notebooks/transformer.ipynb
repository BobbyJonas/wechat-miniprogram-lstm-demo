{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20190522_implement_transformer_from_scratch_for_nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwFJYt0ZQMim",
        "colab_type": "text"
      },
      "source": [
        "- author: Lee Meng\n",
        "- date: 2019-06-03 09:00\n",
        "- title: 淺談神經機器翻譯：用 Transformer 及 TensorFlow 打造巴比倫塔\n",
        "- slug: transformer\n",
        "- tags: \n",
        "- description: \n",
        "- summary: \n",
        "- image: Tour_de_babel.jpg\n",
        "- image_credit_url: \n",
        "- status: draft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feI_yqWl1zwS",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib1SSTSsSg19",
        "colab_type": "text"
      },
      "source": [
        "Components and implementation order\n",
        "- Scaled Dot-Product Attention\n",
        "- Multi-Head Attention \n",
        "- Feed Forward\n",
        "- Residual Connection & Layer Normalization\n",
        "- Encoder block\n",
        "- Decoder block\n",
        "- Encoder\n",
        "- Decoder\n",
        "- Positional Encoding\n",
        "- Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw1cfDGSYC3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "9f98b650-c6cc-473d-ea7d-99888ba03a9d"
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/df/ef509c275be9cf4a8f973da0c8ab355e74a3e17b0633b359a862c6e82fd5/tf_nightly_gpu_2.0_preview-2.0.0.dev20190522-cp36-cp36m-manylinux1_x86_64.whl (349.0MB)\n",
            "\u001b[K     |████████████████████████████████| 349.0MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Collecting wrapt>=1.11.1 (from tf-nightly-gpu-2.0-preview)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/58/7f14cd2c2f3baf06b91497118260db1db29b40ad61d106bab3efabc47372/tensorflow_estimator_2.0_preview-1.14.0.dev2019052300-py2.py3-none-any.whl (428kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Collecting google-pasta>=0.1.6 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/3a/f259a8f28b56292a0011231324230348355c3b60cd0a7940b16843a6b4d2/tb_nightly-1.14.0a20190523-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wrapt, tensorflow-estimator-2.0-preview, google-pasta, tb-nightly, tf-nightly-gpu-2.0-preview\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190523 tensorflow-estimator-2.0-preview-1.14.0.dev2019052300 tf-nightly-gpu-2.0-preview-2.0.0.dev20190522 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93bqeKlIX75F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b02b6ef1-1837-468c-e9d7-41602d549c53"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-dev20190522'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg49urS4Aas3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(9527)\n",
        "tf.random.set_seed(9527)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjYaF98NUHSr",
        "colab_type": "text"
      },
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "在實作 Multi-head 之前，先讓我們實作基本的 Attention 機制。\n",
        "\n",
        "注意力機制基本上可以想成資料庫比對。給定一個查詢 Q，我們去看該 Q 跟所有 K 的匹配程度，接著以此匹配程度對實際的 V 做加權平均，得到最後的 Repr.\n",
        "\n",
        "\n",
        " \n",
        "$$Attention(Q, K, V) = softmax({QK^T \\over \\sqrt{d_{k}}})V $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm_8tyMuVXjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_v, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPx0ioOI_TY",
        "colab_type": "text"
      },
      "source": [
        "很大的負值丟入 Softmax 函式以後會接近 0 ，則如果我們想把後三個位置遮住丟入 softmax 的話，則 mask 應該要是 `[..., 0, 1, 1, 1]` （要被遮住的位置的 mask 值為 1），再乘上 `-1e9` 以後加入 scaled_attention_logits 即可讓後三個位置經過 softmax 出來的值為 0。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CJondEVKjJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb706f5c-8ffd-4397-c95e-f0895f6ba5ba"
      },
      "source": [
        "tf.nn.softmax(tf.constant([1, -1e9, 3]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=129, shape=(3,), dtype=float32, numpy=array([0.11920291, 0.        , 0.880797  ], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7B52cigZ7vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "92e32038-f71b-43d1-b66f-f23d6b37f2d0"
      },
      "source": [
        "q = tf.constant([[0, 10, 0, 0], \n",
        "                 [0, 0, 10, 10]], dtype=tf.float32)\n",
        "q"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=135, shape=(2, 4), dtype=float32, numpy=\n",
              "array([[ 0., 10.,  0.,  0.],\n",
              "       [ 0.,  0., 10., 10.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEGjw_LaAr0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2d5d891f-8736-40c3-ab21-9f62bcced9fd"
      },
      "source": [
        "k = tf.constant([[0, 10, 0, 0], \n",
        "                 [0, 0, 10, 0], \n",
        "                 [0, 0, 10, 10]], dtype=tf.float32)\n",
        "k"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=137, shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 0., 10.,  0.,  0.],\n",
              "       [ 0.,  0., 10.,  0.],\n",
              "       [ 0.,  0., 10., 10.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkrfF_WN9bV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5fbca700-b59d-453e-d90e-db0710f6e04b"
      },
      "source": [
        "v = tf.random.uniform((3, 10))\n",
        "v"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=157, shape=(3, 10), dtype=float32, numpy=\n",
              "array([[0.5907972 , 0.01128781, 0.92228806, 0.07953656, 0.31918705,\n",
              "        0.5416858 , 0.57252204, 0.9974569 , 0.17398036, 0.5514989 ],\n",
              "       [0.32853377, 0.23834121, 0.62532985, 0.0153873 , 0.0709399 ,\n",
              "        0.13619518, 0.8167461 , 0.5599638 , 0.9179418 , 0.7110497 ],\n",
              "       [0.35725784, 0.5407543 , 0.46235597, 0.75289536, 0.6780722 ,\n",
              "        0.6773449 , 0.9228561 , 0.94404805, 0.41801345, 0.00916016]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAkpzsXHMX6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test\n",
        "matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "scaled_logits = matmul_qk / tf.math.sqrt(tf.cast(k.shape[-1], tf.float32))\n",
        "test_attn_weights = tf.nn.softmax(scaled_logits, axis=-1)\n",
        "test_attn = tf.matmul(test_attn_weights, v)\n",
        "\n",
        "# real\n",
        "attn, attn_weights = scaled_dot_product_attn(q, k, v)\n",
        "\n",
        "assert tf.reduce_sum(test_attn - attn) < 1e-9\n",
        "assert tf.reduce_sum(test_attn_weights - attn_weights) < 1e-9\n",
        "assert attn.shape == (q.shape[-2], v.shape[-1])\n",
        "assert attn_weights.shape == (q.shape[-2], k.shape[-2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmzDlPyLHpyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}