{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2018-11-09 21:00\n",
    "- title: 深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory   \n",
    "- slug: deep-learning-note-volume-1-deep-learning-theory-from-professor-hung-yi-lee\n",
    "- tags: \n",
    "- description: \n",
    "- summary: \n",
    "- image: \n",
    "- image_credit_url: \n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- why empirical important? we just can't see all the possiblities and came up with very general theories. experiement helps to understand DL.\n",
    "- 遠古時代 2000, dl 發展真的很快，很多 empirical result 又過時了\n",
    "- 大家很努力在搞清楚 DL 的 NN 到底內部在做什麼（畫 error surface etc)\n",
    "- 如果說完全不懂線性代數或是微積分也沒關係也沒有錯，但是內心可能覺得不踏實\n",
    "- 把複雜的東西講得聽起來很複雜很簡單，講得簡單才是厲害"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Theory 10 影片\n",
    "- 400 minutes 總影片長度, 換幾篇 paper?\n",
    "- +60 mins \n",
    "- 秒複習、怒算都是幫你抽象化不必要的細節（雖然功課可能有）\n",
    "- deep strucuture, optimization, generalization\n",
    "\n",
    "\n",
    "觀看人數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現實世界中，我們常常要的要一個可以把 x 對應的 y 的 fn 。而給定一個 network structure，不管其是 shallow 或是 deep，理論上只要架構夠大，都能讓潛在包含的 function space 包含想要 fit 的 function 。那為何 deep 好？\n",
    "\n",
    "- （抽象化）我們在討論這章的時候，也不考慮實際上 shallow / deep 架構所構成的 function space 怎麼做 optimize / generalization 來實際找到該 target fn, 而是先假設只要包含了，最後就能找到一組參數 fit 出 target function\n",
    "- 可以更有效率地產生更多 piece-wise linear functions, 同樣 performance 下， shallow 需要的 neuron 數目是 deep 的指數倍\n",
    "- 有些在 deep 才能 fit 的 functions 你在 shallow 無法 fit\n",
    "\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 獵人比喻 swallow network 的最好/最差狀態\n",
    "\n",
    "結論：當你想要 fit 一個複雜的 fn, deep is exponentially better than shallow\n",
    "    - 以相同的 performance 或是相同的 neurons 數來比較\n",
    "    - 就像是相加 vs 相乘能力（以 ReLU 來看)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是從（deep structure 組成的） function space 裡頭找出 function star 的方法\n",
    "\n",
    "- （抽象化）optimization 不等於 learning\n",
    "    - 給定一個 loss function, optimization 專注在找到能讓 loss function 最小的一組參數（fit training data well）。而至於使用該參數預測測試資料有沒有 overfit 問題（learning）不是現在這章的關注重點\n",
    "    - 反過來說，有時候找到 global minima 不是最重要的，而且反而容易 overfitting. local minimal 可能就足夠\n",
    "\n",
    "\n",
    "例子：\n",
    "    - 八門遁甲不是一個非常好的招式（deep structure 就算）\n",
    "\n",
    "- training stuck 不一定就代表走到 critical point, 梯度為 0（有可能 loss 很低沒啥改變，但是 grad 的值還是很大）\n",
    "- 且 training 過程，遭遇到的 gradient 是上上下下，而不是跟 loss 一樣直線下降\n",
    "- 如果確定是遇到 critical point, 可以使用 Hessian Matrix 確認是 local minimum / saddle point / local maximum\n",
    "- ReLU network / non-linear netowrk 有 local minima, 要避免撞到這些盲點，選好 initial 的位置很重要；資料可能也有影響\n",
    "- 而 empirical 結果以及一些猜測顯示，我們訓練的 network structure 越大，越不容易遇到 local minima, 而且 local minima 在 loss 越低的時候越容易出現，local minima 的結果可能已經跟 global minima 一樣好。只是未來需要更多研究  \n",
    "- 長相非常不一樣的 networks 可以表現得非常像。因此不同 optimization strategy （adam, SGD）走到 solution 的方式的差異（最終流域、走的方向不同）可能比我們想像大，不止是速度的差別而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 就算你的 training stuck，也不代表你遇到 criticial point (要檢查 grad）；就算玉到 critical point, 也不代表是 local minima（要檢查 hessian matrix）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在 ML 的觀念，當兩個 model 的 training error 差不多的時候，選 capacity 小的，比較不會 overfitting（但這不一定 apply 到 DL）\n",
    "- DL 有非常高的 capacity, 但卻沒有我們想像中的那麼容易 overfitting，仿佛自帶 regularization, 還有很多謎在裡頭\n",
    "    - 例子：MNIST training error 0，但 test error 為零的時候，在一般的 ML 你都會想要減少 model capacity，但在 DL 裡頭，你再增加 model 複雜度還反而降低 test error，學得更好 ..\n",
    "- 但同時我們也看到 NN 可以硬記所有資料，就算你給他 random label，他也能在 training 時學到 100%\n",
    "- 儘管我們能透過 validation set 來選擇兩個在 training set 都表現完美的 network, 有一些額外的 indicators 的話，可以讓我們有更多選擇一個更能 generalize 的 nn 的 guideline，並在未來訓練模型的時候就塞一些東西進去（比方說 regularization term, 然後這也成為一個研究領域），來讓 NN 的 indicators 符合我們想要的樣子\n",
    "    - sensitivity 的 empirical result 讓我們看到 generalization gap 跟 jacob 是成正比的。這代表我們可以在沒有正確 label 的情況下，針對一筆 test data 算 sensitivity，來預測 network 能答對的機率。如果答對的希望渺茫，我們能交給人工判斷，在實務上是一個非常好的應用\n",
    "    - sharpness：直覺上找到的 local minima 附近的 error surface 越平滑，越能 generalize\n",
    "        - training / test set 的分佈有差，但是如果是平滑的 local minima 其跟 test set 分配的差距可能比較小\n",
    "        - batch_size 小的 train 出來的 NN 的 generalization 能力比較強（雖然在 training 的時候都能表現非常好）\n",
    "            - 有假說是，比較大的 batch_size 讓我們比較容易跑到本來不容易跑到的 shape local minima\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- computational graph 是一種描述 function 的語言，而用 graph 的好處是，可以很簡單地利用 chain rule 來怎麼算 node 之間的微分\n",
    "- 在計算梯度時，從 root 開始用 reserve mode 算是比較有效率的。因為基本上我們在 train NN 都是要計算 loss function，而 loss function 以 graph 表示就是一個 scaler, 也就是一個 root, 剛好適合用來反向傳播\n",
    "- Jacobian Matrix 就是 vector 跟 vector 的偏微分\n",
    "- 就算是 computational graph, 事實上要算出 grad，也是要先 forward pass，然後再做一次 backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Recurrent Network 從 52:40 開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "ML course, 49 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在 slot fitting 的應用下，句子裡頭的 每個 word 就是一個 vector 包含所有的 x, 按照在句子中的順序被丟入 RNN, 而 output y vector 如果有 3 維， 就代表要預測該 word 分別是三種 slot 種類的機率 （dest, time, departure)\n",
    "\n",
    "句子中 word 順序會有影響\n",
    "同個字的 output ，依照前面的 word 所產生的 memory 會有所不同（leave / enter shop)\n",
    "\n",
    "RNN 關鍵概念：時間不同，同個 network\n",
    "\n",
    "Jordan 比 Elman 好的原因可萌是因為 Jordan 記憶是存上個時間點的輸出值 y 而不是 hidden layer ，且因為 y 是有 target 的比較好控制它的值\n",
    "\n",
    "RNN 基本概念就已經包含雙向了，同時訓練正向及逆向的 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 的 memory cell 可以想成是一個特殊的 neurons, 有三個 gates: input, output, forget gate。一個 LSTM 的 memory cell 有四個 input: input 本身以及其他三個操控 gate 開關的「信號」，最後則產生一個 output\n",
    "\n",
    "LSTM 之所以叫做 Long Short-term Memory, 就是在說，相比於傳統 RNN，其 memory cell 裡頭的值能夠保留比較久（只要不 forget, 沒有 input)\n",
    "\n",
    "人體 LSTM\n",
    "\n",
    "LSTM 要差四條電源線才能跑，參數是 4 倍\n",
    "\n",
    "基本上 LSTM 就是 standard, 別人說他用 RNN，有很大的機率就是在說他用 LSTM。而最簡單的 RNN 則通常叫做 SimpleRNN\n",
    "\n",
    "LSTM 基本上 X 會跟四組 weights 做 4 次 linear transformation 產生跟一層 LSTM neuron 數目相同的 Z, Zi, Zo, Zf vectors, 然後將各個 vectors 利用一連串的矩陣運算組合起來（可以運用平行運算， GPU 優勢）再最後產生 Y vectors. 而且每個時間點不只是餵進 X, 還會將上一次的 Y 作為 hidden layer output 以及 memory cell 裡頭的 C vectors 拿過來，跟 X 組合起來一起跟 4 組 weights 做結合\n",
    "\n",
    "而當然 LSTM 也可以 deep / multi-layer, 疊 5、 6 層的 LSTM cells\n",
    "\n",
    "GRU 是 LSTM 的一個簡化版本，只有兩個 gates, 少了 1/3 參數但聽說 performance 差不多，可以避免 overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
