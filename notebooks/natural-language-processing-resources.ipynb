{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2018-10-30 08:00\n",
    "- title: 自然語言處理 - 學習資源整理\n",
    "- slug: natural-language-processing-resources\n",
    "- tags: 自然語言處理, 機器學習\n",
    "- description: \n",
    "- summary: \n",
    "- image: alisa-anton-393305-unsplash.jpg\n",
    "- image_credit_url: https://unsplash.com/photos/rjhLxgmP8bA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 學習資源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "    * 一分鐘學ＡＩ(4) — 循環神經網路 – 陳鍾誠 – Medium\n",
    "* NLP\n",
    "    * 徐阿城\n",
    "* 知乎討論\n",
    "    * 神經機器翻譯\n",
    "    * 機器翻譯\n",
    "* Framework\n",
    "    * OpenNMT\n",
    "    * TensorFlow - seq2seq Tutorial\n",
    "* Blog\n",
    "    * Wildml.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 術語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Language Model / n-gram model 很好的回顧\n",
    "    * NLP 笔记 - Language models and smoothing\n",
    "    * Language Model / f(e)\n",
    "    * N-gram model / Markov Assumption / Chain Rule / MLE\n",
    "    * Uniform Model / Unigram Model (依據 work fq) / N-gram Model / Full History Model\n",
    "    * Smoothing: Laplace smoothing (add-one) / Good Turing smoothing, Normalization\n",
    "* SMT\n",
    "    * Statistical Machine Translation，統計機器翻譯\n",
    "        * 非限定領域機器翻譯中效能較佳的一種方法\n",
    "        * 從早期基於詞的機器翻譯已經過渡到基於短語的翻譯，並正在融合句法資訊，以進一步提高翻譯的精確性。\n",
    "    * Noise Channel \n",
    "        * 做機器翻譯 (Lexical Translation Model)\n",
    "            * word alignment / EM algorithm  / parallel corpus（平行語料庫）/ aligned corpus \n",
    "            * 利用統計方式(MLE + EM algorithm)以及平行 corpus，找出最有可能的原文與譯文的各個詞之間的對齊方式\n",
    "            * 从 lexical 层面着手，先进行词对词的翻译，然后再进行词的排序\n",
    "            * 最好的翻譯就等於 argmax ( language model p(e) * translation model p(f | e)\n",
    "            * Ref: \n",
    "                * 機器翻譯 -- Statistical Machine Translation\n",
    "        * 做拼寫糾正\n",
    "            * NLP 笔记 - Spelling, Edit Distance, and Noisy Channels\n",
    "            * 給定一個錯字 Observation / Noisy Word, 我們想知道最有可能產生此結果的原始正確字 (word)推薦給使用者。最有可能的結果是 argmaxP(W)∗P(O|W) 。理想上，我們要推薦給使用者自動校正的詞，該詞真的是使用者要的機率跟 (1) 該詞本身出現的機率 (2) 該正確詞被錯誤打成 noisy word 的機率成正比。(1) 用 LM, (2) 用 Channel Model / edit distance 估計\n",
    "* NMT\n",
    "    * Phase based machine translation (PBMT)\n",
    "    * Neural network language model(NNLM)\n",
    "    * google Neural machine translation(GNMT)\n",
    "        * Zero shot translation\n",
    "* Self-Attention Mechanism\n",
    "    * 模擬人眼在看圖片時能聚焦/專注（加大權重）在局部圖片，讓 NMT 的 decoder 在產生一個 output (word) 時，不只看前一個 hidden state, 而是能學習，在做翻譯的時候，應該要選擇關注過往的哪些 states。這樣能同時處理 aligement 跟 translation，並讓語順不同的語言也可以有不錯的翻譯（ex; 日文 & 英文）\n",
    "    * Easier to visualize what the model is doing\n",
    "    * http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n",
    "        * that vector is a sentence embedding\n",
    "* Reinforcement learning\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
