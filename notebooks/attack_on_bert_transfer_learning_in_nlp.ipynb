{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20190705_bert_nlp_transfer_learning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMJ-Cpt7-47",
        "colab_type": "text"
      },
      "source": [
        "- author: Lee Meng\n",
        "- date: 2019-07-08 09:00\n",
        "- title: 進擊的 BERT：運用自然語言處理的巨人之力\n",
        "- slug: attack_on_bert_transfer_learning_in_nlp\n",
        "- tags: 自然語言處理, NLP, Pytorch\n",
        "- description: 介紹目前自然語言處理領域中非常熱門的語言代表模型 BERT 以及遷移學習的運作方式。本文將透過一個假新聞分類問題，以 Pytorch 向讀者展示如何將強大的語言代表模型運用到自己有興趣的 NLP 任務之上，從而飛得更快更遠。\n",
        "- summary: 介紹目前自然語言處理領域中非常熱門的語言代表模型 BERT 以及遷移學習的運作方式。本文將透過一個假新聞分類問題，以 Pytorch 向讀者展示如何將強大的語言代表模型運用到自己有興趣的 NLP 任務之上，從而飛得更快更遠。\n",
        "- image: attack_on_bert.jpg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E86Eg3ihGJ9T",
        "colab_type": "text"
      },
      "source": [
        "!quote\n",
        "- 這篇文章帶你了解並實際運用現在 NLP 領域的巨人之力：BERT 模型。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7k6mEs9Z-2w",
        "colab_type": "text"
      },
      "source": [
        "如果你還有印象，在[自然語言處理與深度學習入門指南](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)裡我使用了 LSTM 以及 Google 的語言代表模型 [BERT](https://github.com/google-research/bert) 來分類中文假新聞。說來有趣，因為 BERT 本身的強大，我不費吹灰之力就在[該 Kaggle 競賽](https://www.kaggle.com/c/fake-news-pair-classification-challenge/leaderboard)達到 85 % 的正確率，距離第一名 3 %，總排名前 30 %。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZ-l9AiIgiU",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- nlp-kaggle-intro/kaggle-final-result.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGU_WrGSKqUE",
        "colab_type": "text"
      },
      "source": [
        "雖說如此，使用 BERT 一直不是那麼直觀的事情。最近適逢 [PyTorch Hub](https://pytorch.org/hub) 上架 [BERT](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)，李宏毅教授的[機器學習課程](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html)也推出了 [BERT 的教學影片](https://github.com/openai/gpt-2)，我認為現在正是了解並**實際運用** BERT 的最佳時機！\n",
        "\n",
        "\n",
        "閱讀完這篇文章，你也能跟我一樣運用強大的 BERT，而且是以最直覺、簡單的方式。你會了解目前 NLP 領域非常熱門的[遷移學習（Transfer Learning）](https://docs.google.com/presentation/d/1DJI1yX4U5IgApGwavt0AmOCLWwso7ou1Un93sMuAWmA/edit?usp=sharing)技術，並實際運用這股巨人之力來解決你自己有興趣的自然語言任務。我在文中也會提供一些有趣的研究及應用 ，讓你之後可以進一步探索這個變化快速的 NLP 世界。\n",
        "\n",
        "我們等等會簡單回顧 BERT 裡的一些重要概念，但如果你完全不熟 NLP 或是壓根子沒聽過什麼是 BERT，我強力建議你之後找時間（或是現在！）觀看李宏毅教授說明 [ELMo](https://allennlp.org/elmo)、BERT 以及 [GPT](https://github.com/openai/gpt-2) 等模型的影片，淺顯易懂："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmX_SVv-PkTb",
        "colab_type": "text"
      },
      "source": [
        "!youtube\n",
        "- UYPa347-DdE\n",
        "- 李宏毅教授講解目前 NLP 領域的最新研究是如何讓機器讀懂文字的（我超愛這截圖）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0RlmXdQ4_w",
        "colab_type": "text"
      },
      "source": [
        "## BERT：理解上下文的語言代表模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygHH79JbQ40g",
        "colab_type": "text"
      },
      "source": [
        "一個簡單 convention，等等文中會穿插使用的：\n",
        "- 代表\n",
        "- representation\n",
        "- repr.\n",
        "- repr. 向量\n",
        "\n",
        "指的都是一個可以用來**代表**某詞彙（在某個語境下）的多維連續向量（continuous vector）。 \n",
        "\n",
        "現在在 NLP 圈混的，應該沒有人會說自己不曉得 Transformer 的[經典論文 Attention Is All You Need](https://arxiv.org/abs/1706.03762) 以及其知名的[自注意力機制（Self-attention mechanism）](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Encoder-Decoder-%E6%A8%A1%E5%9E%8B-+-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6)。[BERT](https://arxiv.org/abs/1810.04805) 全名為 **B**idirectional **E**ncoder **R**epresentations from **T**ransformers，是 Google 以無監督的方式利用大量無標註文本「煉成」的**語言模型**，其架構為 Transformer 中的 Encoder。\n",
        "\n",
        "我在[淺談神經機器翻譯 & 用 Transformer 英翻中](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html)一文已經鉅細靡遺地解說過所有 Transformer 的相關概念，這邊就不再贅述。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvAzW7waQ4rS",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/bert-intro.jpg\n",
        "- BERT 其實就是 Transformer 中的 Encoder，只是很多層\n",
        "- https://youtu.be/UYPa347-DdE?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L2sky1zQ4gR",
        "colab_type": "text"
      },
      "source": [
        "BERT 是一種語言模型，而[語言模型（**L**anguage **M**odel, LM）](https://youtu.be/iWea12EAu6U)做的事情就是在給定一些詞彙的前提下， 去估計下一個詞彙出現的機率分佈。在[讓 AI 寫點金庸](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)裡的 LSTM 也是一個語言模型 ，只是跟 BERT 差了很多個數量級。\n",
        "\n",
        "為何會想要訓練一個語言模型（LM）？因為：\n",
        "- 不像 [ImageNet](http://www.image-net.org/) 還要找人標注數據，要訓練 LM 的話網路上的所有文本都是你的潛在資料集，數據無限大（BERT 預訓練使用的數據集共有 33 **億**個字，其中包含維基百科及 [BooksCorpus](https://arxiv.org/abs/1506.06724)）\n",
        "- 厲害的 LM 能夠學會語法結構、解讀語義甚至[指代消解](http://ckip.iis.sinica.edu.tw/project/coreference/)。透過特徵擷取或是 fine-tuning 能更有效率地訓練下游任務並提升其表現\n",
        "\n",
        "這就是近來 NLP 領域非常流行的兩階段做法：\n",
        "- 先以無監督方式預訓練一個巨大的 LM\n",
        "- 再將該 LM 拿來做特徵擷取或是 fine-tuning 下游有標註數據的任務\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVdDmz_-tqN1",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/lm-equation.jpg\n",
        "- 給定前 t 個在字典裡的詞彙，語言模型要去估計第 t + 1 個詞彙的機率分佈 P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQcss0atogR",
        "colab_type": "text"
      },
      "source": [
        "當然天下沒有白吃的午餐。\n",
        "\n",
        "要訓練好一個有 1.1 億參數的 12 層 **BERT-BASE** 得用 16 個 [TPU chips](https://cloudplatform.googleblog.com/2018/06/Cloud-TPU-now-offers-preemptible-pricing-and-global-availability.html) 跑上整整 4 天，[花費 500 鎂](https://medium.com/syncedreview/the-staggering-cost-of-training-sota-ai-models-e329e80fa82)；24 層的 **BERT-LARGE** 則有 3.4 億個參數，得用 64 個 TPU chips（約 7000 鎂）訓練。喔對了，別忘了多次實驗得把這些成本乘上幾倍。\n",
        "\n",
        "值得慶幸的是作者們有釋出訓練好的模型。因此只要使用 [TensorFlow](https://github.com/google-research/bert) 或是 [PyTorch](https://github.com/huggingface/pytorch-pretrained-BERT) 將已訓練好的 BERT 載入，就能省去預訓練的所有成本，並馬上使用強大的 BERT。\n",
        "\n",
        "讓我們簡單瞭解一下 BERT 是怎麼訓練出來的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FbhfQWxtsmS",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/bert-pretrain-tasks.jpg\n",
        "- BERT 在預訓練時需要完成的兩個任務\n",
        "- https://youtu.be/UYPa347-DdE?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4k3lp3A5W0A",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Google 在預訓練 BERT 時讓它**同時**進行兩個任務：\n",
        "- 克漏字填空（[1953 年被提出的 Cloze task](https://journals.sagepub.com/doi/abs/10.1177/107769905303000401)）\n",
        "- 判斷第 2 個句子本來是否跟第 1 個句子相接（**N**ext **S**entence **P**rediction, NSP）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHt3eEFthGLf",
        "colab_type": "text"
      },
      "source": [
        "對上通天文下知地理的鄉民們來說，要完成這兩個任務簡單到爆。只要稍微看一下**前後文**就能知道左邊克漏字任務的 `[MASK]` 裡頭該填 `退了`；而 `醒醒吧` 後面接 `你沒有妹妹` 也十分合情合理。\n",
        "\n",
        "多說無益，這是一篇 BERT 實用文章，因此讓我們馬上載入 [PyTorch Hub](https://pytorch.org/hub) 上的 [BERT 模型](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)實際體驗看看。首先我們需要安裝一些簡單的函式庫：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJGMx8bQQ6hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip install tqdm boto3 requests regex -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38RUp4znLP-",
        "colab_type": "text"
      },
      "source": [
        "我們接著把中文 BERT 使用的 tokenizer 載入："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpYfbnqUnLGT",
        "colab_type": "code",
        "outputId": "837bc805-5630-4ebe-8d49-c431e62ac5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "GITHUB_REPO = \"huggingface/pytorch-pretrained-BERT\" # 感謝 HuggingFace 團隊造福後人\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
        "\n",
        "# 取得此預訓練模型所使用的 tokenizer\n",
        "tokenizer = torch.hub.load(GITHUB_REPO, 'bertTokenizer', PRETRAINED_MODEL_NAME)\n",
        "clear_output()\n",
        "\n",
        "# 顯示字典資訊\n",
        "vocab = tokenizer.vocab\n",
        "print(\"字典大小：\", len(vocab))\n",
        "# tokenizer.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "字典大小： 21128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXkvofar85G_",
        "colab_type": "text"
      },
      "source": [
        "沒記錯的話，英文 BERT 的字典大小大約是 3 萬左右。我們可以瞧瞧中文 BERT 字典裡頭紀錄的一些 tokens 以及對應的索引："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atephFc444px",
        "colab_type": "code",
        "outputId": "418ae99c-b560-47bf-b3ed-7020c1d6dc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import random\n",
        "random_tokens = random.sample(list(vocab), 10)\n",
        "random_ids = [vocab[t] for t in random_tokens]\n",
        "\n",
        "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
        "print(\"-\" * 25)\n",
        "for t, id in zip(random_tokens, random_ids):\n",
        "    print(\"{0:15}{1:10}\".format(t, id))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token               index          \n",
            "-------------------------\n",
            "抒                    2830\n",
            "##ein               11858\n",
            "##薯                 19018\n",
            "result              13170\n",
            "149                  9491\n",
            "evolution           12691\n",
            "mwc                 12184\n",
            "##刃                 14202\n",
            "kelly               11394\n",
            "##换                 15997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQyjTKCUxfqF",
        "colab_type": "text"
      },
      "source": [
        "BERT 使用當初 [Google NMT](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html) 提出的 [WordPiece Tokenization](https://arxiv.org/abs/1609.08144) ，將本來的 words 拆成更小粒度的 wordpieces，有效改善 [OOV](https://en.wiktionary.org/wiki/OOV) 問題。中文的話大致上就像是 character-level tokenization，而有 `##` 前綴的 tokens 即為 wordpieces。\n",
        "\n",
        "ㄅㄆㄇㄈ當然也是有被收錄的："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOBbWsuL90k4",
        "colab_type": "code",
        "outputId": "0a09d070-0c11-48a6-a530-0df104fe8e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "indices = list(range(647, 657))\n",
        "some_pairs = [(t, idx) for t, idx in vocab.items() if idx in indices]\n",
        "for pair in some_pairs:\n",
        "    print(pair)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ㄅ', 647)\n",
            "('ㄆ', 648)\n",
            "('ㄇ', 649)\n",
            "('ㄉ', 650)\n",
            "('ㄋ', 651)\n",
            "('ㄌ', 652)\n",
            "('ㄍ', 653)\n",
            "('ㄎ', 654)\n",
            "('ㄏ', 655)\n",
            "('ㄒ', 656)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCnLyMHO9zWj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "另外你也可以在 [Hugging Face 團隊的 repo ](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/hubconfs/bert_hubconf.py) 看到所有可供使用的 BERT 預訓練模型。截至目前為止有以下模型可供使用：\n",
        "\n",
        "- bert-base-chinese\n",
        "- bert-base-uncased\n",
        "- bert-base-cased\n",
        "- bert-base-german-cased\n",
        "- bert-base-multilingual-uncased\n",
        "- bert-base-multilingual-cased\n",
        "- bert-large-cased\n",
        "- bert-large-uncased\n",
        "- bert-large-uncased-whole-word-masking\n",
        "- bert-large-cased-whole-word-masking\n",
        "\n",
        "這些模型的主要差別在於預訓練時用的文本語言以及層數有所不同。常被拿來應用與研究的是英文的 `bert-base-cased`，但為了方便了解 BERT 運作，本文使用包含繁體與簡體中文的預訓練模型。 \n",
        "\n",
        "讓我們實際拿個中文句子來做斷詞看看："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vZZnu7V2Cye",
        "colab_type": "code",
        "outputId": "d93d4d4f-571e-4bcf-da1b-6c856d736d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# 利用中文 BERT 的 tokenizer 將中文句子做 tokenization\n",
        "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(text)\n",
        "print(tokens[:10], '...')\n",
        "print(ids[:10], '...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\n",
            "['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
            "[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IbLtDmI2CaV",
        "colab_type": "text"
      },
      "source": [
        "除了一般的 wordpieces 以外，BERT 裡頭總共有 5 個特殊 tokens 各司其職：\n",
        "- `[CLS]`：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.\n",
        "- `[SEP]`：兩個句子會合併成一個序列，中間插入這個 token 以做區隔\n",
        "- `[UNK]`：沒出現在字典裡頭的字會被這個 token 取代\n",
        "- `[PAD]`：padding 遮罩，將長度不一的序列補齊方便做 batch 運算\n",
        "- `[MASK]`：未知遮罩，僅在預訓練階段會用到\n",
        "\n",
        "padding 遮罩在之前的 [Transformer](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3%E9%81%AE%E7%BD%A9%E5%9C%A8%E6%B3%A8%E6%84%8F%E5%87%BD%E5%BC%8F%E4%B8%AD%E7%9A%84%E6%95%88%E6%9E%9C) 文章有詳細介紹，而 `[MASK]` token 一般在 fine-tuning 或是 feature extraction 時不會用到，這邊只是為了展示預訓練階段的克漏字任務才使用的。\n",
        "\n",
        "現在馬上讓我們看看給定上面有 `[MASK]` 的句子，BERT 會填入什麼字："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgT-9InkQ42X",
        "colab_type": "code",
        "outputId": "bc833376-a484-4ed6-b19c-52b538dab44e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "\"\"\"這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測\"\"\"\n",
        "\n",
        "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
        "tokens_tensor = torch.tensor([ids])  # (1, seq_len)\n",
        "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
        "maskedLM_model = torch.hub.load(GITHUB_REPO, \n",
        "                                'bertForMaskedLM', \n",
        "                                PRETRAINED_MODEL_NAME)\n",
        "clear_output()\n",
        "\n",
        "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
        "maskedLM_model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = maskedLM_model(tokens_tensor, segments_tensors)\n",
        "    # (1, seq_len, num_hidden_units)\n",
        "\n",
        "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
        "masked_index = 5\n",
        "k = 3\n",
        "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
        "\n",
        "print(\"輸入句子    ：\", tokens[:10], '...')\n",
        "print('-' * 50)\n",
        "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
        "    tokens[masked_index] = t\n",
        "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:10]), '...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "輸入句子    ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
            "--------------------------------------------------\n",
            "Top 1 (67%)：['[CLS]', '等', '到', '潮', '水', '來', '了', '，', '就', '知'] ...\n",
            "Top 2 (25%)：['[CLS]', '等', '到', '潮', '水', '濕', '了', '，', '就', '知'] ...\n",
            "Top 3 ( 2%)：['[CLS]', '等', '到', '潮', '水', '過', '了', '，', '就', '知'] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5aADcJaVnQv",
        "colab_type": "text"
      },
      "source": [
        "Google 在訓練中文 BERT 鐵定沒看[批踢踢](https://term.ptt.cc/)，還無法抓到鄉民們想要的那個 `退` 字。不過以語言模型的角度來看預測結果已經挺好的了。BERT 利用關注 `潮水` 這兩個字，從 2 萬多個 wordpieces 的字典裡頭估計該 `[MASK]` token 為 `來`，也還說的過去。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssTSDmH_lv1x",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/bert-attention.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNFP_3mxQ4Q8",
        "colab_type": "text"
      },
      "source": [
        "這是 [BertViz](https://github.com/jessevig/bertviz) 視覺化 BERT 注意力的結果，我等等會列出安裝步驟讓你自己玩玩。值得一提的是，這是第 8 層的 [Multi-head attention](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Multi-head-attention%EF%BC%9A%E4%BD%A0%E7%9C%8B%E4%BD%A0%E7%9A%84%EF%BC%8C%E6%88%91%E7%9C%8B%E6%88%91%E7%9A%84) 裡頭某一個 head 的注意力結果。 \n",
        "\n",
        "學會做克漏字讓 BERT 可以更好去 model 在不同語境下每個詞彙該有的 repr.，而 NSP 任務則能幫助 BERT model 兩個句子之間的關係，這在[問答系統 QA](https://zh.wikipedia.org/wiki/%E5%95%8F%E7%AD%94%E7%B3%BB%E7%B5%B1)、[自然語言推論 NLI ](http://nlpprogress.com/english/natural_language_inference.html)或是任何包含兩個句子的分類任務都很有幫助。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmPLUCASdYqv",
        "colab_type": "text"
      },
      "source": [
        "這樣的 repr. 就是近年超級熱門的 [contextual word representation](https://youtu.be/S-CspeZ8FHc) 概念，跟以往沒有蘊含上下文資訊的 [Word2Vec、GloVe](https://youtu.be/8rXD5-xhemo) 等無語境的詞嵌入向量有很大的差異。用學術一點的說法就是：\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvsWgetBx-EA",
        "colab_type": "text"
      },
      "source": [
        "!quote\n",
        "- Contextual word repr. 讓同 word type 的 word token 在不同語境下有不同的表示方式；而傳統的 word repr. 無論上下文，都會讓同 word type 的 word token 的 repr. 相同。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfruc2Box8Q5",
        "colab_type": "text"
      },
      "source": [
        "直覺上 contextual word representation 比較能反映人類語言的真實情況，畢竟同個詞彙在不同情境下的意涵相異是再正常不過的事情了。讓我再舉個具體例子：\n",
        "\n",
        "\n",
        "```text\n",
        "情境 1：\n",
        "\n",
        "胖虎叫大雄去買漫畫，回來慢了就打他。\n",
        "\n",
        "情境 2：\n",
        "\n",
        "妹妹說胖虎是「胖子」，他聽了很不開心。\n",
        "\n",
        "```\n",
        "\n",
        "很明顯地，在這兩個情境裡頭「他」所代表的語義不同。如果仍使用沒蘊含上下文資訊的詞向量，機器就會很難正確「解讀」這兩個句子所蘊含的語義了。\n",
        "\n",
        "現在讓我們跟隨[這個 Colab 筆記本](https://colab.research.google.com/drive/1g2nhY9vZG-PLC3w3dcHGqwsHBAXnD9EY)來使用剛剛看到的 [BertViz](https://github.com/jessevig/bertviz)，看看 BERT 會怎麼處理這兩個情境：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8sWIJBqfKX2",
        "colab_type": "code",
        "outputId": "0143d67d-4c8e-49da-d425-2d6f54c736ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 安裝 BertViz\n",
        "import sys\n",
        "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "if not 'bertviz_repo' in sys.path:\n",
        "  sys.path += ['bertviz_repo']\n",
        "\n",
        "# import packages\n",
        "from bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "from bertviz.head_view_bert import show\n",
        "\n",
        "# 在 jupyter notebook 裡頭顯示 visualzation 的 helper\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp1AKts0Fuy9",
        "colab_type": "text"
      },
      "source": [
        "Setup 以後就能非常輕鬆地將 BERT 內部的注意力機制視覺化出來：\n",
        "\n",
        "```python\n",
        "# 記得我們是使用中文 BERT\n",
        "bert_version = 'bert-base-chinese'\n",
        "model = BertModel.from_pretrained(bert_version)\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
        "\n",
        "# 情境 1 的句子\n",
        "sentence_a = \"胖虎叫大雄去買漫畫，\"\n",
        "sentence_b = \"回來慢了就打他。\"\n",
        "call_html()\n",
        "show(model, tokenizer, sentence_a, sentence_b)\n",
        "\n",
        "# 注意：執行這段程式碼以後只會顯示下圖左側的結果。\n",
        "# 為了方便你比較，我把情境 2 的結果也同時附上\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_0kQxd9hgzN",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/bert-coreference.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky0jvZ_GfK10",
        "colab_type": "text"
      },
      "source": [
        "這是 BERT 裡第 9 層 Encoder block 的其中一個 head 的注意力結果。\n",
        "\n",
        "圖中的線條代表該 head 在更新「他」（左側）的 repr. 時關注其他詞彙（右側）的注意力程度。越粗代表關注權重（attention weights）越高。很明顯地這個 head 具有一定的[指代消解（Coreference Resolution）](https://youtu.be/i19m4GzBhfc)能力，能正確地找出「他」所指代的對象。\n",
        "\n",
        "指代消解可不是一項簡單任務，但 BERT 透過自注意力機制、深度雙向語言模型以及大量的訓練文本達到這樣的水準，是一件令人雀躍的事情。\n",
        "\n",
        "\n",
        "當然 BERT 並不是第一個嘗試產生 contextual word repr. 的語言模型。在它之前最知名的例子有剛剛提到的 [ELMo](https://allennlp.org/elmo) 以及 [GPT](https://github.com/openai/gpt-2)：\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVOEt4O62W5O",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/bert_elmo_gpt.jpg\n",
        "- ELMo、GPT 以及 BERT 都透過訓練語言模型來獲得 contextual word representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1iJ_or62tt4",
        "colab_type": "text"
      },
      "source": [
        "ELMo 利用兩層雙向 LSTM  做語言模型並將中間得到的隱狀態向量串接當作每個詞彙的 contextual word representation；GPT 則是使用 Transformer 的 Decoder 來訓練一個中規中矩，從左到右的語言模型。\n",
        "\n",
        "BERT 跟它們的差異在於利用 **M**asked **L**anguage **M**odel（MLM，即克漏字的文雅說法）的概念訓練一個**雙向**的語言模型，使得其輸出的每個 token 的 repr. 都同時蘊含了前後文資訊。\n",
        "\n",
        "跟以往模型相比，BERT 能更好地處理自然語言，在著名的問答任務 [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) 也有卓越表現："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9VSgz5M2s7G",
        "colab_type": "text"
      },
      "source": [
        "!image\n",
        "- bert/squad2.jpg\n",
        "- SQuAD 2.0 目前排行榜的前 5 名有 4 個有使用 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVvMUwlw2swe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MMnM-mAgGZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw66m2AQ1NHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzkGiFjngGe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCNuexEiVTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Yllx4Bcfmk",
        "colab_type": "text"
      },
      "source": [
        "### Load chinese BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we8YzV4KvTwM",
        "colab_type": "text"
      },
      "source": [
        "- 李宏毅：BERT 和 GPT-2 都是使用 word piece (例如: fragment 可以拆成 frag + ment 兩個 pieces ，一個 word 也可以獨自形成一個 word piece) ，word piece 可以由蒐集大量的資料找出常出現的 pattern 取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KYogNKl9cbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83MWq5t-0wlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tqdm boto3 requests regex -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_UN7-lcciGf",
        "colab_type": "code",
        "outputId": "e082799f-3406-45d9-e88b-2aa03c7d1d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "github_repo = \"huggingface/pytorch-pretrained-BERT\"\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
        "\n",
        "# bert_tokenizer = torch.hub.load(github_repo, 'bertTokenizer', PRETRAINED_MODEL_NAME, do_basic_tokenize=False)\n",
        "bert_tokenizer = torch.hub.load(github_repo, 'bertTokenizer', PRETRAINED_MODEL_NAME)\n",
        "# TODO: True / False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX2re7z5dv4M",
        "colab_type": "code",
        "outputId": "a8c0c052-0868-4c7c-d144-0c2eb3b6b8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text = \"[CLS] 東京好熱， [SEP] 台灣也是嗎？ [SEP] [PAD]\"\n",
        "tokens = bert_tokenizer.tokenize(text)\n",
        "ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '東', '京', '好', '熱', '，', '[SEP]', '台', '灣', '也', '是', '嗎', '？', '[SEP]', '[PAD]']\n",
            "[101, 3346, 776, 1962, 4229, 8024, 102, 1378, 4124, 738, 3221, 1621, 8043, 102, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLXGuIE7eYI6",
        "colab_type": "text"
      },
      "source": [
        "### Sentence finetune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqMHein0ehWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "tokens_tensors = torch.tensor([ids])\n",
        "masks_tensors = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TZMttGhhKJl",
        "colab_type": "code",
        "outputId": "bdcb00c7-25bc-4908-a67a-479552211ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(tokens_tensors.shape)\n",
        "print(segments_tensors.shape)\n",
        "print(masks_tensors.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 15])\n",
            "torch.Size([1, 15])\n",
            "torch.Size([1, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB_JGMfBebsI",
        "colab_type": "code",
        "outputId": "f513ce93-e9e1-491a-bfe6-aac19912227f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load bertForSequenceClassification\n",
        "finetune_task = \"bertForSequenceClassification\"\n",
        "num_labels = 3\n",
        "\n",
        "classifier = torch.hub.load(github_repo, finetune_task, PRETRAINED_MODEL_NAME, num_labels=num_labels)\n",
        "classifier.eval()\n",
        "\n",
        "# Predict the sequence classification logits\n",
        "with torch.no_grad():\n",
        "    logits = classifier(input_ids=tokens_tensors, \n",
        "                        token_type_ids=segments_tensors, \n",
        "                        attention_mask=masks_tensors)\n",
        "\n",
        "clear_output()\n",
        "print(logits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5932, -0.7918,  0.1374]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuuVYasF4mKI",
        "colab_type": "text"
      },
      "source": [
        "### Compare custom loss to predefined loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOGEyRjA5-sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3kgJuB46FFH",
        "colab_type": "code",
        "outputId": "feed4377-9f5d-4144-ee3a-5dd4e6c6c265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels = torch.tensor([1])\n",
        "tmp_loss = tmp_criterion(logits, labels)\n",
        "tmp_loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5586)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JplQ4YM_4rLI",
        "colab_type": "code",
        "outputId": "9fed29c0-bfbd-4508-ab58-2a050234d70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss = classifier(tokens_tensors, segments_tensors, attention_mask=masks_tensors, labels=labels)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5586, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKmp58gYV51S",
        "colab_type": "code",
        "outputId": "44509b58-39fb-49da-be31-07b4ceab2a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train on GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "classifier.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmYDhcBChe-c",
        "colab_type": "text"
      },
      "source": [
        "### Attentionmap & BertViz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0yck8OaGmW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMwl8kcGDg3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_tokenizer.save_vocabulary(\"vocab.txt\")\n",
        "# classifier.bert.config.to_json_file(\"bert_config.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXJ8meBPDg_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNCrKDKXDg2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmAaBdVWutUC",
        "colab_type": "text"
      },
      "source": [
        "### Download raw data and sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZFhsaFm8Ti8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 看能不能使用 torch utils 直接下載 kaggle dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QtucuYuVqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ignore\n",
        "zip_file = \"drive-download-20190516T113709Z-001.zip\"\n",
        "file_url = \"https://s3-ap-northeast-1.amazonaws.com/smartnews-dmp-tmp/meng/fake_news/\" + zip_file\n",
        "\n",
        "!wget {file_url}\n",
        "!unzip {zip_file}\n",
        "!mv train_bert.tsv train_orig.tsv\n",
        "!mv dev_bert.tsv dev_orig.tsv\n",
        "!mv test_bert.tsv test.tsv\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_z9gFWVuViO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hP0N0EuuJ8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ignore\n",
        "full_train = True\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "df_train = pd.read_csv(\"train_orig.tsv\", sep=\"\\t\")\n",
        "df_dev = pd.read_csv(\"dev_orig.tsv\", sep=\"\\t\")\n",
        "\n",
        "if not full_train:\n",
        "    df_train = df_train.sample(frac=0.1, random_state=9537)\n",
        "\n",
        "df_train = df_train[~(df_train.text_a.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
        "df_train = df_train[~(df_train.text_b.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
        "    \n",
        "    \n",
        "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)    \n",
        "df_dev.sample(frac=0.1, random_state=9537).to_csv(\"dev.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_YIb8HcLNhf",
        "colab_type": "code",
        "outputId": "dc49d6ca-8785-460d-c7e8-e67303018c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df_train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
        "df_train.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018年社保改革新政策：这四大情况可提前支取养老金</td>\n",
              "      <td>2018年社保改革新政策，这三大养老金调整方式须知晓！</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>注射胰岛素会上瘾？5种情况必须使用胰岛素治疗！</td>\n",
              "      <td>谣言粉碎机｜吃粗粮能降糖？胰岛素注射液必须放冰箱？</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>三高了？不如试试用凉水泡它喝，坚持一个月，效果惊到你！</td>\n",
              "      <td>凉水喝了拉肚子还致癌？关于“凉水”你该知道的3个真相</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>罕见的水果吃过5种以上是土豪，吃过10种以上就是贵族！</td>\n",
              "      <td>这些水果吃过3种是土豪，吃过5种是贵族，你吃过几种呢？</td>\n",
              "      <td>agreed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>最管用的“天然胰岛素”，糖尿病的救星终于出现！血糖猛降不是梦</td>\n",
              "      <td>“天然胰岛素”，糖尿病“死对头”，每天吃两口，血糖不升反降！</td>\n",
              "      <td>agreed</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           text_a                          text_b      label\n",
              "0      2018年社保改革新政策：这四大情况可提前支取养老金     2018年社保改革新政策，这三大养老金调整方式须知晓！  unrelated\n",
              "1         注射胰岛素会上瘾？5种情况必须使用胰岛素治疗！       谣言粉碎机｜吃粗粮能降糖？胰岛素注射液必须放冰箱？  unrelated\n",
              "2     三高了？不如试试用凉水泡它喝，坚持一个月，效果惊到你！      凉水喝了拉肚子还致癌？关于“凉水”你该知道的3个真相  unrelated\n",
              "3     罕见的水果吃过5种以上是土豪，吃过10种以上就是贵族！     这些水果吃过3种是土豪，吃过5种是贵族，你吃过几种呢？     agreed\n",
              "4  最管用的“天然胰岛素”，糖尿病的救星终于出现！血糖猛降不是梦  “天然胰岛素”，糖尿病“死对头”，每天吃两口，血糖不升反降！     agreed"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHj6wYJSIwy3",
        "colab_type": "code",
        "outputId": "9f53eb58-6464-4c91-e372-f86ca800dad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23871, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-jF0gI7Ai8L",
        "colab_type": "text"
      },
      "source": [
        "### Load tsv as pytorch dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpFWdlCGGlrw",
        "colab_type": "text"
      },
      "source": [
        "- input: (text_a, text_b, label)\n",
        "- output: \n",
        "    - tokens_tensor: `[CLS] T1 T2 [SEP] T3 T4 [SEP] [PAD]`\n",
        "    - segments_tensors\n",
        "    - mask_tensors\n",
        "    \n",
        "```python\n",
        "input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1)\n",
        "input_type_ids = torch.zeros(x.shape, device=x.device, dtype=torch.long)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kRsPBbP-jnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pysnooper -q\n",
        "import pysnooper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6TvvF0rIz_2",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "# The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O2cY0IYuvpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        " \n",
        "    \n",
        "class FakeNewsDataset(Dataset):\n",
        "    # 讀取原始 tsv 檔並 setup 一些參數\n",
        "    def __init__(self, mode, tokenizer):\n",
        "        assert mode in [\"train\", \"dev\", \"test\"]\n",
        "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\")\n",
        "        self.len = len(self.df)\n",
        "        self.labels = sorted(self.df.label.unique())\n",
        "        self.label_map = {label: i for i, label in enumerate(self.labels)}\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    # 回傳一筆訓練 / 驗證 / 測試數據\n",
        "#     @pysnooper.snoop()\n",
        "    def __getitem__(self, idx):\n",
        "        text_a, text_b, label = self.df.iloc[idx, :].values\n",
        "        \n",
        "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
        "        word_pieces = [\"[CLS]\"]\n",
        "        tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        word_pieces += tokens_a + [\"[SEP]\"]\n",
        "        len_a = len(word_pieces)\n",
        "        \n",
        "        # 第二個句子的 BERT tokens\n",
        "        tokens_b = self.tokenizer.tokenize(text_b)\n",
        "        word_pieces += tokens_b + [\"[SEP]\"]\n",
        "        len_b = len(word_pieces) - len_a\n",
        "        \n",
        "        # 將整個 token 序列轉換成索引序列\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "        \n",
        "        # 將 label 也轉換成索引\n",
        "        label_id = self.label_map[label]\n",
        "        \n",
        "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
        "                                        dtype=torch.long)\n",
        "        \n",
        "        return (tokens_tensor, segments_tensor, torch.tensor(label_id))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "    label_ids = torch.stack([s[2] for s in samples])\n",
        "    \n",
        "    tokens_tensors = pad_sequence(tokens_tensors, \n",
        "                                  batch_first=True)\n",
        "    \n",
        "    segments_tensors = pad_sequence(segments_tensors, \n",
        "                                    batch_first=True)\n",
        "    \n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
        "                                dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(\n",
        "        tokens_tensors != 0, 1)\n",
        "    \n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZwaqjGuSnL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 1\n",
        "v = df_train.loc[idx, ['text_a', 'text_b']].values\n",
        "text_a = v[0]\n",
        "text_b = v[1]\n",
        "input_words = [\"[CLS]\"] + bert_tokenizer.tokenize(text_a) + [\"[SEP]\"] + bert_tokenizer.tokenize(text_b) + ['[SEP]']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz7Zlui1RC4H",
        "colab_type": "code",
        "outputId": "7a302f0c-28ea-4110-ce81-f408511de431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train = FakeNewsDataset(\"train\", tokenizer=bert_tokenizer)\n",
        "trainloader = DataLoader(train, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
        "\n",
        "c = 0\n",
        "for i, data in enumerate(trainloader):\n",
        "    \n",
        "    tokens_tensors, segments_tensors, masks_tensors, label_ids = \\\n",
        "        data\n",
        "    \n",
        "    print(f\"batch {i}:\")\n",
        "    print(\"tokens_tensors:\", tokens_tensors.shape)\n",
        "    print(\"segments_tensors:\", segments_tensors.shape)\n",
        "    print(\"masks_tensors:\", masks_tensors.shape)\n",
        "    print(\"label_ids:\", label_ids.shape)\n",
        "    print('-' * 30)\n",
        "    c += 1\n",
        "    \n",
        "    # input > token > segment > position\n",
        "    input_words += ['[PAD]'] * (len(masks_tensors[idx]) - len(input_words))\n",
        "    for i, t, s, m in zip(input_words, tokens_tensors[idx], segments_tensors[idx], masks_tensors[idx]):\n",
        "        print(i, t.item(), '   ', s.item(),  '  ', m.item())\n",
        "    \n",
        "    \n",
        "    if c >= 1:\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0:\n",
            "tokens_tensors: torch.Size([64, 63])\n",
            "segments_tensors: torch.Size([64, 63])\n",
            "masks_tensors: torch.Size([64, 63])\n",
            "label_ids: torch.Size([64])\n",
            "------------------------------\n",
            "[CLS] 101     0    1\n",
            "注 3800     0    1\n",
            "射 2198     0    1\n",
            "胰 5536     0    1\n",
            "岛 2270     0    1\n",
            "素 5162     0    1\n",
            "会 833     0    1\n",
            "上 677     0    1\n",
            "瘾 4614     0    1\n",
            "？ 8043     0    1\n",
            "5 126     0    1\n",
            "种 4905     0    1\n",
            "情 2658     0    1\n",
            "况 1105     0    1\n",
            "必 2553     0    1\n",
            "须 7557     0    1\n",
            "使 886     0    1\n",
            "用 4500     0    1\n",
            "胰 5536     0    1\n",
            "岛 2270     0    1\n",
            "素 5162     0    1\n",
            "治 3780     0    1\n",
            "疗 4545     0    1\n",
            "！ 8013     0    1\n",
            "[SEP] 102     0    1\n",
            "谣 6469     1    1\n",
            "言 6241     1    1\n",
            "粉 5106     1    1\n",
            "碎 4810     1    1\n",
            "机 3322     1    1\n",
            "｜ 8078     1    1\n",
            "吃 1391     1    1\n",
            "粗 5110     1    1\n",
            "粮 5117     1    1\n",
            "能 5543     1    1\n",
            "降 7360     1    1\n",
            "糖 5131     1    1\n",
            "？ 8043     1    1\n",
            "胰 5536     1    1\n",
            "岛 2270     1    1\n",
            "素 5162     1    1\n",
            "注 3800     1    1\n",
            "射 2198     1    1\n",
            "液 3890     1    1\n",
            "必 2553     1    1\n",
            "须 7557     1    1\n",
            "放 3123     1    1\n",
            "冰 1102     1    1\n",
            "箱 5056     1    1\n",
            "？ 8043     1    1\n",
            "[SEP] 102     1    1\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n",
            "<pad> 0     0    0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJFoPUhkF-1d",
        "colab_type": "text"
      },
      "source": [
        "### 直接預測\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJSNXU9yGCpM",
        "colab_type": "code",
        "outputId": "c5ccbc5d-9e1a-4ae5-f489-3e6d513a2f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
        "df_dev.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3206, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-jZjOo7GVCH",
        "colab_type": "code",
        "outputId": "89956662-3457-4340-f94a-761f66b66862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "df_dev.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unrelated    2187\n",
              "agreed        934\n",
              "disagreed      85\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gI9BpDtGcYW",
        "colab_type": "code",
        "outputId": "ddf9fde4-ed95-4b3a-bb4b-6fae0277939d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_dev.label.apply(lambda x: 1 if x == \"unrelated\" else 0).sum() / len(df_dev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.682158452900811"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-dMg09yGwaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = FakeNewsDataset(\"dev\", tokenizer=bert_tokenizer)\n",
        "devloader = DataLoader(dev, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8m1IgRRGBFA",
        "colab_type": "code",
        "outputId": "060b05c6-7a34-40d9-8542-5ec5b321aa81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in devloader:\n",
        "        tokens_tensors, segments_tensors, \\\n",
        "        masks_tensors, labels = [t.to(device) for t in data]\n",
        "        \n",
        "        logits = classifier(tokens_tensors, segments_tensors, masks_tensors)\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on dev set: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on dev set: 67 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3H5AM-qKbmH",
        "colab_type": "code",
        "outputId": "ddc71825-5edc-4325-f98d-20eee7699741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "labels, predicted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1, 0, 0, 2, 2, 0], device='cuda:0'),\n",
              " tensor([0, 0, 0, 2, 0, 0], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIGM4QADxp6r",
        "colab_type": "text"
      },
      "source": [
        "### Loss function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFyHvMmoyBdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Model has {num_params} parameters\") \n",
        "\n",
        "# model.to('cuda:0')\n",
        "# loss = torch.nn.NLLLoss()\n",
        "# loss = loss.to('cuda:0')\n",
        "\n",
        "# learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-VpuNKfxpxa",
        "colab_type": "code",
        "outputId": "8a54a748-6f43-4961-a84a-542433223637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "learnable_params = [p for p in classifier.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)\n",
        "print(f\"# params: {sum(p.numel() for p in learnable_params)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# params: 102269955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gR5ez4Gxrn5",
        "colab_type": "text"
      },
      "source": [
        "### Finetune Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zD1tYmD1z3G",
        "colab_type": "text"
      },
      "source": [
        "Outputs:\n",
        "    if `labels` is not `None`:\n",
        "        Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "    if `labels` is `None`:\n",
        "        Outputs the classification logits of shape [batch_size, num_labels]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq9SLgQH1v_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.train()\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaHFoCLrNQ5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_on_dev():\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in devloader:\n",
        "            tokens_tensors, segments_tensors, \\\n",
        "            masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "            logits = classifier(tokens_tensors, segments_tensors, masks_tensors)\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on dev set: %.2f %%' % (\n",
        "        100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oInpjd_vWhfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PMyb7v9vy_X",
        "colab_type": "code",
        "outputId": "2a345e53-d55b-46cd-ea46-b2b657fdc2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbYQ-vijv5Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = '/content/drive/My Drive/latest_checkpoint.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5snyAkUAwVwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    classifier.load_state_dict(torch.load(MODEL_PATH))\n",
        "    print(\"recovered latest checkpoints.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO45MeycU2XO",
        "colab_type": "text"
      },
      "source": [
        "- 1 epoch: 50 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKAW6vnxrs0",
        "colab_type": "code",
        "outputId": "867e7ad3-4cfe-42b3-855a-628975c8982b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "%%time\n",
        "EPOCHS = 30\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    \n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        tokens_tensors, segments_tensors, \\\n",
        "        masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "#         logits = classifier(input_ids=tokens_tensors, \n",
        "#                             token_type_ids=segments_tensors, \n",
        "#                             attention_mask=masks_tensors)\n",
        "#         loss = criterion(logits, labels)\n",
        "#         loss.backward()\n",
        "        loss = classifier(input_ids=tokens_tensors, \n",
        "                            token_type_ids=segments_tensors, \n",
        "                            attention_mask=masks_tensors, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print('[epoch %d, steps %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss))\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    \n",
        "    evaluate_on_dev()\n",
        "    torch.save(classifier.state_dict(), \n",
        "               f\"/content/drive/My Drive/checkpoint_epoch_{epoch + 1}.pth\")\n",
        "    torch.save(classifier.state_dict(), MODEL_PATH)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 1, steps   100] loss: 47.437\n",
            "[epoch 1, steps   200] loss: 38.050\n",
            "[epoch 1, steps   300] loss: 36.146\n",
            "Accuracy of the network on dev set: 83.91 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [05:26<48:59, 326.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 2, steps   100] loss: 31.601\n",
            "[epoch 2, steps   200] loss: 29.002\n",
            "[epoch 2, steps   300] loss: 27.861\n",
            "Accuracy of the network on dev set: 85.50 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [10:53<43:32, 326.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 3, steps   100] loss: 25.822\n",
            "[epoch 3, steps   200] loss: 22.838\n",
            "[epoch 3, steps   300] loss: 26.275\n",
            "Accuracy of the network on dev set: 83.81 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [16:19<38:06, 326.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 4, steps   100] loss: 24.291\n",
            "[epoch 4, steps   200] loss: 22.369\n",
            "[epoch 4, steps   300] loss: 21.653\n",
            "Accuracy of the network on dev set: 85.75 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [21:46<32:39, 326.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 5, steps   100] loss: 19.547\n",
            "[epoch 5, steps   200] loss: 15.473\n",
            "[epoch 5, steps   300] loss: 16.879\n",
            "Accuracy of the network on dev set: 83.06 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [27:12<27:12, 326.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 6, steps   100] loss: 16.239\n",
            "[epoch 6, steps   200] loss: 12.886\n",
            "[epoch 6, steps   300] loss: 13.843\n",
            "Accuracy of the network on dev set: 82.41 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [32:39<21:45, 326.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 7, steps   100] loss: 15.607\n",
            "[epoch 7, steps   200] loss: 17.340\n",
            "[epoch 7, steps   300] loss: 19.988\n",
            "Accuracy of the network on dev set: 86.03 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [38:05<16:19, 326.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 8, steps   100] loss: 13.632\n",
            "[epoch 8, steps   200] loss: 13.587\n",
            "[epoch 8, steps   300] loss: 13.003\n",
            "Accuracy of the network on dev set: 86.28 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [43:31<10:52, 326.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 9, steps   100] loss: 11.371\n",
            "[epoch 9, steps   200] loss: 10.259\n",
            "[epoch 9, steps   300] loss: 11.037\n",
            "Accuracy of the network on dev set: 85.25 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [48:58<05:26, 326.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 10, steps   100] loss: 9.708\n",
            "[epoch 10, steps   200] loss: 9.456\n",
            "[epoch 10, steps   300] loss: 10.046\n",
            "Accuracy of the network on dev set: 85.75 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [54:26<00:00, 326.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "CPU times: user 31min 2s, sys: 22min 52s, total: 53min 54s\n",
            "Wall time: 54min 26s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4qBxVymBOuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "678e2832-7924-4f31-8261-857405a558fe"
      },
      "source": [
        "train.df.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23871, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77wjd3G3VDqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6f5fadc-6e5f-4e25-eabf-91b735f100a3"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "        tokens_tensors, segments_tensors, \\\n",
        "        masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "        logits = classifier(tokens_tensors, segments_tensors, masks_tensors)\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on train set: %.2f %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on train set: 97.64 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-etz60i0XmcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klmDXLoxYdgt",
        "colab_type": "text"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzLyEbjZh9h",
        "colab_type": "text"
      },
      "source": [
        "- 1 epoch 20 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIpe5Uy-ZzUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for p in classifier.bert.parameters():\n",
        "#     p.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAceVaWZzJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learnable_params = [p for p in classifier.parameters() if p.requires_grad]\n",
        "# print(f\"# params: {sum(p.numel() for p in learnable_params)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk8ElzfMg_R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# EPOCHS = 10\n",
        "\n",
        "# for epoch in tqdm(range(EPOCHS)):\n",
        "    \n",
        "    \n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "#         tokens_tensors, segments_tensors, \\\n",
        "#         masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         logits = classifier(input_ids=tokens_tensors, \n",
        "#                             token_type_ids=segments_tensors, \n",
        "#                             attention_mask=masks_tensors)\n",
        "#         loss = criterion(logits, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if i % 100 == 99:    # print every 100 mini-batches\n",
        "#             print('[epoch %d, steps %5d] loss: %.3f' %\n",
        "#                   (epoch + 1, i + 1, running_loss))\n",
        "#             running_loss = 0.0\n",
        "    \n",
        "    \n",
        "#     evaluate_on_dev()\n",
        "#     torch.save(classifier.state_dict(), './checkpoint.pth')\n",
        "\n",
        "# print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwbxcJXUMdGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in trainloader:\n",
        "#         tokens_tensors, segments_tensors, \\\n",
        "#         masks_tensors, labels = [t.to(device) for t in data]\n",
        "        \n",
        "#         logits = classifier(tokens_tensors, segments_tensors, masks_tensors)\n",
        "#         _, predicted = torch.max(logits.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on train set: %d %%' % (\n",
        "#     100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSORczoNz-tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: demo 加　script 版本"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0rrDY-0z97B",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv33bO1xxryG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYzorr6wjoYz",
        "colab_type": "text"
      },
      "source": [
        "### 產生 test 結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETd7RjB2-sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        " \n",
        "    \n",
        "class FakeNewsDataset(Dataset):\n",
        "    # 讀取原始 tsv 檔並 setup 一些參數\n",
        "    def __init__(self, mode, tokenizer):\n",
        "        assert mode in [\"train\", \"dev\", \"test\"]\n",
        "        self.mode = mode\n",
        "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\")\n",
        "        self.len = len(self.df)\n",
        "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2} # TEMP\n",
        "\n",
        "            \n",
        "#         self.labels = sorted(self.df.label.unique())\n",
        "#         self.label_map = {label: i for i, label in enumerate(self.labels)}\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    # 回傳一筆訓練 / 驗證 / 測試數據\n",
        "#     @pysnooper.snoop()\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"test\":\n",
        "            text_a, text_b = self.df.iloc[idx, :].values\n",
        "            label_tensor = None\n",
        "        else:\n",
        "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
        "            # 將 label 也轉換成索引\n",
        "            label_id = self.label_map[label]\n",
        "            label_tensor = torch.tensor(label_id)\n",
        "            \n",
        "        \n",
        "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
        "        word_pieces = [\"[CLS]\"]\n",
        "        tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        word_pieces += tokens_a + [\"[SEP]\"]\n",
        "        len_a = len(word_pieces)\n",
        "        \n",
        "        # 第二個句子的 BERT tokens\n",
        "        tokens_b = self.tokenizer.tokenize(text_b)\n",
        "        word_pieces += tokens_b + [\"[SEP]\"]\n",
        "        len_b = len(word_pieces) - len_a\n",
        "        \n",
        "        # 將整個 token 序列轉換成索引序列\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "        \n",
        "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
        "                                        dtype=torch.long)\n",
        "        \n",
        "        return (tokens_tensor, segments_tensor, label_tensor)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "    \n",
        "    if samples[0][2] is not None:\n",
        "        label_ids = torch.stack([s[2] for s in samples])\n",
        "    else:\n",
        "        label_ids = None\n",
        "    \n",
        "    tokens_tensors = pad_sequence(tokens_tensors, \n",
        "                                  batch_first=True)\n",
        "    \n",
        "    segments_tensors = pad_sequence(segments_tensors, \n",
        "                                    batch_first=True)\n",
        "    \n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
        "                                dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(\n",
        "        tokens_tensors != 0, 1)\n",
        "    \n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XJJJul33J1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b211d7f-62fc-40b3-a97f-747aaa513e77"
      },
      "source": [
        "# dev.label_map"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'agreed': 0, 'disagreed': 1, 'unrelated': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxjvICV-1F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c1df4acf-f5ef-49ad-f0f4-a0de1dfa9980"
      },
      "source": [
        "# test.df.title2_zh.apply(lambda x: 1 if type(x) == float else 0 ).sum()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCY6cdXS-lzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaXIvgrMloRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = FakeNewsDataset(\"test\", tokenizer=bert_tokenizer)\n",
        "testloader = DataLoader(test, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC0dtEm1_MMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.df.fillna(\"\", inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7GstntFlZt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_test():\n",
        "\n",
        "    predictions = None\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            tokens_tensors = data[0].to(device)\n",
        "            segments_tensors = data[1].to(device)\n",
        "            masks_tensors = data[2].to(device)\n",
        "\n",
        "            logits = classifier(tokens_tensors, segments_tensors, masks_tensors)\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            \n",
        "            if predictions is None:\n",
        "                \n",
        "                predictions = predicted\n",
        "            else:\n",
        "                predictions = torch.cat((predictions, predicted))\n",
        "\n",
        "#     print('Accuracy of the network on dev set: %.2f %%' % (\n",
        "#         100 * correct / total))\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xYPNAJV5ikg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = predict_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsUWIcOy83fA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3b1a0dc-7e40-465c-e658-d679301f5963"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80126])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaitKMaj7tj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ec8fea1-0a5b-47bb-9c37-5dce0b17b4c3"
      },
      "source": [
        "index_map = {v: k for k, v in test.label_map.items()}\n",
        "index_map"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'agreed', 1: 'disagreed', 2: 'unrelated'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC_k0DsF6Y3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "8109c6ae-9ffe-4e26-a0ba-bf27ba73966f"
      },
      "source": [
        "result = pd.DataFrame({\"result\": predictions.tolist()})\n",
        "result.columns = ['label']\n",
        "result['label'] = result.label.apply(lambda x: index_map[x])\n",
        "result.head()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label\n",
              "0  unrelated\n",
              "1  unrelated\n",
              "2  unrelated\n",
              "3  unrelated\n",
              "4  unrelated"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tylM31y267V6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "616d8d84-614c-4f18-b54e-f9465322e5c8"
      },
      "source": [
        "!test -d test.csv.zip || wget https://s3-ap-northeast-1.amazonaws.com/smartnews-dmp-tmp/meng/test.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-07 14:53:09--  https://s3-ap-northeast-1.amazonaws.com/smartnews-dmp-tmp/meng/test.csv.zip\n",
            "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.4.142\n",
            "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.4.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6906894 (6.6M) [application/zip]\n",
            "Saving to: ‘test.csv.zip’\n",
            "\n",
            "\rtest.csv.zip          0%[                    ]       0  --.-KB/s               \rtest.csv.zip        100%[===================>]   6.59M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-07-07 14:53:09 (48.4 MB/s) - ‘test.csv.zip’ saved [6906894/6906894]\n",
            "\n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIEM05_d67KJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "c705ebd5-98a9-40ef-9dad-4220db21c529"
      },
      "source": [
        "df_test = pd.read_csv(\"test.csv\").fillna('')\n",
        "df_test.head()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tid1</th>\n",
              "      <th>tid2</th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>title1_en</th>\n",
              "      <th>title2_en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>321187</td>\n",
              "      <td>167562</td>\n",
              "      <td>59521</td>\n",
              "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
              "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
              "      <td>egypt 's presidential election failed to win m...</td>\n",
              "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>321190</td>\n",
              "      <td>167564</td>\n",
              "      <td>91315</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>321189</td>\n",
              "      <td>167563</td>\n",
              "      <td>167564</td>\n",
              "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>Will the United States wage war on Iraq withou...</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>321193</td>\n",
              "      <td>167564</td>\n",
              "      <td>160994</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>The hanging Saddam is a surrogate? This man's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>321191</td>\n",
              "      <td>167564</td>\n",
              "      <td>15084</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>Chinese loquat loquat plaster in America? Pure...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...                                          title2_en\n",
              "0  321187  ...  Lyon! Lyon officials have denied that Felipe F...\n",
              "1  321190  ...  The Top 10 Americans believe that the Lizard M...\n",
              "2  321189  ...  A message from Saddam Hussein after he was cap...\n",
              "3  321193  ...  The hanging Saddam is a surrogate? This man's ...\n",
              "4  321191  ...  Chinese loquat loquat plaster in America? Pure...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvF4bGoG7NiN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "ee65880f-9c78-445b-c8c6-9a5e4c2ca30d"
      },
      "source": [
        "final_result = pd.concat([df_test.reset_index().loc[:, 'id'], result.loc[:, 'label']], axis=1)\n",
        "final_result.columns = ['Id', 'Category']\n",
        "final_result.head()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>321187</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>321190</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>321189</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>321193</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>321191</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Id   Category\n",
              "0  321187  unrelated\n",
              "1  321190  unrelated\n",
              "2  321189  unrelated\n",
              "3  321193  unrelated\n",
              "4  321191  unrelated"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hna32DG07NbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_result.to_csv('bert_epoch10_bsize64_pytorch_10prec_num_sample_23871_train.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKV_0gIk6Pmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1XaO_0fm5M0",
        "colab_type": "text"
      },
      "source": [
        "We can see that this is a *massive* gain over our CNN baseline and also improves over our ELMo contextual embeddings for this dataset.  BERT has been shown high-performance results across many datasets, and integrating it into unstructured prediction problems is quite simple, as we saw in this section.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this section we investigated the Transformer model architecture, particularly in the context of pretraining LMs.  We discussed some of the model details and we looked at how BERT extends the GPT approach from OpenAI.  We then built our own fine-tuned classifier using the Hugging Face PyTorch library to create and re-load the BERT model and add our own layers on top.\n",
        "\n",
        "### Some further resources\n",
        "\n",
        "We have only scratched the surface of the exciting way that transfer learning is transforming NLP. \n",
        "\n",
        "\n",
        "- **Transformer Architecture**\n",
        "  - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html): mentioned previously, but so good it deserves mentioning again\n",
        "  - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/):  good tutorial on how the Transformer works\n",
        "-  A really nice blogpost on transfer learning from Sebastian Ruder (http://ruder.io/nlp-imagenet/)\n",
        "\n",
        "- **Transfer Learning**\n",
        "  - A [fantastic tutorial at NAACL this year](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit) which is both thorough and introductory.  It covers a lot of material including how to probe pretrain models to try and figure out what they are up to\n",
        "  - A nice colab from the Google BERT devs showing using BERT from TF-Hub (https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb)\n",
        "\n",
        "- **Model Intepretation and Probing**\n",
        "  - Jesse Vig's Blog post analyzing the different heads of BERT based\n",
        "    - Part I: https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77\n",
        "    -  Part II: https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n",
        "    - And a colab that drills into the [Q and K vectors during multi-head attention here](https://colab.research.google.com/drive/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB): \n",
        "  - [Kevin Clark's Jupyter Notebooks](https://github.com/clarkkev/attention-analysis) for [What Does BERT Look At? An Analysis of BERT's Attention, Clark et al., 2019](https://arxiv.org/abs/1906.04341)\n",
        "  - [Tal Linzen's code](https://github.com/TalLinzen/rnn_agreement) for [Assessing the ability of LSTMs to learn syntax-sensitive dependencies, Linzen et al., 2016](https://arxiv.org/abs/1611.01368)\n",
        "  - [Yoav Goldberg's code](https://github.com/yoavg/bert-syntax) assessing syntactic abilities of BERT\n",
        "  - [Nelson Liu's code](https://github.com/nelson-liu/contextual-repr-analysis) for [Linguistic Knowledge and Transferability of Contextual Representations, Liu et al., 2019](https://homes.cs.washington.edu/~nfliu/papers/liu+gardner+belinkov+peters+smith.naacl2019.pdf)\n",
        "\n",
        "- **More about Neural NLP**\n",
        "  -  Get right into the source material.  Some papers that are helpful to understand deep learning in NLP (https://github.com/dpressel/lit)\n",
        "\n",
        "- **Get Hacking**\n",
        "  - Implementations of most of what we talked about today in TensorFlow and PyTorch (https://github.com/dpressel/baseline)\n",
        "\n",
        "There is also an end-to-end example using the Baseline API above to train a GPT-like LM using the code above in PyTorch:\n",
        "\n",
        "https://github.com/dpressel/baseline/blob/master/api-examples/pretrain-transformer-lm.py\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DogOyWz4J0Oj",
        "colab_type": "text"
      },
      "source": [
        "# Part III: Fine-tuning a pre-trained model\n",
        "\n",
        "In the last section, we looked at using a biLM networks layers as embeddings for our classification model.  In that approach, we maintain the exact same model architecture as before, but just switching our word embeddings out for context embeddings (or, more commonly, using them in concert).\n",
        "\n",
        "The paper [Improving Language Understanding\n",
        "by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford et al 2018) explored a different approach, much more similar to what is typically done in computer vision.  In fine-tuning, we reuse the network architecture and simply replace the head.  We dont use any model specific architecture anymore, just a final layer.  There is an accompanying blog post [here](https://openai.com/blog/language-unsupervised/).  The image below is borrowed from that blog post\n",
        "\n",
        "![alt text](https://openai.com/content/images/2018/06/zero-shot-transfer@2x.png)\n",
        "\n",
        "As we can see from the images, these models can rapidly improve our downstream performance with very limited fine-tuning supervision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwPi74SXcePJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## The Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzWFknRFubv4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The original Transformer is an all-attention encoder-decoder model first introduced in [Attention Is All You Need, Vaswani et al., 2017](https://arxiv.org/abs/1706.03762).  It is described at a high-level in [this Google AI post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n",
        "Here is an image of the model architecture for a Transformer:\n",
        "\n",
        "![Transformer Architecture](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)\n",
        "\n",
        "The reference implementation from Google is the [tensor2tensor repository](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor).  There is a lot going on in that codebase, which some people may find hard to follow.\n",
        "\n",
        "We are going to go through each component in a hands-on manner, which will hopefully give you a visual feel of what is happening.\n",
        "\n",
        "If you want to understand Transformers better, there is a terrific blog post called [The Annotated Transformer, Rush, 2018](http://nlp.seas.harvard.edu/2018/04/03/attention.html) where you can see how to code up a Transformer from scratch to do Neural Machine Translation (NMT) while following along with the paper.\n",
        "\n",
        "In versions used in practice, there are slight differences from the actual image, most notably, that layer norm is performed first.  Also, in a causal LM pre-training setting, as in the case of GPT, we have no need for the decoder, which simplifies our architecture substantially, leaving only a masked self-attention in the encoder (this prevents us from seeing the future as we predict).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSc6xaSKc51A",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### A Transformer Encoder Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCjAPGnbt7X3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here is code adapted from [Baseline](https://github.com/dpressel/baseline) that implements a Transformer block used in a GPT-like architecture (pictured above).  We are going to take a closer look at these blocks, so lets think of this as the high-level overview.  The input to this class is a `torch.Tensor` of shape `BxT`.  The first sub-component in a Transformer block is the Multi-Headed Attention.  The second is the \"FFN\" shown in the image -- an MLP layer followed by a linear projection back to the original size.  We encapsulate these transformations in an `nn.Sequential`.  Notice that each sub-layer is also a residual connection.\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_heads, d_model, pdrop, scale=True, activation_type='relu', d_ff=None):\n",
        "        \"\"\"\n",
        "        :param num_heads (`int`): the number of heads for self-attention\n",
        "        :param d_model (`int`): The model dimension size\n",
        "        :param pdrop (`float`): The dropout probability\n",
        "        :param scale (`bool`): Whether we are doing scaled dot-product attention\n",
        "        :param activation_type: What activation type to use\n",
        "        :param d_ff: The feed forward layer size\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff if d_ff is not None else num_heads * d_model\n",
        "        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)\n",
        "        self.ffn = nn.Sequential(nn.Linear(self.d_model, self.d_ff),\n",
        "                                 pytorch_activation(activation_type),\n",
        "                                 nn.Linear(self.d_ff, self.d_model))\n",
        "        self.ln1 = nn.LayerNorm(self.d_model, eps=1e-12)\n",
        "        self.ln2 = nn.LayerNorm(self.d_model, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(pdrop)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        :param x: the inputs\n",
        "        :param mask: a mask for the inputs\n",
        "        :return: the encoder output\n",
        "        \"\"\"\n",
        "        # Builtin Attention mask\n",
        "        x = self.ln1(x)\n",
        "        h = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(h)\n",
        "\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.dropout(self.ffn(x))\n",
        "        return x\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUTC74cgdPDE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Multi-headed Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImxtZOiYt-nq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Multi-headed attention is one of the key innovations of the Transformer.  The idea was to allow each attention head to learn different relations.\n",
        "\n",
        "![MHA](https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png)\n",
        "\n",
        "#### Scaled dot product attention\n",
        "\n",
        "Here is a picture of the operations involved in scaled dot product attention.\n",
        "\n",
        "![MHA Architecture](http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png)\n",
        "\n",
        "`Q`, `K` and `V` are low-order projections of the input.  For Encoder-Decoders, the `Q` is a query vector in the decoder, and `K` and `V` are representations of the Encoder.  A dot product of the encoder keys and the query vector determines a set of weights that are applied against the `V` (again, also a representation of the encoder values).  In the case of the encoder, these are all drawn from the same input.  Basic dot product attention was actually introduced in [Effective Approaches to Attention-based Neural Machine Translation, Luong et al., 2014](https://arxiv.org/abs/1508.04025), but in the the Transformer paper, the authors made a strong case that the basic dot product attention benefits from scaling.\n",
        "\n",
        "This is implemented (again adapted from Baseline), as follows:\n",
        "\n",
        "```python\n",
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    We apply the query to the keys to recieve our weights via softmax, which are then applied\n",
        "    for each value, but in a series of efficient matrix operations.  In the case of self-attention,\n",
        "    the key, query and values are all low order projections of the same input.\n",
        "\n",
        "    :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n",
        "    :param key: a set of keys from encoder or self\n",
        "    :param value: a set of values from encoder or self\n",
        "    :param mask: masking (for destination) to prevent seeing what we shouldnt\n",
        "    :param dropout: apply dropout operator post-attention (this is not a float)\n",
        "    :return: A tensor that is (BxHxTxT)\n",
        "\n",
        "    \"\"\"\n",
        "    # (., H, T, T) = (., H, T, D) x (., H, D, T)\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        weights = dropout(weights)\n",
        "    return torch.matmul(weights, value), weights\n",
        "```\n",
        "\n",
        "\n",
        "#### The Multi-head part\n",
        "\n",
        "Each of the attention operations above that we apply is going to learn some weighted representation of our input -- what are we paying attention to?  There are lots of things that might be useful!   We might want to attend to the next word for language modeling.  To remember what we said, we might want to learn something like which pronouns refer to which nouns that we saw in previous tokens (this is called anaphora resolution and is a subset of coreference resolution).  We might hope that it picks up parse dependencies, that could help us with tasks that benefit from syntax.  Remember that each of our `Q`, `K` and `V` are low-order projections of our input.  What if we had many low-order projections and used each to learn different weightings?  This  is exactly what multi-head attention is.  Each \"head\" does the operation above and learns something meaningful (or at least, we hope it does!).\n",
        "\n",
        "Here is some code that implements multi-headed attention using our function above:\n",
        "\n",
        "```python\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-headed attention from https://arxiv.org/abs/1706.03762 via http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "    Multi-headed attention provides multiple looks of low-order projections K, Q and V using an attention function\n",
        "    (specifically `scaled_dot_product_attention` in the paper.  This allows multiple relationships to be illuminated\n",
        "    via attention on different positional and representational information from each head.\n",
        "\n",
        "    The number of heads `h` times the low-order projection dim `d_k` is equal to `d_model` (which is asserted upfront).\n",
        "    This means that each weight matrix can be simply represented as a linear transformation from `d_model` to `d_model`,\n",
        "    and partitioned into heads after the fact.\n",
        "\n",
        "    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the\n",
        "    sub-sequent `FFN` sub-layer.\n",
        "\n",
        "    There are 3 uses of multi-head attention in the Transformer.\n",
        "    For encoder-decoder layers, the queries come from the previous decoder layer, and the memory keys come from\n",
        "    the encoder.  For encoder layers, the K, Q and V all come from the output of the previous layer of the encoder.\n",
        "    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using\n",
        "    future values\n",
        "    \"\"\"\n",
        "    def __init__(self, h, d_model, dropout=0.1, scale=False):\n",
        "        \"\"\"Constructor for multi-headed attention\n",
        "\n",
        "        :param h: The number of heads\n",
        "        :param d_model: The model hidden size\n",
        "        :param dropout (``float``): The amount of dropout to use\n",
        "        :param attn_fn: A function to apply attention, defaults to SDP\n",
        "        \"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.w_Q = nn.Linear(d_model, d_model)\n",
        "        self.w_K = nn.Linear(d_model, d_model)\n",
        "        self.w_V = nn.Linear(d_model, d_model)\n",
        "        self.w_O = nn.Linear(d_model, d_model)\n",
        "        self.attn_fn = scaled_dot_product_attention if scale else dot_product_attention\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"Low-order projections of query, key and value into multiple heads, then attention application and dropout\n",
        "\n",
        "        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n",
        "        :param key: a set of keys from encoder or self\n",
        "        :param value: a set of values from encoder or self\n",
        "        :param mask: masking (for destination) to prevent seeing what we shouldnt\n",
        "        :return: Multi-head attention output, result of attention application to sequence (B, T, d_model)\n",
        "        \"\"\"\n",
        "        batchsz = query.size(0)\n",
        "\n",
        "        # (B, H, T, D)\n",
        "        query = self.w_Q(query).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.w_K(key).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.w_V(value).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attn = self.attn_fn(query, key, value, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(batchsz, -1, self.h * self.d_k)\n",
        "        return self.w_O(x)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "We are going to take a look at how multi-headed attention works visually. To do this, we are going to use the [viz-bert codebase](https://github.com/jessevig/bertviz) from Jesse Vig.  The accompanying paper is [A Multiscale Visualization of Attention in the Transformer Model, Vig, 2019](https://arxiv.org/pdf/1906.05714.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CFtKaxh3jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import sys\n",
        "\n",
        "# !test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
        "# # !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n",
        "# !test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "# if not 'bertviz_repo' in sys.path:\n",
        "#   sys.path += ['bertviz_repo']\n",
        "# !pip install regex\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV8mVhPmiV12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from bertviz import attention, visualization\n",
        "# from bertviz.pytorch_pretrained_bert import BertModel as VizBertModel\n",
        "# from bertviz.pytorch_pretrained_bert import BertTokenizer as VizBertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fAhbp8_iaYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%javascript\n",
        "# require.config({\n",
        "#   paths: {\n",
        "#       d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
        "#   }\n",
        "# });"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GVPSIRCilZK",
        "colab_type": "text"
      },
      "source": [
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfHXfykipHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = VizBertModel.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = VizBertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# sentence_a = \"The dog crossed the road .\"\n",
        "# sentence_b = \"The owner came out and put him on a leash .\"\n",
        "# attention_visualizer = visualization.AttentionVisualizer(model, tokenizer)\n",
        "# tokens_a, tokens_b, attn = attention_visualizer.get_viz_data(sentence_a, sentence_b)\n",
        "# call_html()\n",
        "# attention.show(tokens_a, tokens_b, attn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrWwpDJgiwe_",
        "colab_type": "text"
      },
      "source": [
        "Try playing around with `sentence_a` and `sentence_b`.  You can select and unselect different attention heads, as well as the layer that you are visualizing.  There is a lot going on here.  [This blog post](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)  by Jesse Vig, the author of the software we are using to render the attention heads above, discusses how BERT attention heads learn various types of attention.  [Clark et al 2019 have a paper](https://arxiv.org/abs/1906.04341) that also delves into what learns, particular in the context of our linguistic notions of syntax\n",
        "\n",
        "It turns out BERT learns a lot of stuff:\n",
        "\n",
        "\n",
        "- **next/previous/identical word tracking**\n",
        "\n",
        "- **stuff that correlates closely to linguistic notions of syntax**:  \n",
        "\n",
        "  - BERT attention heads learn something like coreference\n",
        "  - BERT attention heads learn some approximation of dependency parsing.  Different attention heads learn different dependency/governor relationships\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aKHvPcqhcHI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#### Multi-Headed Attention is easy now in PyTorch!!\n",
        "\n",
        "This operation is now built into PyTorch.  There is a caveat that only scaled-dot product attention is supported.  The code above does not use that module since it supports both scaled and unscaled attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlF7dO75fGqg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Positional embeddings\n",
        "\n",
        "To eliminate auto-regressive (RNN) models from the transformer, positional embeddings need to be created and added to the word embeddings.  Otherwise, during attention there would be no way to account for word position. There are several ways to support positional embeddings.\n",
        "\n",
        "The first way is very simple -- you just need to create a `nn.Embedding` that you give your offsets for each token.  Embedding representations will be learned for each position, but you can only learn up to the number of positions you have seen.\n",
        "\n",
        "Another way, used in the original Transformer is to embed a bunch of sinusoids with different frequencies that are a function of the position:\n",
        "\n",
        "$$PE_{(pos,2i)}=sin(pos/10000^{2i}/dmodel)$$\n",
        "$$PE_{(pos,2i+1)}=cos(pos/10000^{2i}/dmodel)$$ \n",
        "\n",
        "where $pos$ is the position and $i$ is the dimension corresponding to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000\\times2\\pi$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ7U0w14oEet",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KldN58J3ueE6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "For this section of the tutorial, we are going to fine-tune BERT [Devlin et al 2018](https://arxiv.org/abs/1810.04805), a transformer architecture that replaces the causal LM objective with 2 new objectives:\n",
        "\n",
        "1. **Masking out words** with some probability, predict the missing words (MLM objective)\n",
        "\n",
        "![MLM](https://2.bp.blogspot.com/-pNxcHHXNZg0/W9iv3evVyOI/AAAAAAAADfA/KTSvKXNzzL0W8ry28PPl7nYI1CG_5WuvwCLcBGAs/s1600/f1.png)\n",
        "\n",
        "2. Given 2 adjacent sentences, **predict if the second sentence follows the first** (NSP objective)\n",
        "\n",
        "![NSP](https://4.bp.blogspot.com/-K_7yu3kjF18/W9iv-R-MnyI/AAAAAAAADfE/xUwR_G1iTY0vq9X-Z3LnW5t4NLS9BQzdgCLcBGAs/s1600/f2.png)\n",
        "\n",
        "From an architecture diagram, [this blog post announcing BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) notes the differences:\n",
        "\n",
        "![BERT vs GPT and ELMo](https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s1600/image3.png)\n",
        "\n",
        "Our model will simply build on the existing model architecture with a single transformation layer to the output number of classes.  BERT is [open source](https://github.com/google-research/bert) but the code is in TensorFlow, and since this tutorial is written in PyTorch, we need a different solution.  We will use the [Hugging Face Transformer codebase](https://github.com/huggingface/pytorch-pretrained-BERT) as our API -- it can read in the original Google-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kx9qSNMgqVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pytorch-pretrained-bert\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1avGnWmjhbwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import io\n",
        "# import os\n",
        "# import re\n",
        "# import codecs\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "# from pytorch_pretrained_bert.modeling import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jwk_aHZfFa8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Tokenization in BERT\n",
        "\n",
        "In the last sequence, we talked about how ELMo biLMs can limit their parameters while accounting for unseen words using character-compositional word embeddings.  This technique is very powerful, but its also slow.  It is common in NMT to use some sort of sub-word encoding that limits the vocabulary size, but allows us to not have unattested words.  The `tensor2tensor` codebase, for example, creates an invertible encoding for words into sub-tokens with a limited vocabulary.  The tokenizer is built from a corpus upfront and stored in a file, and then can be used to encode text.\n",
        "\n",
        "There are 4 phases in this algorithm described in the tensor2tensor codebase:\n",
        "\n",
        "\n",
        "    1. Tokenize into a list of tokens.  Each token is a unicode string of either\n",
        "      all alphanumeric characters or all non-alphanumeric characters.  We drop\n",
        "      tokens consisting of a single space that are between two alphanumeric\n",
        "      tokens.\n",
        "    2. Escape each token.  This escapes away special and out-of-vocabulary\n",
        "      characters, and makes sure that each token ends with an underscore, and\n",
        "      has no other underscores.\n",
        "    3. Represent each escaped token as a the concatenation of a list of subtokens\n",
        "      from the limited vocabulary.  Subtoken selection is done greedily from\n",
        "      beginning to end.  That is, we construct the list in order, always picking\n",
        "      the longest subtoken in our vocabulary that matches a prefix of the\n",
        "      remaining portion of the encoded token.\n",
        "    4. Concatenate these lists.  This concatenation is invertible due to the\n",
        "      fact that the trailing underscores indicate when one list is finished.\n",
        "\n",
        "\n",
        "\n",
        "We can access Google's trained BERT Tokenizer via the Hugging Face API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS2Kj03ZwPN8",
        "colab_type": "text"
      },
      "source": [
        "### Bert tokenizer / vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqkPLsMm0Nb8",
        "colab_type": "text"
      },
      "source": [
        "Our model this time around is very simple.  It has an output linear layer that comes from pooled output from BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR08ZqxRJpXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def whitespace_tokenizer(words):\n",
        "#     return words.split() \n",
        "\n",
        "# def sst2_tokenizer(words):\n",
        "#     REPLACE = { \"'s\": \" 's \",\n",
        "#                 \"'ve\": \" 've \",\n",
        "#                 \"n't\": \" n't \",\n",
        "#                 \"'re\": \" 're \",\n",
        "#                 \"'d\": \" 'd \",\n",
        "#                 \"'ll\": \" 'll \",\n",
        "#                 \",\": \" , \",\n",
        "#                 \"!\": \" ! \",\n",
        "#                 }\n",
        "#     words = words.lower()\n",
        "#     words = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", words)\n",
        "#     for k, v in REPLACE.items():\n",
        "#             words = words.replace(k, v)\n",
        "#     return [w.strip() for w in words.split()]\n",
        "\n",
        "# BERT_TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# BERT_MODEL = BertModel.from_pretrained('bert-base-uncased')\n",
        "# def bert_tokenizer(words, pretokenizer=whitespace_tokenizer):\n",
        "#     subwords = ['[CLS]']\n",
        "#     for word in pretokenizer(words):\n",
        "#         if word == '<unk>':\n",
        "#             subword = '[UNK]'\n",
        "#         else:\n",
        "#             subword = BERT_TOKENIZER.tokenize(word)\n",
        "#         subwords += subword\n",
        "#     return subwords + ['[SEP]']\n",
        "\n",
        "# def bert_vectorizer(sentence):\n",
        "#     return BERT_TOKENIZER.convert_tokens_to_ids(sentence)\n",
        "#     #return [BERT_TOKENIZER.vocab.get(subword, BERT_TOKENIZER.vocab['[PAD]']) for subword in sentence]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLycXCphwTvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words = \"National Taiwan University\"\n",
        "# print(\"words:\")\n",
        "# print(words)\n",
        "# print()\n",
        "# print(\"BERT_TOKENIZER.tokenize(words):\")\n",
        "# print(BERT_TOKENIZER.tokenize(words))\n",
        "# print()\n",
        "# print(\"bert_tokenizer(words):\")\n",
        "# tokens = bert_tokenizer(words)\n",
        "# print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8CZin74kzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(tokens)\n",
        "# ids = BERT_TOKENIZER.convert_tokens_to_ids(tokens)\n",
        "# print(ids)\n",
        "# print(BERT_TOKENIZER.convert_ids_to_tokens(ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbGlYxDGwIdV",
        "colab_type": "text"
      },
      "source": [
        "### FineTuneClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7nF-N93hmFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class FineTuneClassifier(nn.Module):\n",
        "\n",
        "#     def __init__(self, base_model, num_classes, embed_dim, hidden_units=[]):\n",
        "#         super().__init__()\n",
        "#         self.base_model = base_model\n",
        "#         input_units = embed_dim\n",
        "#         output_units = embed_dim\n",
        "#         sequence = []\n",
        "#         for h in hidden_units:\n",
        "#             sequence.append(nn.Linear(input_units, h))\n",
        "#             input_units = h\n",
        "#             output_units = h\n",
        "            \n",
        "#         sequence.append(nn.Linear(output_units, num_classes))\n",
        "#         self.outputs = nn.Sequential(*sequence)\n",
        "\n",
        "#     def forward(self, inputs):\n",
        "#         x, lengths = inputs\n",
        "        \n",
        "#         input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1)\n",
        "#         input_type_ids = torch.zeros(x.shape, device=x.device, dtype=torch.long)\n",
        "#         _, pooled = self.base_model(x, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
        "        \n",
        "#         stacked = self.outputs(pooled)\n",
        "#         return F.log_softmax(stacked, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04KqbE6RiIvI",
        "colab_type": "text"
      },
      "source": [
        "All the rest of our code comes from the previous sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSltrQlhiNBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from typing import List, Tuple\n",
        "# import os\n",
        "# import io\n",
        "# import re\n",
        "# import codecs\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK-qaONtv7mH",
        "colab_type": "text"
      },
      "source": [
        "### ConfusionMatrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxEsJMosv1dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class ConfusionMatrix:\n",
        "#     \"\"\"Confusion matrix with metrics\n",
        "\n",
        "#     This class accumulates classification output, and tracks it in a confusion matrix.\n",
        "#     Metrics are available that use the confusion matrix\n",
        "#     \"\"\"\n",
        "#     def __init__(self, labels):\n",
        "#         \"\"\"Constructor with input labels\n",
        "\n",
        "#         :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n",
        "#         \"\"\"\n",
        "#         if type(labels) is dict:\n",
        "#             self.labels = []\n",
        "#             for i in range(len(labels)):\n",
        "#                 self.labels.append(labels[i])\n",
        "#         else:\n",
        "#             self.labels = labels\n",
        "#         nc = len(self.labels)\n",
        "#         self._cm = np.zeros((nc, nc), dtype=np.int)\n",
        "\n",
        "#     def add(self, truth, guess):\n",
        "#         \"\"\"Add a single value to the confusion matrix based off `truth` and `guess`\n",
        "\n",
        "#         :param truth: The real `y` value (or ground truth label)\n",
        "#         :param guess: The guess for `y` value (or assertion)\n",
        "#         \"\"\"\n",
        "\n",
        "#         self._cm[truth, guess] += 1\n",
        "\n",
        "#     def __str__(self):\n",
        "#         values = []\n",
        "#         width = max(8, max(len(x) for x in self.labels) + 1)\n",
        "#         for i, label in enumerate([''] + self.labels):\n",
        "#             values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "#         values += ['\\n']\n",
        "#         for i, label in enumerate(self.labels):\n",
        "#             values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "#             for j in range(len(self.labels)):\n",
        "#                 values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\n",
        "#             values += ['\\n']\n",
        "#         values += ['\\n']\n",
        "#         return ''.join(values)\n",
        "\n",
        "#     def save(self, outfile):\n",
        "#         ordered_fieldnames = OrderedDict([(\"labels\", None)] + [(l, None) for l in self.labels])\n",
        "#         with open(outfile, 'w') as f:\n",
        "#             dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)\n",
        "#             dw.writeheader()\n",
        "#             for index, row in enumerate(self._cm):\n",
        "#                 row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n",
        "#                 row_dict.update({\"labels\": self.labels[index]})\n",
        "#                 dw.writerow(row_dict)\n",
        "\n",
        "#     def reset(self):\n",
        "#         \"\"\"Reset the matrix\n",
        "#         \"\"\"\n",
        "#         self._cm *= 0\n",
        "\n",
        "#     def get_correct(self):\n",
        "#         \"\"\"Get the diagonals of the confusion matrix\n",
        "\n",
        "#         :return: (``int``) Number of correct classifications\n",
        "#         \"\"\"\n",
        "#         return self._cm.diagonal().sum()\n",
        "\n",
        "#     def get_total(self):\n",
        "#         \"\"\"Get total classifications\n",
        "\n",
        "#         :return: (``int``) total classifications\n",
        "#         \"\"\"\n",
        "#         return self._cm.sum()\n",
        "\n",
        "#     def get_acc(self):\n",
        "#         \"\"\"Get the accuracy\n",
        "\n",
        "#         :return: (``float``) accuracy\n",
        "#         \"\"\"\n",
        "#         return float(self.get_correct())/self.get_total()\n",
        "\n",
        "#     def get_recall(self):\n",
        "#         \"\"\"Get the recall\n",
        "\n",
        "#         :return: (``float``) recall\n",
        "#         \"\"\"\n",
        "#         total = np.sum(self._cm, axis=1)\n",
        "#         total = (total == 0) + total\n",
        "#         return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "#     def get_support(self):\n",
        "#         return np.sum(self._cm, axis=1)\n",
        "\n",
        "#     def get_precision(self):\n",
        "#         \"\"\"Get the precision\n",
        "#         :return: (``float``) precision\n",
        "#         \"\"\"\n",
        "\n",
        "#         total = np.sum(self._cm, axis=0)\n",
        "#         total = (total == 0) + total\n",
        "#         return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "#     def get_mean_precision(self):\n",
        "#         \"\"\"Get the mean precision across labels\n",
        "\n",
        "#         :return: (``float``) mean precision\n",
        "#         \"\"\"\n",
        "#         return np.mean(self.get_precision())\n",
        "\n",
        "#     def get_weighted_precision(self):\n",
        "#         return np.sum(self.get_precision() * self.get_support())/float(self.get_total())\n",
        "\n",
        "#     def get_mean_recall(self):\n",
        "#         \"\"\"Get the mean recall across labels\n",
        "\n",
        "#         :return: (``float``) mean recall\n",
        "#         \"\"\"\n",
        "#         return np.mean(self.get_recall())\n",
        "\n",
        "#     def get_weighted_recall(self):\n",
        "#         return np.sum(self.get_recall() * self.get_support())/float(self.get_total())\n",
        "\n",
        "#     def get_weighted_f(self, beta=1):\n",
        "#         return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())\n",
        "\n",
        "#     def get_macro_f(self, beta=1):\n",
        "#         \"\"\"Get the macro F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "#         :param beta: (``float``) defaults to 1 (F1)\n",
        "#         :return: (``float``) macro F_b\n",
        "#         \"\"\"\n",
        "#         if beta < 0:\n",
        "#             raise Exception('Beta must be greater than 0')\n",
        "#         return np.mean(self.get_class_f(beta))\n",
        "\n",
        "#     def get_class_f(self, beta=1):\n",
        "#         p = self.get_precision()\n",
        "#         r = self.get_recall()\n",
        "\n",
        "#         b = beta*beta\n",
        "#         d = (b * p + r)\n",
        "#         d = (d == 0) + d\n",
        "\n",
        "#         return (b + 1) * p * r / d\n",
        "\n",
        "#     def get_f(self, beta=1):\n",
        "#         \"\"\"Get 2 class F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "#         :param beta: (``float``) defaults to 1 (F1)\n",
        "#         :return: (``float``) 2-class F_b\n",
        "#         \"\"\"\n",
        "#         p = self.get_precision()[1]\n",
        "#         r = self.get_recall()[1]\n",
        "#         if beta < 0:\n",
        "#             raise Exception('Beta must be greater than 0')\n",
        "#         d = (beta*beta * p + r)\n",
        "#         if d == 0:\n",
        "#             return 0\n",
        "#         return (beta*beta + 1) * p * r / d\n",
        "\n",
        "#     def get_all_metrics(self):\n",
        "#         \"\"\"Make a map of metrics suitable for reporting, keyed by metric name\n",
        "\n",
        "#         :return: (``dict``) Map of metrics keyed by metric names\n",
        "#         \"\"\"\n",
        "#         metrics = {'acc': self.get_acc()}\n",
        "#         # If 2 class, assume second class is positive AKA 1\n",
        "#         if len(self.labels) == 2:\n",
        "#             metrics['precision'] = self.get_precision()[1]\n",
        "#             metrics['recall'] = self.get_recall()[1]\n",
        "#             metrics['f1'] = self.get_f(1)\n",
        "#         else:\n",
        "#             metrics['mean_precision'] = self.get_mean_precision()\n",
        "#             metrics['mean_recall'] = self.get_mean_recall()\n",
        "#             metrics['macro_f1'] = self.get_macro_f(1)\n",
        "#             metrics['weighted_precision'] = self.get_weighted_precision()\n",
        "#             metrics['weighted_recall'] = self.get_weighted_recall()\n",
        "#             metrics['weighted_f1'] = self.get_weighted_f(1)\n",
        "#         return metrics\n",
        "\n",
        "#     def add_batch(self, truth, guess):\n",
        "#         \"\"\"Add a batch of data to the confusion matrix\n",
        "\n",
        "#         :param truth: The truth tensor\n",
        "#         :param guess: The guess tensor\n",
        "#         :return:\n",
        "#         \"\"\"\n",
        "#         for truth_i, guess_i in zip(truth, guess):\n",
        "#             self.add(truth_i, guess_i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNm2y4iuv570",
        "colab_type": "text"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwt3mczrv27L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class Trainer:\n",
        "#     def __init__(self, optimizer: torch.optim.Optimizer):\n",
        "#         self.optimizer = optimizer\n",
        "\n",
        "#     def run(self, model, labels, train, loss, batch_size): \n",
        "#         model.train()       \n",
        "#         train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#         cm = ConfusionMatrix(labels)\n",
        "\n",
        "#         for batch in train_loader:\n",
        "#             loss_value, y_pred, y_actual = self.update(model, loss, batch)\n",
        "#             _, best = y_pred.max(1)\n",
        "#             yt = y_actual.cpu().int().numpy()\n",
        "#             yp = best.cpu().int().numpy()\n",
        "#             cm.add_batch(yt, yp)\n",
        "\n",
        "#         print(cm.get_all_metrics())\n",
        "#         return cm\n",
        "    \n",
        "#     def update(self, model, loss, batch):\n",
        "#         self.optimizer.zero_grad()\n",
        "#         x, lengths, y = batch\n",
        "#         lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "#         x_sorted = x[perm_idx]\n",
        "#         y_sorted = y[perm_idx]\n",
        "#         y_sorted = y_sorted.to('cuda:0')\n",
        "#         inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "#         y_pred = model(inputs)\n",
        "#         loss_value = loss(y_pred, y_sorted)\n",
        "#         loss_value.backward()\n",
        "#         self.optimizer.step()\n",
        "#         return loss_value.item(), y_pred, y_sorted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG822nORwBMt",
        "colab_type": "text"
      },
      "source": [
        "### Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN4fnBDlwAUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class Evaluator:\n",
        "#     def __init__(self):\n",
        "#         pass\n",
        "\n",
        "#     def run(self, model, labels, dataset, batch_size=1):\n",
        "#         model.eval()\n",
        "#         valid_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "#         cm = ConfusionMatrix(labels)\n",
        "#         for batch in valid_loader:\n",
        "#             y_pred, y_actual = self.inference(model, batch)\n",
        "#             _, best = y_pred.max(1)\n",
        "#             yt = y_actual.cpu().int().numpy()\n",
        "#             yp = best.cpu().int().numpy()\n",
        "#             cm.add_batch(yt, yp)\n",
        "#         return cm\n",
        "\n",
        "#     def inference(self, model, batch):\n",
        "#         with torch.no_grad():\n",
        "#             x, lengths, y = batch\n",
        "#             lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "#             x_sorted = x[perm_idx]\n",
        "#             y_sorted = y[perm_idx]\n",
        "#             y_sorted = y_sorted.to('cuda:0')\n",
        "#             inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "#             y_pred = model(inputs)\n",
        "#             return y_pred, y_sorted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuxb2IWZwDNe",
        "colab_type": "text"
      },
      "source": [
        "### fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47u69m1YwC4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# def fit(model, labels, optimizer, loss, epochs, batch_size, train, valid, test):\n",
        "\n",
        "#     trainer = Trainer(optimizer)\n",
        "#     evaluator = Evaluator()\n",
        "#     best_acc = 0.0\n",
        "    \n",
        "#     for epoch in range(epochs):\n",
        "#         print('EPOCH {}'.format(epoch + 1))\n",
        "#         print('=================================')\n",
        "#         print('Training Results')\n",
        "#         cm = trainer.run(model, labels, train, loss, batch_size)\n",
        "#         print('Validation Results')\n",
        "#         cm = evaluator.run(model, labels, valid)\n",
        "#         print(cm.get_all_metrics())\n",
        "#         if cm.get_acc() > best_acc:\n",
        "#             print('New best model {:.2f}'.format(cm.get_acc()))\n",
        "#             best_acc = cm.get_acc()\n",
        "#             torch.save(model.state_dict(), './checkpoint.pth')\n",
        "#     if test:\n",
        "#         model.load_state_dict(torch.load('./checkpoint.pth'))\n",
        "#         cm = evaluator.run(model, labels, test)\n",
        "#         print('Final result')\n",
        "#         print(cm.get_all_metrics())\n",
        "#     return cm.get_acc()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzmBovI1xjoi",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYSvpwOczJBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BASE = 'trec'\n",
        "# TRAIN = os.path.join(BASE, 'trec.nodev.utf8')\n",
        "# VALID = os.path.join(BASE, 'trec.dev.utf8')\n",
        "# TEST = os.path.join(BASE, 'trec.test.utf8')\n",
        "\n",
        "# # lowercase=False so we can defer to BERT's tokenizer to handle\n",
        "# r = Reader((TRAIN, VALID, TEST,), lowercase=False, vectorizer=bert_vectorizer, tokenizer=bert_tokenizer)\n",
        "# train = r.load(TRAIN)\n",
        "# valid = r.load(VALID)\n",
        "# test = r.load(TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHqndauRjmKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_small_dims = 768\n",
        "# batch_size = 50\n",
        "# epochs = 12\n",
        "# model = FineTuneClassifier(BERT_MODEL, len(r.labels), bert_small_dims)\n",
        "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Model has {num_params} parameters\") \n",
        "\n",
        "\n",
        "# model.to('cuda:0')\n",
        "# loss = torch.nn.NLLLoss()\n",
        "# loss = loss.to('cuda:0')\n",
        "\n",
        "# learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)\n",
        "\n",
        "# fit(model, r.labels, optimizer, loss, epochs, batch_size, train, valid, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZH0kgbluKBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZ-D4bfrJxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}