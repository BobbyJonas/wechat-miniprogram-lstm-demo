{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUI-dMnfxKbb"
   },
   "source": [
    "- author: Lee Meng\n",
    "- date: 2019-09-09 08:00\n",
    "- title: 淺談 GPT-2 語言模型並生成金庸武俠小說\n",
    "- slug: use-gpt2-and-pytorch-to-generate-all-chinese-jing-yong-novels\n",
    "- tags: 自然語言處理, NLP, Pytorch\n",
    "- summary: 這篇文章將簡單向讀者介紹 OpenAI 的知名語言模型 GPT-2，並展示能夠生成金庸小說的 GPT-2 模型。文中也將透過視覺化工具 BertViz 來帶讀者直觀了解 Transformer-based 的 NLP 模型背後的注意力機制。讀者也能透過文中提供的 GPT-2 模型自行生成全新的金庸橋段。\n",
    "- description: 這篇文章將簡單向讀者介紹 OpenAI 的知名語言模型 GPT-2，並展示能夠生成金庸小說的 GPT-2 模型。文中也將透過視覺化工具 BertViz 來帶讀者直觀了解 Transformer-based 的 NLP 模型背後的注意力機制。讀者也能透過文中提供的 GPT-2 模型自行生成全新的金庸橋段。\n",
    "- image: godslar-zboO0K1WfY4-unsplash.jpg\n",
    "- image_credit_url: https://unsplash.com/search/photos/china?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 這是一篇以「介紹 GPT-2」之名，行「推坑金庸小說」之實的 NLP 文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嗨，會點擊進來，我想你應該至少看過武俠小說泰斗[金庸](https://zh.wikipedia.org/wiki/金庸)的一部著作吧！\n",
    "\n",
    "這篇文章將簡單介紹 [OpenAI](https://openai.com/) 在今年提出的知名**語言模型** [GPT-2](https://openai.com/blog/better-language-models)，並展示一個能夠用來生成金庸風格文本的小型 GPT-2。文中會附上 [Google Colab](https://colab.research.google.com/) 連結，讓你也能實際利用該模型來生成全新的小說橋段。我們也將透過視覺化工具 [BertViz](https://github.com/jessevig/bertviz) 來直觀地感受 [Transformer-based 的 NLP 模型](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html)是如何利用[注意力機制（Attention Mechanism）](https://www.youtube.com/watch?v=jd9DtlR90ak&feature=youtu.be)來生成文本的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先睹為快：看看 GPT-2 生成的金庸橋段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以把本文想成是[如何用 TensorFlow 2 及 TensorFlow.js 寫天龍八部](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)的升級版本（是的，我太愛金庸所以要寫第兩篇）。本文跟該篇文章的最大差異在於：\n",
    "- **模型升級**：當初我們使用輕量的[長短期記憶 LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 作為語言模型，這篇則使用更新、更強大的 GPT-2\n",
    "- **數據增加**：當初我們只讓 LSTM「閱讀」一本[《天龍八部》](https://bit.ly/2luD3JM)，這篇則用了整整 14 部金庸武俠小說來訓練 GPT-2\n",
    "- **強調概念**：當初我們用 [TensorFlow](https://www.tensorflow.org/) 自己一步步實作 LSTM，這篇則會專注在 GPT-2 的運作原理而非實作細節"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/金庸全作品.jpg\n",
    "- 用來訓練 GPT-2 的金庸武俠小說：第一排由左到右：飛雪連天射白鹿；第二排：笑書神俠倚碧鴛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 飛雪連天射白鹿，笑書神俠倚碧鴛。\n",
    "- 金庸作品首字詩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從《飛狐外傳》、《倚天屠龍記》到《鴛鴦刀》，這 14 部經典的金庸著作你讀過幾本呢？哪一本是你的最愛呢？還記得多少橋段呢？\n",
    "\n",
    "在實際介紹 GPT-2 之前，讓我們先看看將這些作品都「讀」過上百遍的 GPT-2 會生成出怎麼樣的橋段。你可以從底下的這些生成例子感受一下模型的語言能力以及腦補技巧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "Gallery 的 CSS / JS / HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "# 用來產生 sample image html elements\n",
    "# from glob import glob\n",
    "# files = glob(\"images/gpt2/*.jpg\")\n",
    "# files = sorted(files, key=lambda x: int(x.split('_')[0].replace(\"images/gpt2/\", \"\")))\n",
    "# s = '<img class=\"mySlides\" src=\"{}{}\" style=\"width:100%\">'\n",
    "# for f in files:\n",
    "#     print(s.format('{static}', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "<!-- https://www.w3schools.com/w3css/w3css_slideshow.asp -->\n",
    ".w3-content,\n",
    ".w3-auto {\n",
    "\tmargin-left: auto;\n",
    "\tmargin-right: auto\n",
    "}\n",
    "\n",
    ".w3-content {\n",
    "\tmax-width: 980px\n",
    "}\n",
    "\n",
    ".w3-display-container:hover .w3-display-hover {\n",
    "\tdisplay: block\n",
    "}\n",
    "\n",
    ".w3-display-container:hover span.w3-display-hover {\n",
    "\tdisplay: inline-block\n",
    "}\n",
    "\n",
    ".w3-display-container {\n",
    "\tposition: relative\n",
    "}\n",
    "\n",
    ".w3-button:hover {\n",
    "\tcolor: #000!important;\n",
    "\tbackground-color: inherit;\n",
    "}\n",
    "\n",
    ".w3-button {\n",
    "\tborder: none;\n",
    "\tdisplay: inline-block;\n",
    "\tpadding: 3px 3px;\n",
    "\tvertical-align: middle;\n",
    "\toverflow: hidden;\n",
    "\ttext-decoration: none;\n",
    "\tcolor: inherit;\n",
    "\tbackground-color: white;\n",
    "\ttext-align: center;\n",
    "\tcursor: pointer;\n",
    "\twhite-space: nowrap\n",
    "}\n",
    "\n",
    ".w3-button {\n",
    "\t-webkit-touch-callout: none;\n",
    "\t-webkit-user-select: none;\n",
    "\t-khtml-user-select: none;\n",
    "\t-moz-user-select: none;\n",
    "\t-ms-user-select: none;\n",
    "\tuser-select: none;\n",
    "    height: 2.4rem;\n",
    "}\n",
    "\n",
    ".w3-button:disabled {\n",
    "\tcursor: not-allowed;\n",
    "\topacity: 0.3\n",
    "}\n",
    "\n",
    ".w3-display-left {\n",
    "\tposition: absolute;\n",
    "\ttop: 50%;\n",
    "\tleft: 0%;\n",
    "\ttransform: translate(0%, -50%);\n",
    "\t-ms-transform: translate(-0%, -50%)\n",
    "}\n",
    "\n",
    ".w3-display-right {\n",
    "\tposition: absolute;\n",
    "\ttop: 50%;\n",
    "\tright: 0%;\n",
    "\ttransform: translate(0%, -50%);\n",
    "\t-ms-transform: translate(0%, -50%)\n",
    "}\n",
    "    \n",
    ".mySlides {display:none;}\n",
    "</style>\n",
    "\n",
    "<div class=\"w3-content w3-display-container\" style=\"background-color:#404452 !important\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/4_天龍八部.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/1_飛狐外傳.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/2_雪山飛狐.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/3_連城訣.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/5_射雕英雄傳.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/6_白馬嘯西風.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/7_鹿鼎記.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/8_笑傲江湖.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/9_書劍恩仇錄.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/10_神鵰俠侶.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/11_俠客行.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/12_倚天屠龍記.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/13_碧血劍.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides\" src=\"{static}images/gpt2/14_鴛鴦刀.jpg\" style=\"width:100%\">\n",
    " \n",
    "  <button class=\"w3-button w3-black w3-display-left\" onclick=\"plusDivs(-1)\">&#10094;</button>\n",
    "  <button class=\"w3-button w3-black w3-display-right\" onclick=\"plusDivs(1)\">&#10095;</button>\n",
    "</div>\n",
    "<div style=\"text-align:center\">\n",
    "點擊左右箭頭可查看 GPT-2 生成不同金庸著作的橋段\n",
    "</div>  \n",
    "\n",
    "<script>\n",
    "var slideIndex = 1;\n",
    "showDivs(slideIndex);\n",
    "\n",
    "function plusDivs(n) {\n",
    "  showDivs(slideIndex += n);\n",
    "}\n",
    "\n",
    "function showDivs(n) {\n",
    "  var i;\n",
    "  var x = document.getElementsByClassName(\"mySlides\");\n",
    "  if (n > x.length) {slideIndex = 1}\n",
    "  if (n < 1) {slideIndex = x.length}\n",
    "  for (i = 0; i < x.length; i++) {\n",
    "    x[i].style.display = \"none\";  \n",
    "  }\n",
    "  x[slideIndex-1].style.display = \"block\";  \n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "完成 rendering sample gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "這些文本不是我自己瞎掰的。用這 14 本武俠小說訓練完 GPT-2 之後，我從這些小說中隨意抽取一段文字作為**前文脈絡**，接著就讓它自己腦補後續橋段。你可以從左上角得知模型是在生成哪部武俠小說。\n",
    "\n",
    "這些文本當然不完美，但跟[我們當初用 LSTM 生成《天龍八部》](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)生成的結果相比，已有長足進步：\n",
    "- 生成的文本更加通順、語法也顯得更為自然\n",
    "- 記憶能力好，能夠持續生成跟前文相關的文章而不亂跳人物\n",
    "\n",
    "值得一提的是，如果你讀過《天龍八部》，就會知道第一個例子的前文脈絡是段譽與王語嫣墜入井內並兩情相悅的橋段。儘管每次生成的結果會因為隨機抽樣而有所不同，GPT-2 在看了前文後這次為我們生成了一本超級放閃的言情小說，儘管放閃的兩個人貌似跟我們預期地不太一樣。且在該平行時空底下，貌似慕容復也到了井裡（笑\n",
    "\n",
    "你可以比較當初 LSTM 的生成結果來感受 GPT-2 進步了多少："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "Gallery 的 CSS / JS / HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"w3-content w3-display-container\" style=\"background-color:#404452 !important\">\n",
    "    <img class=\"mySlides1\" src=\"{static}images/gpt2/gpt-vs-lstm.jpg\" style=\"width:100%\">\n",
    "    <img class=\"mySlides1\" src=\"{static}images/gpt2/4_天龍八部.jpg\" style=\"width:100%\">\n",
    " \n",
    "  <button class=\"w3-button w3-black w3-display-left\" onclick=\"plusDivs(-1)\">&#10094;</button>\n",
    "  <button class=\"w3-button w3-black w3-display-right\" onclick=\"plusDivs(1)\">&#10095;</button>\n",
    "</div>\n",
    "<div style=\"text-align:center\">\n",
    "點擊左右箭頭切換 LSTM 與 GPT-2 的生成橋段\n",
    "</div>  \n",
    "\n",
    "<script>\n",
    "var slideIndex = 1;\n",
    "showDivs(slideIndex);\n",
    "\n",
    "function plusDivs(n) {\n",
    "  showDivs(slideIndex += n);\n",
    "}\n",
    "\n",
    "function showDivs(n) {\n",
    "  var i;\n",
    "  var x = document.getElementsByClassName(\"mySlides1\");\n",
    "  if (n > x.length) {slideIndex = 1}\n",
    "  if (n < 1) {slideIndex = x.length}\n",
    "  for (i = 0; i < x.length; i++) {\n",
    "    x[i].style.display = \"none\";  \n",
    "  }\n",
    "  x[slideIndex-1].style.display = \"block\";  \n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "完成 rendering sample gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "儘管訓練文本及參數量有所差異，你還是可以比較一下兩者之間的差距。儘管看似很有金庸的架勢，細看會發現 LSTM 的用詞不太自然（什麼是「四具屍體匆匆忙逼」？）。且明明前文脈絡提到的是段譽與王語嫣，LSTM 突然提到南海鱷神、接著馬上跳到虛竹、然後又扯到蕭峰 ... \n",
    "\n",
    "很明顯地，跟當初用詞不順且不斷切換角色的 LSTM 相比，我們的 GPT-2 長進了許多。在看過生成結果以後，讓我們實際聊聊 GPT-2 是個怎麼樣的語言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2：基於 Transformer 的巨大語言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 的前身是 [GPT](https://blog.openai.com/language-unsupervised/)，其全名為 [**G**enerative **P**re-**T**raining](https://openai.com/blog/better-language-models/)。在 [GPT-2 的論文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)裡頭，作者們首先從網路上爬了將近 40 GB，名為 [WebText（開源版）](https://skylion007.github.io/OpenWebTextCorpus/) 的文本數據，並用此龐大文本訓練了數個以 [Transformer](https://arxiv.org/abs/1706.03762) 架構為基底的[語言模型（language model）](https://youtu.be/iWea12EAu6U)，透過這些模型來預測文本的下一個字（word）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- bert/lm-equation.jpg\n",
    "- 給定前 t 個在字典裡的詞彙，語言模型要去估計第 t + 1 個詞彙的機率分佈 P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如今無人不知、無人不曉的神經網路架構 Transformer 在 2017 年由至今已超過 3,000 次引用的論文 [Attention Is All You Need](https://arxiv.org/abs/1706.03762) 提出，是一個不使用循環神經網路、[卷積神經網路](https://demo.leemeng.tw)並完全仰賴[注意力機制](https://www.youtube.com/watch?v=jd9DtlR90ak&feature=youtu.be)的 [Encoder-Decoder 模型](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Encoder-Decoder-%E6%A8%A1%E5%9E%8B-+-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6)。我在[淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html)一文裡已經用了大量動畫帶你理解並實作自注意力機制及 Transformer，這邊就不再贅述了。\n",
    "\n",
    "基本上只要了解 Transformer 架構，你馬上就懂 GPT-2 了。因為該語言模型的本體事實上就是 Transformer 裡的 **Decoder**："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/elmo-bert-gpt2.jpg\n",
    "- GPT-2 與兩知名模型 ELMo 與 BERT 使用的參數量比較\n",
    "- https://youtu.be/UYPa347-DdE?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&t=2967"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更精準地說，GPT-2 使用的 Transformer Decoder 是[原始論文](https://arxiv.org/abs/1706.03762)的[小變形](https://arxiv.org/abs/1801.10198)，但[序列生成（Sequence Generation）](https://youtu.be/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9)的概念是完全相同的。\n",
    "\n",
    "架構本身沒什麼特別。但 GPT-2 之所以這麼出名，是因為它訓練模型時所使用的數據以及參數量都是前所未有的**龐大**：\n",
    "- **訓練數據**：使用從 800 萬個網頁爬來的 40 GB 高品質文本。把金庸 14 部著作全部串起來也不過 50 MB。這是 800 倍金庸著作的數據量大小。想像一下光是要看完這 14 部著作**一遍**所需花費的時間就好。\n",
    "- **模型參數**：15 億參數，是已經相當巨大、擁有 3.4 億參數的 [BERT-Large](https://github.com/google-research/bert) 語言代表模型的 4.5 倍之多。BERT-Large 使用了 24 層 Transformer blocks，GPT-2 則使用了 48 層。\n",
    "\n",
    "這可是有史以來最多參數的語言模型。而獨角獸（unicorns）的形象則是因為當初作者在[論文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)裡嘗試使用 GPT-2 生成文本時，給了它一段關於「住在安地斯山脈，且會說英文的一群獨角獸」作為前文脈絡（context），而 GPT-2 接著生成的結果有模有樣、頭頭是道，也讓所有人都驚呆了："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/gpt2-unicorns.jpg\n",
    "- GPT-2 作者用跟本文生成金庸橋段一樣的方式讓模型生成獨角獸文章\n",
    "- https://openai.com/blog/better-language-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以前往我在[由淺入深的深度學習資源整理](https://leemeng.tw/deep-learning-resources.html)裡就已經介紹過的 [Talk to Transformer](https://talktotransformer.com/) 生成上例的獨角獸文章、復仇者聯盟劇本或是任何其他類型的**英文**文章。\n",
    "\n",
    "我懂，對非英文母語的我們來說，其實很難感受 GPT-2 生成的文章到底有多厲害。這也是為何我決定要使用金庸小說來訓練一個自己的模型介紹 GPT-2，因為這樣你比較能夠實際感受並了解模型生成的文本。[官方釋出的 GPT-2 能夠輸出中文字](https://github.com/openai/gpt-2/issues/31)，但因為大部分文本都是透過 [Reddit](https://www.reddit.com/) 爬下來的英文文章，事實上是沒辦法做到像是本文的中文生成的。\n",
    "\n",
    "另個讓 GPT-2 當初在社群上被熱烈討論的原因是因為論文作者等人在[部落格上展示驚人的生成結果後表示](https://openai.com/blog/better-language-models/)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 因為顧慮到這項技術可能會遭到惡意運用，我們目前並不打算釋出已訓練好的模型。但為了促進研究我們將釋出小規模的模型供研究者實驗。\n",
    "- OpenAI, 2019/02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此言一出，一片譁然。看看隔壁棚以開源為志向的 [Google BERT](https://github.com/google-research/bert)！網路上有人甚至稱 OpenAI 一點都不 open，而是 CloseAI；也有人說 OpenAI 只是為了炒話題，GPT-2 並沒有那麼厲害；當然也有人認為作者們的論文已經有足夠的學術貢獻，並非一定得釋出模型。\n",
    "\n",
    "不過至少目前看來 OpenAI 只是採用相對謹慎的態度在釋出模型。該團隊在今年 2 月將最小的 124M GPT-2 Small（1.2 億參數）與論文一起釋出，並在 5 月釋出 355M 的 GPT-2 Medium，而就在[不久前的 8 月釋出了 774M GPT-2 Large](https://openai.com/blog/gpt-2-6-month-follow-up/)，其模型大小是 1558M GPT-2 完全體的一半。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/gpt2-model-sizes.jpg\n",
    "- 作者們實驗的 4 種不同大小的 GPT-2 模型，774M 版本在 8 月已被釋出\n",
    "- https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一堆人歡欣鼓舞，迫不及待地把玩最新玩具 GPT-2 Large。剛剛提到的 [Talk to Transformer](https://talktotransformer.com) 事實上就已經是在使用最新的 GPT-2 Large 了，手腳很快。\n",
    "\n",
    "其他相關應用多如牛毛。比方說之前介紹過的 [This Waifu Does Not Exist](https://www.thiswaifudoesnotexist.net/) 在使用 GAN 生成動漫頭像的同時也利用 GPT-2 隨機生成一段動漫劇情；而 [TabNine](https://tabnine.com/) 則是一個利用 GPT-2 做智慧 auto-complete 的開發工具，志在讓工程師們減少不必要的打字，甚至推薦更好的寫法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mp4\n",
    "- images/gpt2/tabnine_demo_java_3.mp4\n",
    "- images/gpt2/tabnine_demo_java_3.jpg\n",
    "- TabNine 透過 GPT-2 讓工程師更有效率地開發程式（以 Java 為例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由強大的深度學習模型驅動，可以想像未來如 TabNine 這種會實際影響我們工作、生活的應用會越來越多。而這也是為何你最好花點時間 follow 深度學習以及 AI 發展趨勢。\n",
    "\n",
    "GPT-2 公布時在多個 language modeling 任務取得 SOTA 結果，因此所有人都在引頸期盼著 OpenAI 將最大的 GPT-2 模型釋出。而該團隊也表示[他們會先觀察人們怎麼使用 774M GPT-2，並持續探討開源的可能性](https://openai.com/blog/gpt-2-6-month-follow-up/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/bert-gpt2-researcher.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不過別走開！GPT-2 的故事還沒有到此結束。OpenAI 的目標可不只是一個能生成獨角獸文章的語言模型而已，重頭戲現在才要開始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 論文作者：GPT-2 能做的可不只是生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們前面就已經提過，[GPT](https://blog.openai.com/language-unsupervised/) 的全名是 **G**enerative **P**re-**T**raining。**G**enerative（生成）指的是上節看到的 language modeling，用搜集來的文本讓 GPT-2 預測（生成）下一個字；**P**re-**T**raining 則是最近 NLP 界非常流行的**兩階段**遷移學習的第一階段：預訓練。相關概念我們在[進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)就已經非常詳細地探討過了，但這邊還是簡單說明一下。\n",
    "\n",
    "近年 NLP 界十分流行的兩階段遷移學習指的多是先蒐集大量無標注文本，並以無監督的方式訓練一個**通用** NLP 模型（Unsupervised Learning）。接著再微調（Fine-tune）該模型以符合**特定**任務的需求。常見的 NLP 任務有文章分類、自然語言推論、問答以及閱讀理解等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- bert/bert-intro.jpg\n",
    "- Google 的語言代表模型 BERT 則是 Transformer 中的 Encoder\n",
    "- https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，當初 [OpenAI 提出的 GPT](https://openai.com/blog/language-unsupervised/) 跟 [Google 的語言代表模型 BERT](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html) 都信奉著兩階段遷移學習：利用大量文本訓練出一個**通用**、具有高度自然語言理解能力的 NLP 模型。有了一個這樣的通用模型之後，就能透過簡單微調**同個**模型來解決各式各樣的 NLP 任務，而無需每次都為不同任務設計特定的神經網路架構，省時省力有效率。\n",
    "\n",
    "兩者的差別則在於進行無監督式訓練時選用的訓練目標以及使用的模型有所不同：\n",
    "- GPT 選擇 Transformer 裡的 **Decoder**，訓練目標為一般的語言模型，預測下個字\n",
    "- BERT 選擇 Transformer 裡的 **Encoder**，訓練目標則為克漏字填空以及下句預測\n",
    "\n",
    "不管如何，兩者都使用了 [Transformer](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html) 架構的一部份。而這主要是因為 Transformer 裡頭的自注意力機制（Self-Attention Mechanism）十分有效且相當適合平行運算。以下則是某層 Decoder block 透過自注意力機制處理一段文字的示意圖："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/decoder-block-attention.jpg\n",
    "- 訓練好的 Transformer Decoder 在處理某詞彙時能關注前方相關的其他詞彙，進而融入語境資訊\n",
    "- https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以很輕易地看出 `it` 本身的語意代表著前面出現過的 `robot`，而這是因為你懂得去關注前面相關的詞彙。在給定一段文字時，[傳統詞嵌入（Word Embeddings）](https://youtu.be/kEMJRjEdNzM)的方法很難做到這件事情，但透過自注意力機制，我們可以讓模型學會怎麼「關注」上下文以決定每個詞彙所代表的語意。以上例而言，學好的 GPT-2 可以在看到 `it` 時知道該去關注前面的 `a` 及 `robot` ，並進而調整 `it` 在當下代表的意思（修正該詞彙的 vector representation）。而被修正的 vector representation 也就是所謂的 [Contextual Word Representation](https://youtu.be/S-CspeZ8FHc)。\n",
    "\n",
    "如果你還是無法理解我在胡扯些什麼的話，只要記得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- Transformer 的自注意力機制讓我們可以用更有意義、具備當下語境的方式來表達一段文字裡頭的每個詞彙，進而提升模型的自然語言理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後面章節的一些 GPT-2 視覺化結果或許能讓你更直觀地理解這個概念。\n",
    "\n",
    "GPT-**2** 基本上就是 GPT 加強版：使用的數據及模型大小都直接霸氣地乘上 **10 倍**。有了如此龐大的數據與模型，在做完第一階段的無監督式訓練以後，GPT-2 作者們表示不屑再遵循兩階段遷移學習步驟，直接做 zero-shot learning！這也就意味著直接把 GPT-2 帶上考場，「裸測」它在多個跟 WebText 無關的 NLP 任務上的表現。實驗結果如下： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/gpt2-zero-shot-result.jpg\n",
    "- 由左至右分別為閱讀理解、翻譯、摘要以及問答任務"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乍看之下你可能會覺得這張圖沒什麼。畢竟就算是最大、最右側的 GPT-2 模型（1542M）在多數特定的 NLP 任務上還是比不過專門為該任務設計的神經網路架構。但 GPT-2 的作者們認為他們這篇論文的最大貢獻在於展現了用大量無標註數據訓練巨大語言模型的潛力：越大，越好！除了摘要（Summarization）任務之外，基本上只要模型參數越大，zero-shot 的結果就越好。\n",
    "\n",
    "且因為在模型參數越來越大時，訓練 / 測試集結果仍然持續進步且表現水準相仿，作者們認為就算是最大的 GPT-2 模型也還 underfit 他們爬的 WebText 數據集，還有進步空間。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- gpt2/gpt-underfit.jpg\n",
    "- 不同大小的 GPT-2 在 WebText 上表現\n",
    "- https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令某些研究者興奮的是，這實驗結果隱含的一個可能性是「或許只要用夠多的文本訓練夠大的語言模型，就能讓該模型在沒有任何監督的情況下完成更多 NLP 任務」\n",
    "\n",
    "總而言之，GPT-2 整篇論文的核心思想可以被這樣總結："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 給定越多參數以及越多樣、越大量的文本，無監督訓練一個語言模型或許就可讓該模型具備更強的自然語言理解能力，並在沒有監督的情況下自動學會解決不同類型的 NLP 任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡單講四個字，觸類旁通。現在回頭看看，你從論文標題：[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 應該也能理解 GPT-2 作者當初想要傳達的想法了。他們也希望這些實驗結果能吸引更多研究者往這個方向發展，雖然不是每個人都有能力做這種等級的研究。\n",
    "\n",
    "呼！基本上這節已經包含了所有我認為你需要了解的重要 GPT-2 概念。如果你欲罷不能、想要了解更多，我會在文末附上更多連結供你參考。在下一節，也是最後一節，讓我們再回去稍微瞧瞧文章開頭看到的金庸 GPT-2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一窺 GPT-2 如何生成金庸小說"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得事先聲明的是，因為網路上已有許多 GPT-2 的實作範例，比如說在 Github 上已破萬星的 [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) 以及本文用來訓練生成模型的 [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)，因此本文將專注在說明 GPT-2 的運作原理而非實作細節。有興趣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果你很好學，應該會花很多時間看我貼的 reference\n",
    "just to give a comprehensive list again here:\n",
    "\n",
    "最困擾的就是要從哪部複習開始。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20190817_gpt2_chinese_jinyong_collection.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
