{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2019-03-25 09:00\n",
    "- title: 讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部\n",
    "- slug: how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js\n",
    "- tags: TensorFlow, TensorFlow.js, 自然語言處理\n",
    "- description: TODO\n",
    "- summary: TODO\n",
    "- image: text-generation-cover.jpg\n",
    "- image_credit_url: \n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"{static}tfjs-apps/lstm-text-generation/index.css\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"margin-bottom: 1rem\">\n",
    "    <p>\n",
    "        木婉清轉頭向他，背脊向著南海鱷神，低聲道：「你是世上第一個見到我容貌的男子！」緩緩拉開了面幕。段譽登時全身一震，眼前所見，如新月清暉，如花樹堆雪，一張臉秀麗絕俗。\n",
    "        <br>\n",
    "        <span style=\"float:right;margin-right: 1.5rem\">第四回：崖高人遠</span>\n",
    "        <br>\n",
    "    </p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "[《天龍八部》](https://bit.ly/2TUycBQ)一直是我最喜歡的[金庸著作](https://zh.wikipedia.org/wiki/%E9%87%91%E5%BA%B8%E4%BD%9C%E5%93%81)之一，最近重新翻閱，有很多新的感受。\n",
    "\n",
    "閱讀到一半我突發奇想，決定用[深度學習](https://leemeng.tw/deep-learning-resources.html)來訓練一個能夠生成《天龍八部》的[循環神經網路](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF)。訓練結果還不完美，但我認為已經很有娛樂性質，且有時已經能夠產生令人驚嘆或是捧腹大笑的文章了。\n",
    "\n",
    "因此我決定使用 [Tensorflow.js](https://www.tensorflow.org/js) 將利用 [TensorFlow 2.0](https://www.tensorflow.org/alpha) 訓練出來的模型弄上線，讓你也能實際看看這個 AI 上嗑了什麼藥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/dali-old-castle.jpg\n",
    "- 大理古城一隅，段譽出身之地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 demo 之後，針對那些熟悉 [Python](https://www.python.org/) 或是 [JavaScript](https://www.javascript.com/) 的你，我將簡單地說明如何使用 TensorFlow 以及 Tensorflow.js 來實作這樣的 AI 應用。\n",
    "\n",
    "希望你能從中得到些啟發，並在閱讀本文後實際動手做點自己的東西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo：生成全新的天龍八部橋段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本篇使用一個十分簡單的[長短期記憶 RNN](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E) 來生成文章。在多次「閱讀」天龍八部之後，這個模型可以在給定一段新文本的情況下，逐字產生類似風格的文章。\n",
    "\n",
    "比方說給定書中的一個橋段：\n",
    "\n",
    "```text\n",
    "烏老大偏生要考一考慕容復，說道：「慕容公子，你瞧這不是大大的\n",
    "```\n",
    "\n",
    "模型順著上面的話，接著產生：\n",
    "\n",
    "```text\n",
    "不算？」馬夫人道：「不錯，咱們非要尋死不可。」\n",
    "\n",
    "段譽大喜，說道：「小姑娘，你待我這麼好，鬼鬼祟祟，一切又不聽你的話，你管甚麼老兄弟不相干，我去幫過彥之。」\n",
    "\n",
    "王夫人哼了一聲，說道：「這裏是甚麼話？」段譽道：「不行！你別過來。用真蠻子，我便將這件事了，一大惡人擠在地下，立時便會斃命，那便如何是好？」\n",
    "```\n",
    "\n",
    "模型產生的文章內容本身很ㄎ一ㄤ，惹人發笑，但用詞本身很天龍八部。（至少我自己寫不出這樣的內容）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/antony-xia-522590-unsplash.jpg\n",
    "- 姑蘇慕容家所在的蘇州"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在馬上就讓我們產生一些新的橋段吧！首先將已經訓練好的模型載入你的瀏覽器。依據你的網路速度，載入時間會有所差異。\n",
    "\n",
    "成功載入後只要不重新整理此頁面，你將可以不斷地使用同個模型產生新的文章。\n",
    "\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <button id=\"load-model\" style=\"display:inline-block\">載入模型</button>\n",
    "    <div id=\"app-status\" style=\"display:inline-block\"></div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外你會發現有 2 個可供你調整的參數：\n",
    "\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成長度（字單位）</span>\n",
    "        <input id=\"generate-length\" value=\"150\"></input>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成溫度（隨機度）</span>\n",
    "        <input id=\"temperature\" value=\"0.6\"></input>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "第一次可以直接使用預設值。現在點擊「生成文章」來產生全新的天龍八部橋段："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <button id=\"generate-text\" disabled=\"true\">生成文章</button>\n",
    "        <button id=\"initialize-seed\" disabled=\"true\">重置輸入</button>\n",
    "    </div>\n",
    "</section>\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">起始句子：</span>\n",
    "        <span id=\"text-generation-status\" style=\"display: none\"></span>\n",
    "        <textarea id=\"seed-text\" value=\"\" rows=\"1\" style=\"min-height: 6em\">蕭峯吃了一驚，心想：「哥哥大喜之餘，說話有些忘形了，眼下亂成</textarea>\n",
    "    </div>\n",
    "</section>\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成結果：</span>\n",
    "        <textarea id=\"generated-text\" readonly=\"true\" value=\"\" rows=\"10\"></textarea>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何？希望模型產生的結果有令你會心一笑。它當初可快把我逗死了。\n",
    "\n",
    "現在你可以嘗試幾件事情：\n",
    "- 點**生成文章**來讓模型依據同輸入產生新橋段\n",
    "- 點**重置輸入**來隨機取得一個新的起始句子\n",
    "- 增加模型生成的**文章長度**\n",
    "- 調整**生成溫度**來改變文章的變化性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/chris-rhoads-254898-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成溫度是一個介於 0 到 1 的值，而當溫度越高，模型產生出來的結果越隨機、越不可預測（也就越ㄎㄧㄤ）；而溫度越低，產生的結果就會越像天龍八部原文。優點是真實，但同時字詞的重複性也會提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 機器並沒有情感，只有人類可以賦予事物意義。我們無法讓機器自動找出所謂的最佳生成溫度，因為人的感覺十分主觀：找出你自己覺得最適合的溫度來生成文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你沒有打算深入探討技術細節，那只需要記得在這篇文章裡頭的模型是一個以「字」為單位的語言模型（Character-based Language Model）即可：給定一連串已經出現過的字詞，模型會想辦法去預測出下一個可能出現的字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/raychan-1229841-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，我們並不單純是拿出現機率最高的字出來當結果，這樣太無趣了。\n",
    "\n",
    "每次機器做預測前都會拿著一個包含大量中文字的機率分布 p，在決定要吐出哪個字時，會對該機率分佈 p 做抽樣，從中隨機選出一個字。因此就跟你在上面 demo 看到的一樣，就算輸入句子相同，每次模型仍然可能生成完全不同的文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/max-felner-448887-unsplash.jpg\n",
    "- 抽樣的過程類似擲骰子，儘管有些結果較易出現，你還是有機會骰到豹子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為隨機抽樣的關係，每次模型產生的結果基本上都是獨一無二的。\n",
    "\n",
    "如果你在生成文章的過程中得到什麼有趣的虛擬橋段，都歡迎留言或是透過 SNS 與我分享！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型是怎麼被訓練的\n",
    "\n",
    "在看完 demo 以後，你可能會好奇這個模型是怎麼被訓練出來的。\n",
    "\n",
    "實際的開發流程大致可以分為兩個部分：\n",
    "- [用 TensorFlow 2.0 訓練一個 LSTM 模型](https://www.tensorflow.org/alpha/tutorials/sequences/text_generation)\n",
    "- [使用 TensorFlow.js 部屬該模型](https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation)\n",
    "\n",
    "這些在 TensorFlow 以及 TensorFlow.js 官網都有十分詳細的說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/tf-demo.png\n",
    "- 這篇文章參考了不少 TensorFlow 官網（左）及 TensorFlow.js 線上 demo（右）的程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你也想開發一個類似的應用，閱讀官方教學中你所熟悉的語言版本（Python / JavaScript）是最直接的作法：\n",
    "- [TensorFlow 2.0 Alpha - Text generation with an RNN](https://www.tensorflow.org/alpha/tutorials/sequences/text_generation)\n",
    "- [TensorFlow.js Example: Train LSTM to Generate Text](https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation)\n",
    "\n",
    "因為官方已經有提供能在 [Google Colab](https://colab.research.google.com/) 上使用 GPU [訓練 LSTM 的教學筆記本](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb)，本文便不再另行提供。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/simon-abrams-286276-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，符合以下條件可以讓你更輕鬆地閱讀接下來的內容：\n",
    "- 熟悉 [Python](https://www.python.org/)\n",
    "- 碰過 [Keras](https://keras.io/) 或是 [TensorFlow](https://www.tensorflow.org/)\n",
    "- 具備[機器學習 & 深度學習基礎](https://leemeng.tw/deep-learning-resources.html#courses)\n",
    "- 了解何謂[循環神經網路](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF)以及[長短期記憶](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E)\n",
    "\n",
    "如果你是喜歡先把基礎打好的人，可以先查閱我上面附的這些資源連結。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0 開發"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平常有在接觸深度學習的讀者或許都已經知道，最近 TensorFlow 隆重推出 [2.0 Alpha 預覽版](https://www.tensorflow.org/alpha)，希望透過全新的 API 讓更多人可以輕鬆地開發機器學習以及深度學習應用。\n",
    "\n",
    "當初撰寫本文的其中一個目的，也是想趁著這次大改版來讓自己熟悉 TensorFlow 2.0 的開發方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"resp-container\">\n",
    "    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/TTQQiJ-mHYA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對我來說，TensorFlow 2.0 有幾個重點：\n",
    "- 將 [tf.keras](https://www.tensorflow.org/alpha/guide/keras/overview) 視為其官方高級 API\n",
    "- 方便除錯的 [Eager Execution](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb) 成為預設值\n",
    "- 負責讀取、處理大量數據的 [tf.data](https://www.tensorflow.org/alpha/guide/data_performance) API\n",
    "- 自動幫你建構計算圖的 [tf.function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)\n",
    "\n",
    "在這篇文章裡頭會看到前 3 者。下節列出的程式碼皆在 [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb) 上用最新版本的 TensorFlow 2.0 Nightly 執行。\n",
    "\n",
    "```bash\n",
    "pip install tf-nightly-gpu-2.0-preview\n",
    "```\n",
    "\n",
    "如果有 GPU 則強烈建議安裝 GPU 版本的 TF Nightly，訓練速度跟 CPU 版本可以差到 10 倍以上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度學習專案步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好戲終於登場。\n",
    "\n",
    "如同多數的深度學習專案，要訓練一個以 LSTM 為基礎的語言模型，你大致需要走過以下幾個步驟："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/deep-learning-pj-steps-menglee.jpg\n",
    "- 開發一個 DL 專案時我常用的流程架構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個流程是一個大方向，依據不同情境你可能需要自行調整流程來符合自己的需求。\n",
    "\n",
    "這篇文章會用 TensorFlow 2.0 簡單地帶你走過所有步驟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定義問題及要解決的任務"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明顯地，在訓練模型前首先得確認我們的問題（Problem）以及想要交給機器解決的任務（Task）是什麼。\n",
    "\n",
    "前面已經提過，我們的目標就是要找出一個天龍八部的語言模型（Language Model），讓該模型在被餵進一段文字以後，能吐出類似天龍八部的文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"resp-container\">\n",
    "    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "<center>\n",
    "    十分推薦李宏毅教授講解序列生成的影片\n",
    "    <br>\n",
    "    <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這實際上是一個[序列生成（Sequence Generation）](https://youtu.be/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9)問題，而機器所要解決的任務也變得明確：給定一段文字單位的序列，它要能吐出下一個合理的文字單位。\n",
    "\n",
    "這邊說的文字單位（Token）可以是\n",
    "- 字（Character，如劍、寺、雲）\n",
    "- 詞（Word，如吐蕃、師弟、阿修羅）\n",
    "\n",
    "本文則使用「字」作為一個文字單位。假設有一個天龍八部的句子：\n",
    "\n",
    "```text\n",
    "『六脈神劍經』乃本寺鎮寺之寶，大理段氏武學的至高法要。\n",
    "``` \n",
    "\n",
    "這時候裡頭的每個字包含標點符號都是一個文字單位，整個句子就構成一個文字序列。我們可以擷取一部份的句子：\n",
    "\n",
    "```text\n",
    "『六脈神劍經』乃本寺鎮寺之寶，大理段氏武\n",
    "```\n",
    "\n",
    "接著在訓練模型時要求它讀入這段文字，並預測出原文裡頭下一個出現的字：`學`。\n",
    "\n",
    "一旦訓練完成，就能得到你開頭看到的那個以字為單位的語言模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 準備原始數據、資料清理\n",
    "\n",
    "巧婦難為無米之炊，沒有數據一切免談。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "with open(\"../../raw_text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    num_words = len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/caroline-attwood-243834-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我在網路上自行蒐集了天龍八部原文，做些簡單的數據清理後發現整本小說總共約含 120 萬個中文字，實在是一部曠世巨作。儘管因為版權問題不宜提供下載連結，Google 是你的好朋友。\n",
    "\n",
    "現在假設我們把原文全部存在一個 Python 字串 `text` 裡頭，則部分內容可能如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咱們見敵方人多，不得師父號令，沒敢隨便動手。」左子穆道：「嗯，來了多少人？」干光豪道：「大約七八十人。」左子穆嘿嘿冷笑，道：「七八十人，便想誅滅無量劍了？只怕也沒這麼容易。」\n",
      "\n",
      "龔光傑道：「他們用箭射過來一封信，封皮上寫得好生無禮。」說著將信呈上。\n",
      "\n",
      "左子穆見信封上寫著：「字諭左子穆」五個大字，便不接信，說道：「你拆來瞧瞧。」龔光傑道：「是！」拆開信封，抽出信箋。\n",
      "\n",
      "那少女在段譽耳邊低聲道：\n"
     ]
    }
   ],
   "source": [
    "# 隨意取出第 9505 到 9702 的中文字\n",
    "print(text[9505:9702])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們也可以看看整本小說裡頭包含多少中文字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天龍八部小說共有 1235431 中文字\n",
      "包含了 4330 個獨一無二的字\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set(text))\n",
    "print(\"天龍八部小說共有 {} 中文字\".format(\n",
    "    len(text)))\n",
    "print(\"包含了 {} 個獨一無二的字\".format(\n",
    "    num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如同我們在[寫給所有人的自然語言處理與深度學習入門指南](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)提過的，要將文字數據交給神經網路處理，我們得先做些前處理，把這些中文字對應到一個個的數字（或稱索引）才行。\n",
    "\n",
    "我們可以使用 `tf.keras` 裡頭的 `Tokenizer` 幫我們把整篇小說建立字典，並將同樣的中文字對應到同樣的索引數字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本的中文字序列：\n",
      "\n",
      "['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n",
      "\n",
      "--------------------\n",
      "\n",
      "轉換後的索引序列：\n",
      "\n",
      "[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 初始化一個以字為單位的 Tokenizer\n",
    "tokenizer = tf.keras\\\n",
    "    .preprocessing\\\n",
    "    .text\\\n",
    "    .Tokenizer(\n",
    "        num_words=num_words,\n",
    "        char_level=True,\n",
    "        filters=''\n",
    ")\n",
    "\n",
    "# 讓 tokenizer 讀過天龍八部全文，\n",
    "# 為每個出現的字建立字典並將中文字轉\n",
    "# 成對應的數字索引\n",
    "tokenizer.fit_on_texts(text)\n",
    "text_as_int = tokenizer\\\n",
    "    .texts_to_sequences([text])[0]\n",
    "\n",
    "# 隨機選片段文本方便之後做說明\n",
    "s_idx = 21004\n",
    "e_idx = 21020\n",
    "partial_indices = \\\n",
    "    text_as_int[s_idx:e_idx]\n",
    "partial_texts = [\n",
    "    tokenizer.index_word[idx] \\\n",
    "    for idx in partial_indices\n",
    "]\n",
    "\n",
    "# 渲染結果，可忽略\n",
    "print(\"原本的中文字序列：\")\n",
    "print()\n",
    "print(partial_texts)\n",
    "print()\n",
    "print(\"-\" * 20)\n",
    "print()\n",
    "print(\"轉換後的索引序列：\")\n",
    "print()\n",
    "print(partial_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明顯地，現在整部天龍八部都已經被轉成一個巨大的數字序列，每一個數字代表著一個獨立的中文字。\n",
    "\n",
    "我們可以換個方向再看一次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人類看的中文字   機器看的輸入索引  \n",
      "------------------------------\n",
      "司                557\n",
      "空                371\n",
      "玄                215\n",
      "雙                214\n",
      "掌                135\n",
      "飛                418\n",
      "舞               1209\n",
      "，                  1\n",
      "逼                837\n",
      "得                 25\n",
      "牠               1751\n",
      "無                 49\n",
      "法                147\n",
      "近                537\n",
      "前                111\n",
      "。                  2\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"{0:10}{1:10}\".format(\n",
    "        \"人類看的中文字\", \"機器看的輸入索引\"))\n",
    "print(\"-\" * 30)\n",
    "for idx in partial_indices:\n",
    "    word = tokenizer.index_word[idx]\n",
    "    print(\"{0:10}{1:10}\".format(word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 建立能丟入模型的資料集\n",
    "\n",
    "做完基本的數據前處理以後，我們需要將 `text_as_int` 這個巨大的數字序列轉換成神經網路容易消化的格式與大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小說的中文字數： 1235431 \n",
      " 前 5 個索引： [1639, 148, 3, 3, 280]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"小說的中文字數：\", \n",
    "    len(text_as_int), \n",
    "    \"\\n\",\n",
    "    \"前 5 個索引：\",\n",
    "    text_as_int[:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 在建立資料集時，你要先能想像最終交給模型的數據長什麼樣子。這樣能幫助你對數據做適當的轉換。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依照不同的機器學習任務，你會需要不同格式的資料形式。\n",
    "\n",
    "在本文的序列生成任務裡頭，理想的模型要能依據前文來判斷出下一個中文字。因此我們要丟給模型的輸入為一串代表某些中文字的數字序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際丟給模型的數字序列：\n",
      "[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n",
      "\n",
      "方便我們理解的文本序列：\n",
      "['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"實際丟給模型的數字序列：\")\n",
    "print(partial_indices)\n",
    "print()\n",
    "print(\"方便我們理解的文本序列：\")\n",
    "print(partial_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而模型要給我們的理想輸出應該是向左位移一個字的結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際丟給模型的數字序列：\n",
      "[371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n",
      "\n",
      "方便我們理解的文本序列：\n",
      "['空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"實際丟給模型的數字序列：\")\n",
    "print(partial_indices[1:])\n",
    "print()\n",
    "print(\"方便我們理解的文本序列：\")\n",
    "print(partial_texts[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為什麼是這樣的配對？\n",
    "\n",
    "想一想，一個模型如果可以給我們這樣的輸出，代表它：\n",
    "- 在看到第一個輸入字 `司` 時可以正確輸出 `空`\n",
    "- 在之前看過 `司`，且新輸入為 `空` 的情況下，可以輸出 `玄` \n",
    "- 在之前看過 `司空`，且新輸入為 `玄` 的情況下，可以輸出 `雙`\n",
    "- 在之前看過 `司空玄雙掌飛`，且新輸入為 `舞` 的情況下，可以輸出 `，`\n",
    "\n",
    "當一個語言模型可以做到這樣的事情，就代表它已經掌握了**訓練文本**（此文中則為天龍八部）裡頭用字的統計結構，因此我們可以拿來它來產生新的天龍八部文章。\n",
    "\n",
    "你現在應該也可以很清楚地了解，這個語言模型是專門針對天龍八部的文本所誕生的。畢竟日常生活中，給你 `舞` 這個字，你接 `，` 的機率有多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/niketh-vellanki-202943-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了讓你加深印象，讓我把序列擺直，再次列出模型的輸入以及輸出關係："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "時間點  輸入字    輸入索引   輸出字  輸出索引  \n",
      "----------------------------------------\n",
      "    1    司       557      空      371\n",
      "    2    空       371      玄      215\n",
      "    3    玄       215      雙      214\n",
      "    4    雙       214      掌      135\n",
      "    5    掌       135      飛      418\n",
      "    6    飛       418      舞      1209\n",
      "    7    舞       1209     ，      1\n",
      "    8    ，       1        逼      837\n",
      "    9    逼       837      得      25\n",
      "   10    得       25       牠      1751\n",
      "   11    牠       1751     無      49\n",
      "   12    無       49       法      147\n",
      "   13    法       147      近      537\n",
      "   14    近       537      前      111\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"{0:5}{1:7}{2:7}{3:5}{4:6}\".format(\n",
    "        \"時間點\", \"輸入字\", \n",
    "        \"輸入索引\", \"輸出字\", \n",
    "        \"輸出索引\"))\n",
    "print(\"-\" * 40)\n",
    "for t, idx in enumerate(partial_indices[:14]):\n",
    "    if idx + 1 == len(partial_indices):\n",
    "        break\n",
    "    word = tokenizer.index_word[idx]\n",
    "    next_idx = partial_indices[t + 1]\n",
    "#     print(next_idx)\n",
    "    next_word = tokenizer.index_word[next_idx]\n",
    "    print(\"{0:5}{1:7}{2:9}{3:6}{4:7}\".format(\n",
    "        t + 1,\n",
    "        ' '* 4 + word, \n",
    "        ' '* 5 + str(idx), \n",
    "        ' '* 5 + next_word, \n",
    "        ' '* 6 + str(next_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一列（row）是一個時間點，而\n",
    "- **輸入索引**代表模型在當下時間吃進去的輸入\n",
    "- **輸出索引**則代表我們要模型輸出的結果\n",
    "\n",
    "輸入字 / 輸出字則只是方便我們理解對照，實際上模型只吃數字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們了解一筆輸入 / 輸出該有的數據格式了。兩者皆是一個固定長度的數字序列，而後者是前者往左位移一個數字的結果。\n",
    "\n",
    "但這只是一筆數據（以下說的一筆數據，都隱含了輸入序列以及對應的輸出數字序列）。在有 GPU 的情況下，我們常常會一次丟一批（batch）數據，讓 GPU 可以平行運算，加快訓練速度。\n",
    "\n",
    "現在假設我們想要建構一個一次可以給我們 128 筆長度為 10 的數據的資料集，可以用 `tf.data` 這樣做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方便說明，你可以設成更大的數字\n",
    "# 讓模型從更長的序列預測結果\n",
    "SEQ_LENGTH = 10 # 數字序列長度\n",
    "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
    "\n",
    "# 將 python list 轉成 TensorFlow \n",
    "# 最愛的 Tensor <3\n",
    "characters = tf\\\n",
    "    .data\\\n",
    "    .Dataset\\\n",
    "    .from_tensor_slices(\n",
    "        text_as_int)\n",
    "\n",
    "# 將以被數字序列表示的天龍八部文本\n",
    "# 拆成多個長度為 SEQ_LENGTH 的序列，\n",
    "# 並將最後長度不滿 SEQ_LENGTH 的序列\n",
    "# 捨去\n",
    "sequences = characters\\\n",
    "    .batch(SEQ_LENGTH + 1, \n",
    "           drop_remainder=True)\n",
    "\n",
    "# 一整個資料集所包含的成對輸入/輸出的數量\n",
    "steps_per_epoch = \\\n",
    "    len(text_as_int) // SEQ_LENGTH\n",
    "\n",
    "# 將一個片段的數字序列取頭作為輸入序列\n",
    "# 並向左位移一個字的數據序列作為輸出序列\n",
    "def build_seq_pairs(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# 將每個片段序列分別取頭取尾，建立輸出輸入序列，\n",
    "# 將得到的所有數據隨機打亂順序，\n",
    "# 最後再一次拿出 BATCH_SIZE 筆數據作為\n",
    "# 模型一次訓練步驟的輸入資料\n",
    "ds = sequences\\\n",
    "    .map(build_seq_pairs)\\\n",
    "    .shuffle(steps_per_epoch)\\\n",
    "    .batch(BATCH_SIZE, \n",
    "           drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初次接觸 `tf.data` 的讀者可能覺得很陌生，但在熟悉此 API 以後，你將可以套用類似的方式呼叫 TensorFlow Data API 處理任何文本數據，而不需要每次遇到新資料都得重頭開始寫類似的功能（`batch`、`shuffle` etc）。\n",
    "\n",
    "上面已經有非常多的註解幫助你理解發生了什麼事，但如果你還是想自己動手，可以參考[官方用 TensorFlow 2.0 訓練 LSTM 的 Colab 筆記本](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了 eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "最後產生的資料集物件 `ds` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "雖然我不是酷拉皮卡，具現化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for b_inp, b_tar in ds.take(1):\n",
    "    d = tokenizer.index_word\n",
    "    print(\"起始句子的 batch：\")\n",
    "    print(b_inp, \"\\n\")\n",
    "    print(\"目標句子的 batch：\")\n",
    "    print(b_tar, \"\\n\")\n",
    "    print(\"-\" * 20, \"\\n\")\n",
    "    \n",
    "    print(\"第一個起始句子的索引序列：\")\n",
    "    first_i = b_inp.numpy()[0]\n",
    "    print(first_i, \"\\n\")\n",
    "    print(\"第一個目標句子的索引序列：\")\n",
    "    first_t = b_tar.numpy()[0]\n",
    "    print(first_t, \"\\n\")\n",
    "    print(\"-\" * 20, \"\\n\")\n",
    "    \n",
    "    print(\"第一個起始句子的文本序列：\")\n",
    "    print([d[i] for i in first_i], \"\\n\")\n",
    "    print(\"第一個目標句子的文本序列：\")\n",
    "    print([d[i] for i in first_t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定義能解決問題的函式集\n",
    "\n",
    "\n",
    "!quote\n",
    "- （在深度學習的情境下）給定一連串的文字序列，任何一個可以模擬下一個文字出現機率的神經網路都可以被稱之為語言模型。一個語言模型能夠捕捉到特定語言的統計結構。\n",
    "\n",
    "這邊的重點是，我們期待一個語言模型的「記憶」並不會在每個時間點開始就被重置，而是會不斷地「記住」之前看過了什麼字\n",
    "\n",
    "\n",
    "\n",
    "### 5. 定義評量函式好壞的指標\n",
    "\n",
    "\n",
    "\n",
    "值得一提的是，透過定義這樣的輸入輸出關係，我們將一個沒有正確解答的任務轉換成監督式學習的任務了。\n",
    "\n",
    "\n",
    "\n",
    "### 6. 訓練並選擇出最好的函式\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 7. 將函式 / 模型拿來做預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人跟機器的差別：我們理解世界觀，機器理解統計關係\n",
    "我看十遍也寫不出類似的文章，機器看十遍無法理解天龍八部裡頭眾生的愛恨情仇。\n",
    "\n",
    "\n",
    "向金庸致敬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "以下為 tfjs app 部署用的 js。記得每次 `yarn build` 以後都要使用最新的 js "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"{static}tfjs-apps/lstm-text-generation/dist/lstm-text-generation.03657dc5.js\"></script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
