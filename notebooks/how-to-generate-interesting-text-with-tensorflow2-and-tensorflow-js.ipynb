{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2019-03-25 09:00\n",
    "- title: 讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部\n",
    "- slug: how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js\n",
    "- tags: TensorFlow, TensorFlow.js, 自然語言處理\n",
    "- description: TODO\n",
    "- summary: TODO\n",
    "- image: text-generation-cover.jpg\n",
    "- image_credit_url: \n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"{static}tfjs-apps/lstm-text-generation/index.css\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"margin-bottom: 1rem\">\n",
    "    <p>\n",
    "        木婉清轉頭向他，背脊向著南海鱷神，低聲道：「你是世上第一個見到我容貌的男子！」緩緩拉開了面幕。段譽登時全身一震，眼前所見，如新月清暉，如花樹堆雪，一張臉秀麗絕俗。\n",
    "        <br>\n",
    "        <span style=\"float:right;margin-right: 1.5rem\">第四回：崖高人遠</span>\n",
    "        <br>\n",
    "    </p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "[《天龍八部》](https://bit.ly/2TUycBQ)一直是我最喜歡的[金庸著作](https://zh.wikipedia.org/wiki/%E9%87%91%E5%BA%B8%E4%BD%9C%E5%93%81)之一，最近重新翻閱，有很多新的感受。\n",
    "\n",
    "閱讀到一半我突發奇想，決定用[深度學習](https://leemeng.tw/deep-learning-resources.html)來訓練一個能夠生成《天龍八部》的[循環神經網路](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF)。訓練結果還不完美，但我認為已經很有娛樂性質，且有時已經能夠產生令人驚嘆或是捧腹大笑的文章了。\n",
    "\n",
    "因此我決定使用 [Tensorflow.js](https://www.tensorflow.org/js) 將利用 [TensorFlow 2.0](https://www.tensorflow.org/alpha) 訓練出來的模型弄上線，讓你也能實際看看這個 AI 上嗑了什麼藥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/dali-old-castle.jpg\n",
    "- 大理古城一隅，段譽出身之地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 demo 之後，針對那些熟悉 [Python](https://www.python.org/) 或是 [JavaScript](https://www.javascript.com/) 的你，我將簡單地說明如何使用 TensorFlow 以及 Tensorflow.js 來實作這樣的 AI 應用。\n",
    "\n",
    "希望你能從中得到些啟發，並在閱讀本文後實際動手做點自己的東西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo：生成全新的天龍八部橋段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本篇使用一個十分簡單的[長短期記憶 RNN](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E) 來生成文章。在多次「閱讀」天龍八部之後，這個模型可以在給定一段新文本的情況下，逐字產生類似風格的文章。\n",
    "\n",
    "比方說給定書中的一個橋段：\n",
    "\n",
    "```text\n",
    "烏老大偏生要考一考慕容復，說道：「慕容公子，你瞧這不是大大的\n",
    "```\n",
    "\n",
    "模型順著上面的話，接著產生：\n",
    "\n",
    "```text\n",
    "不算？」馬夫人道：「不錯，咱們非要尋死不可。」\n",
    "\n",
    "段譽大喜，說道：「小姑娘，你待我這麼好，鬼鬼祟祟，一切又不聽你的話，你管甚麼老兄弟不相干，我去幫過彥之。」\n",
    "\n",
    "王夫人哼了一聲，說道：「這裏是甚麼話？」段譽道：「不行！你別過來。用真蠻子，我便將這件事了，一大惡人擠在地下，立時便會斃命，那便如何是好？」\n",
    "```\n",
    "\n",
    "模型產生的文章內容本身很ㄎ一ㄤ，惹人發笑，但用詞本身很天龍八部。（至少我自己寫不出這樣的內容）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/antony-xia-522590-unsplash.jpg\n",
    "- 姑蘇慕容家所在的蘇州"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在馬上就讓我們產生一些新的橋段吧！首先將已經訓練好的模型載入你的瀏覽器。依據你的網路速度，載入時間會有所差異。\n",
    "\n",
    "成功載入後只要不重新整理此頁面，你將可以不斷地使用同個模型產生新的文章。\n",
    "\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <button id=\"load-model\" style=\"display:inline-block\">載入模型</button>\n",
    "    <div id=\"app-status\" style=\"display:inline-block\"></div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外你會發現有 2 個可供你調整的參數：\n",
    "\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成長度（字單位）</span>\n",
    "        <input id=\"generate-length\" value=\"150\"></input>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成溫度（隨機度）</span>\n",
    "        <input id=\"temperature\" value=\"0.6\"></input>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "第一次可以直接使用預設值。現在點擊「生成文章」來產生全新的天龍八部橋段："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <button id=\"generate-text\" disabled=\"true\">生成文章</button>\n",
    "        <button id=\"initialize-seed\" disabled=\"true\">重置輸入</button>\n",
    "    </div>\n",
    "</section>\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">起始句子：</span>\n",
    "        <span id=\"text-generation-status\" style=\"display: none\"></span>\n",
    "        <textarea id=\"seed-text\" value=\"\" rows=\"1\" style=\"min-height: 6em\">蕭峯吃了一驚，心想：「哥哥大喜之餘，說話有些忘形了，眼下亂成</textarea>\n",
    "    </div>\n",
    "</section>\n",
    "<section style=\"margin-bottom: 3rem\">\n",
    "    <div>\n",
    "        <span class=\"input-title\">生成結果：</span>\n",
    "        <textarea id=\"generated-text\" readonly=\"true\" value=\"\" rows=\"10\"></textarea>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何？希望模型產生的結果有令你會心一笑。它當初可快把我逗死了。\n",
    "\n",
    "現在你可以嘗試幾件事情：\n",
    "- 點**生成文章**來讓模型依據同輸入產生新橋段\n",
    "- 點**重置輸入**來隨機取得一個新的起始句子\n",
    "- 增加模型生成的**文章長度**\n",
    "- 調整**生成溫度**來改變文章的變化性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/chris-rhoads-254898-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成溫度是一個介於 0 到 1 的值，而當溫度越高，模型產生出來的結果越隨機、越不可預測（也就越ㄎㄧㄤ）；而溫度越低，產生的結果就會越像天龍八部原文。優點是真實，但同時字詞的重複性也會提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 機器並沒有情感，只有人類可以賦予事物意義。我們無法讓機器自動找出所謂的最佳生成溫度，因為人的感覺十分主觀：找出你自己覺得最適合的溫度來生成文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你沒有打算深入探討技術細節，那只需要記得在這篇文章裡頭的模型是一個以「字」為單位的語言模型（Character-based Language Model）即可：給定一連串已經出現過的字詞，模型會想辦法去預測出下一個可能出現的字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/raychan-1229841-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，我們並不單純是拿出現機率最高的字出來當結果，這樣太無趣了。\n",
    "\n",
    "每次機器做預測前都會拿著一個包含大量中文字的機率分布 p，在決定要吐出哪個字時，會對該機率分佈 p 做抽樣，從中隨機選出一個字。因此就跟你在上面 demo 看到的一樣，就算輸入句子相同，每次模型仍然可能生成完全不同的文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/max-felner-448887-unsplash.jpg\n",
    "- 抽樣的過程類似擲骰子，儘管有些結果較易出現，你還是有機會骰到豹子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為隨機抽樣的關係，每次模型產生的結果基本上都是獨一無二的。\n",
    "\n",
    "如果你在生成文章的過程中得到什麼有趣的虛擬橋段，都歡迎留言或是透過 SNS 與我分享！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型是怎麼被訓練的\n",
    "\n",
    "在看完 demo 以後，你可能會好奇這個模型是怎麼被訓練出來的。\n",
    "\n",
    "實際的開發流程大致可以分為兩個部分：\n",
    "- [用 TensorFlow 2.0 訓練一個 LSTM 模型](https://www.tensorflow.org/alpha/tutorials/sequences/text_generation)\n",
    "- [使用 TensorFlow.js 部屬該模型](https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation)\n",
    "\n",
    "這些在 TensorFlow 以及 TensorFlow.js 官網都有十分詳細的說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/tf-demo.png\n",
    "- 這篇文章參考了不少 TensorFlow 官網（左）及 TensorFlow.js 線上 demo（右）的程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你也想開發一個類似的應用，閱讀官方教學中你所熟悉的語言版本（Python / JavaScript）是最直接的作法：\n",
    "- [TensorFlow 2.0 Alpha - Text generation with an RNN](https://www.tensorflow.org/alpha/tutorials/sequences/text_generation)\n",
    "- [TensorFlow.js Example: Train LSTM to Generate Text](https://github.com/tensorflow/tfjs-examples/tree/master/lstm-text-generation)\n",
    "\n",
    "因為官方已經有提供能在 [Google Colab](https://colab.research.google.com/) 上使用 GPU [訓練 LSTM 的教學筆記本](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb)，本文便不再另行提供。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/simon-abrams-286276-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，符合以下條件可以讓你更輕鬆地閱讀接下來的內容：\n",
    "- 熟悉 [Python](https://www.python.org/)\n",
    "- 碰過 [Keras](https://keras.io/) 或是 [TensorFlow](https://www.tensorflow.org/)\n",
    "- 具備[機器學習 & 深度學習基礎](https://leemeng.tw/deep-learning-resources.html#courses)\n",
    "- 了解何謂[循環神經網路](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF)以及[長短期記憶](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E)\n",
    "\n",
    "如果你是喜歡先把基礎打好的人，可以先查閱我上面附的這些資源連結。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0 開發"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平常有在接觸深度學習的讀者或許都已經知道，最近 TensorFlow 隆重推出 [2.0 Alpha 預覽版](https://www.tensorflow.org/alpha)，希望透過全新的 API 讓更多人可以輕鬆地開發機器學習以及深度學習應用。\n",
    "\n",
    "當初撰寫本文的其中一個目的，也是想趁著這次大改版來讓自己熟悉 TensorFlow 2.0 的開發方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"resp-container\">\n",
    "    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/TTQQiJ-mHYA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對我來說，TensorFlow 2.0 有幾個重點：\n",
    "- 將 [tf.keras](https://www.tensorflow.org/alpha/guide/keras/overview) 視為其官方高級 API\n",
    "- 方便除錯的 [Eager Execution](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb) 成為預設值\n",
    "- 負責讀取、處理大量數據的 [tf.data](https://www.tensorflow.org/alpha/guide/data_performance) API\n",
    "- 自動幫你建構計算圖的 [tf.function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)\n",
    "\n",
    "在這篇文章裡頭會看到前 3 者。下節列出的程式碼皆在 [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb) 上用最新版本的 TensorFlow 2.0 Nightly 執行。\n",
    "\n",
    "```bash\n",
    "pip install tf-nightly-gpu-2.0-preview\n",
    "```\n",
    "\n",
    "如果有 GPU 則強烈建議安裝 GPU 版本的 TF Nightly，訓練速度跟 CPU 版本可以差到 10 倍以上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度學習專案步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好戲終於登場。\n",
    "\n",
    "如同多數的深度學習專案，要訓練一個以 LSTM 為基礎的語言模型，你大致需要走過以下幾個步驟："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/deep-learning-pj-steps-menglee.jpg\n",
    "- 開發一個 DL 專案時我常用的流程架構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個流程是一個大方向，依據不同情境你可能需要自行調整流程來符合自己的需求。\n",
    "\n",
    "這篇文章會用 TensorFlow 2.0 簡單地帶你走過所有步驟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定義問題及要解決的任務"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明顯地，在訓練模型前首先得確認我們的問題（Problem）以及想要交給機器解決的任務（Task）是什麼。\n",
    "\n",
    "前面已經提過，我們的目標就是要找出一個天龍八部的語言模型（Language Model），讓該模型在被餵進一段文字以後，能吐出類似天龍八部的文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"resp-container\">\n",
    "    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "<center>\n",
    "    十分推薦李宏毅教授講解序列生成的影片\n",
    "    <br>\n",
    "    <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這實際上是一個[序列生成（Sequence Generation）](https://youtu.be/f1KUUz7v8g4?list=PLJV_el3uVTsPMxPbjeX7PicgWbY7F8wW9)問題，而機器所要解決的任務也變得明確：給定一段文字單位的序列，它要能吐出下一個合理的文字單位。\n",
    "\n",
    "這邊說的文字單位（Token）可以是\n",
    "- 字（Character，如劍、寺、雲）\n",
    "- 詞（Word，如吐蕃、師弟、阿修羅）\n",
    "\n",
    "本文則使用「字」作為一個文字單位。假設有一個天龍八部的句子：\n",
    "\n",
    "```text\n",
    "『六脈神劍經』乃本寺鎮寺之寶，大理段氏武學的至高法要。\n",
    "``` \n",
    "\n",
    "這時候裡頭的每個字包含標點符號都是一個文字單位，整個句子就構成一個文字序列。我們可以擷取一部份的句子：\n",
    "\n",
    "```text\n",
    "『六脈神劍經』乃本寺鎮寺之寶，大理段氏武\n",
    "```\n",
    "\n",
    "接著在訓練模型時要求它讀入這段文字，並預測出原文裡頭下一個出現的字：`學`。\n",
    "\n",
    "一旦訓練完成，就能得到你開頭看到的那個以字為單位的語言模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 準備原始數據、資料清理\n",
    "\n",
    "巧婦難為無米之炊，沒有數據一切免談。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "import tensorflow as tf\n",
    "import os\n",
    "tf.enable_eager_execution()\n",
    "with open(\"../../raw_text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    num_words = len(set(text))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/caroline-attwood-243834-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我在網路上自行蒐集了天龍八部原文，做些簡單的數據清理後發現整本小說總共約含 120 萬個中文字，實在是一部曠世巨作。儘管因為版權問題不宜提供下載連結，Google 是你的好朋友。\n",
    "\n",
    "現在假設我們把原文全部存在一個 Python 字串 `text` 裡頭，則部分內容可能如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咱們見敵方人多，不得師父號令，沒敢隨便動手。」左子穆道：「嗯，來了多少人？」干光豪道：「大約七八十人。」左子穆嘿嘿冷笑，道：「七八十人，便想誅滅無量劍了？只怕也沒這麼容易。」\n",
      "\n",
      "龔光傑道：「他們用箭射過來一封信，封皮上寫得好生無禮。」說著將信呈上。\n",
      "\n",
      "左子穆見信封上寫著：「字諭左子穆」五個大字，便不接信，說道：「你拆來瞧瞧。」龔光傑道：「是！」拆開信封，抽出信箋。\n",
      "\n",
      "那少女在段譽耳邊低聲道：\n"
     ]
    }
   ],
   "source": [
    "# 隨意取出第 9505 到 9702 的中文字\n",
    "print(text[9505:9702])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們也可以看看整本小說裡頭包含多少中文字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天龍八部小說共有 1235431 中文字\n",
      "包含了 4330 個獨一無二的字\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set(text))\n",
    "print(\"天龍八部小說共有 {} 中文字\".format(\n",
    "    len(text)))\n",
    "print(\"包含了 {} 個獨一無二的字\".format(\n",
    "    num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相較於英文只有 26 個字母，中文字自然是比較多的。\n",
    "\n",
    "如同我們在[寫給所有人的自然語言處理與深度學習入門指南](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)看過的，要將文本數據丟入神經網路，我們得先做些前處理，將這些中文字對應到一個個的數字（或稱索引）才行。\n",
    "\n",
    "我們可以使用 `tf.keras` 裡頭的 `Tokenizer` 幫我們把整篇小說建立字典，並將同樣的中文字對應到同樣的索引數字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本的中文字序列：\n",
      "\n",
      "['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n",
      "\n",
      "--------------------\n",
      "\n",
      "轉換後的索引序列：\n",
      "\n",
      "[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 初始化一個以字為單位的 Tokenizer\n",
    "tokenizer = tf.keras\\\n",
    "    .preprocessing\\\n",
    "    .text\\\n",
    "    .Tokenizer(\n",
    "        num_words=num_words,\n",
    "        char_level=True,\n",
    "        filters=''\n",
    ")\n",
    "\n",
    "# 讓 tokenizer 讀過天龍八部全文，\n",
    "# 為每個出現的字建立字典並將中文字轉\n",
    "# 成對應的數字索引\n",
    "tokenizer.fit_on_texts(text)\n",
    "text_as_int = tokenizer\\\n",
    "    .texts_to_sequences([text])[0]\n",
    "\n",
    "# 隨機選取一個片段文本方便之後做說明\n",
    "s_idx = 21004\n",
    "e_idx = 21020\n",
    "partial_indices = \\\n",
    "    text_as_int[s_idx:e_idx]\n",
    "partial_texts = [\n",
    "    tokenizer.index_word[idx] \\\n",
    "    for idx in partial_indices\n",
    "]\n",
    "\n",
    "# 渲染結果，可忽略\n",
    "print(\"原本的中文字序列：\")\n",
    "print()\n",
    "print(partial_texts)\n",
    "print()\n",
    "print(\"-\" * 20)\n",
    "print()\n",
    "print(\"轉換後的索引序列：\")\n",
    "print()\n",
    "print(partial_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明顯地，現在整部天龍八部都已經被轉成一個巨大的數字序列，每一個數字代表著一個獨立的中文字。\n",
    "\n",
    "我們可以換個方向再看一次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人類看的中文字   機器看的輸入索引  \n",
      "------------------------------\n",
      "司                557\n",
      "空                371\n",
      "玄                215\n",
      "雙                214\n",
      "掌                135\n",
      "飛                418\n",
      "舞               1209\n",
      "，                  1\n",
      "逼                837\n",
      "得                 25\n",
      "牠               1751\n",
      "無                 49\n",
      "法                147\n",
      "近                537\n",
      "前                111\n",
      "。                  2\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"{0:10}{1:10}\".format(\n",
    "        \"人類看的中文字\", \"機器看的輸入索引\"))\n",
    "print(\"-\" * 30)\n",
    "for idx in partial_indices:\n",
    "    word = tokenizer.index_word[idx]\n",
    "    print(\"{0:10}{1:10}\".format(word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 建立能丟入模型的資料集\n",
    "\n",
    "做完基本的數據前處理以後，我們需要將 `text_as_int` 這個巨大的數字序列轉換成神經網路容易消化的格式與大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小說的中文字數： 1235431 \n",
      " 前 5 個索引： [1639, 148, 3, 3, 280]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"小說的中文字數：\", \n",
    "    len(text_as_int), \n",
    "    \"\\n\",\n",
    "    \"前 5 個索引：\",\n",
    "    text_as_int[:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 在建立資料集時，你要先能想像最終交給模型的數據長什麼樣子。這樣能幫助你對數據做適當的轉換。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依照不同的機器學習任務，你會需要不同格式的資料形式。\n",
    "\n",
    "在本文的序列生成任務裡頭，理想的模型要能依據前文來判斷出下一個中文字。因此我們要丟給模型的是一串代表某些中文字的數字序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際丟給模型的數字序列：\n",
      "[557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n",
      "\n",
      "方便我們理解的文本序列：\n",
      "['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"實際丟給模型的數字序列：\")\n",
    "print(partial_indices)\n",
    "print()\n",
    "print(\"方便我們理解的文本序列：\")\n",
    "print(partial_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而模型要給我們的理想輸出應該是向左位移一個字的結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際丟給模型的數字序列：\n",
      "[371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2]\n",
      "\n",
      "方便我們理解的文本序列：\n",
      "['空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"實際丟給模型的數字序列：\")\n",
    "print(partial_indices[1:])\n",
    "print()\n",
    "print(\"方便我們理解的文本序列：\")\n",
    "print(partial_texts[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為什麼是這樣的配對？\n",
    "\n",
    "想一想，一個模型如果可以給我們這樣的輸出，代表它：\n",
    "- 看到第一個輸入字 `司` 時可以正確輸出 `空`\n",
    "- 在之前看過 `司`，且新輸入字為 `空` 的情況下，可以輸出 `玄` \n",
    "- 在之前看過 `司空`，且新輸入字為 `玄` 的情況下，可以輸出 `雙`\n",
    "- 在之前看過 `司空玄雙掌飛`，且新輸入字為 `舞` 的情況下，可以輸出 `，`\n",
    "\n",
    "當一個語言模型可以做到這樣的事情，就代表它已經掌握了**訓練文本**（此文中為天龍八部）裡頭用字的統計結構，因此我們可以用它來產生新的天龍八部文章。\n",
    "\n",
    "你現在應該也可以了解，這個語言模型是專為天龍八部的文本所誕生的。畢竟日常生活中，給你 `舞` 這個字，你接 `，` 的機率有多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/niketh-vellanki-202943-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了讓你加深印象，讓我把序列擺直，再次列出模型的輸入以及輸出關係："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "時間點 輸入字  輸入索引   輸出字  輸出索引  \n",
      "-------------------------------------\n",
      "   1    司     557      空      371\n",
      "   2    空     371      玄      215\n",
      "   3    玄     215      雙      214\n",
      "   4    雙     214      掌      135\n",
      "   5    掌     135      飛      418\n",
      "   6    飛     418      舞      1209\n",
      "   7    舞     1209     ，      1\n",
      "   8    ，     1        逼      837\n",
      "   9    逼     837      得      25\n",
      "  10    得     25       牠      1751\n",
      "  11    牠     1751     無      49\n",
      "  12    無     49       法      147\n",
      "  13    法     147      近      537\n",
      "  14    近     537      前      111\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"{0:4}{1:5}{2:7}{3:5}{4:6}\".format(\n",
    "        \"時間點\", \"輸入字\", \n",
    "        \"輸入索引\", \"輸出字\", \n",
    "        \"輸出索引\"))\n",
    "print(\"-\" * 37)\n",
    "for t, idx in enumerate(partial_indices[:14]):\n",
    "    if idx + 1 == len(partial_indices):\n",
    "        break\n",
    "    word = tokenizer.index_word[idx]\n",
    "    next_idx = partial_indices[t + 1]\n",
    "#     print(next_idx)\n",
    "    next_word = tokenizer.index_word[next_idx]\n",
    "    print(\"{0:4}{1:5}{2:9}{3:4}{4:7}\".format(\n",
    "        t + 1,\n",
    "        ' '* 4 + word, \n",
    "        ' '* 5 + str(idx), \n",
    "        ' '* 5 + next_word, \n",
    "        ' '* 6 + str(next_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一列（row）是一個時間點，而\n",
    "- **輸入索引**代表模型在當下時間吃進去的輸入\n",
    "- **輸出索引**則代表我們要模型輸出的結果\n",
    "\n",
    "輸入字・輸出字則只是方便我們理解對照，實際上模型只吃數字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- nlp-kaggle-intro/pop-zebra-754186-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們了解一筆輸入・輸出該有的數據格式了。兩者皆是一個固定長度的數字序列，而後者是前者往左位移一個數字的結果。\n",
    "\n",
    "但這只是一筆數據（以下說的一筆數據，都隱含了輸入序列以及對應的輸出序列的 2 個數字序列）。\n",
    "\n",
    "在有 GPU 的情況下，我們常常會一次丟一批（batch）數據，讓 GPU 可以平行運算，加快訓練速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/gpu.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在假設我們想要一個資料集，而此資料集可以一次給我們 128 筆長度為 10 的輸入・輸出序列，則我們可以用 `tf.data` 這樣做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方便說明，你可以設成更大的數字\n",
    "# 讓模型從更長的序列預測結果\n",
    "SEQ_LENGTH = 10  # 數字序列長度\n",
    "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
    "\n",
    "# text_as_int 是一個 python list\n",
    "# 我們利用 from_tensor_slices 將其\n",
    "# 轉變成 TensorFlow 最愛的 Tensor <3\n",
    "characters = tf\\\n",
    "    .data\\\n",
    "    .Dataset\\\n",
    "    .from_tensor_slices(\n",
    "        text_as_int)\n",
    "\n",
    "# 將被以數字序列表示的天龍八部文本\n",
    "# 拆成多個長度為 SEQ_LENGTH (10) 的序列\n",
    "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
    "sequences = characters\\\n",
    "    .batch(SEQ_LENGTH + 1, \n",
    "           drop_remainder=True)\n",
    "\n",
    "# 天龍八部全文所包含的成對輸入/輸出的數量\n",
    "steps_per_epoch = \\\n",
    "    len(text_as_int) // SEQ_LENGTH\n",
    "\n",
    "# 將一個片段的數字序列取頭作為輸入序列\n",
    "# 並把向左位移一個字的數據序列作為輸出序列\n",
    "def build_seq_pairs(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# 將每個片段序列分別取頭取尾，\n",
    "# 建立輸入 / 輸出序列，\n",
    "# 再將得到的所有數據隨機打亂順序\n",
    "# 最後再一次拿出 BATCH_SIZE 筆數據作為\n",
    "# 模型一次訓練步驟的所使用的資料\n",
    "ds = sequences\\\n",
    "    .map(build_seq_pairs)\\\n",
    "    .shuffle(steps_per_epoch)\\\n",
    "    .batch(BATCH_SIZE, \n",
    "           drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼不短，且初次接觸 `tf.data` 的讀者可能會覺得很陌生。\n",
    "\n",
    "但內容不難，且在熟悉以後你可以用非常類似的方式呼叫 [TensorFlow Data API](https://www.tensorflow.org/guide/datasets) 來處理**任何**文本數據，不需要每次遇到新的文本都得重頭開始寫類似的功能（`batch`、`shuffle` etc）。\n",
    "\n",
    "上面已經有非常多的註解幫助你理解發生了什麼事，但如果你想自己動手，可以參考[官方用 TensorFlow 2.0 訓練 LSTM 的 Colab 筆記本](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb)。\n",
    "\n",
    "雖然我不是酷拉皮卡，但如果要把上面用 `build_seq_pairs` 函式將一個文本序列拆成輸入・輸出序列的處理具現化的話，大概就像是下面這樣（假設序列長度為 6）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "文本擷取的片段序列     輸入/輸出序列\n",
    "----------------------------------\n",
    "                 -> 烏老大拱手還\n",
    "                 |\n",
    "烏老大拱手還禮 -----\n",
    "                 |\n",
    "                 -> 老大拱手還禮\n",
    "\n",
    "\n",
    "                 -> 星宿派人數遠\n",
    "                 |\n",
    "星宿派人數遠較 -----\n",
    "                 |\n",
    "                 -> 宿派人數遠較\n",
    "\n",
    "\n",
    "                 -> 過不多時，賈\n",
    "                 |\n",
    "過不多時，賈老 -----\n",
    "                 |\n",
    "                 -> 不多時，賈老\n",
    "```                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此為此，我們已經使用 `tf.data` 建立一個可以拿來給模型使用的資料集了。\n",
    "\n",
    "TensorFlow 2.0 預設就是 [Eager Execution](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb)，因此你不再需要使用老朋友 `tf.Session()` 或是 `tf.placeholder` 就能非常直覺地存取數據："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "起始句子的 batch：\n",
      "tf.Tensor(\n",
      "[[  19   31   84 ...  111    2    3]\n",
      " [1454  120    2 ...   10   12    7]\n",
      " [ 313  516  136 ...   41   31   17]\n",
      " ...\n",
      " [   9    5  396 ...    3   24   18]\n",
      " [1063  508  547 ...    3    3  153]\n",
      " [  23   16  107 ...  641    1   14]], shape=(128, 10), dtype=int32) \n",
      "\n",
      "目標句子的 batch：\n",
      "tf.Tensor(\n",
      "[[  31   84 1626 ...    2    3    3]\n",
      " [ 120    2  114 ...   12    7   94]\n",
      " [ 516  136  937 ...   31   17    1]\n",
      " ...\n",
      " [   5  396   83 ...   24   18  301]\n",
      " [ 508  547  146 ...    3  153   62]\n",
      " [  16  107  260 ...    1   14   53]], shape=(128, 10), dtype=int32) \n",
      "\n",
      "-------------------- \n",
      "\n",
      "第一個起始句子的索引序列：\n",
      "[  19   31   84 1626   18  238  125  111    2    3] \n",
      "\n",
      "第一個目標句子的索引序列：\n",
      "[  31   84 1626   18  238  125  111    2    3    3] \n",
      "\n",
      "-------------------- \n",
      "\n",
      "第一個起始句子的文本序列：\n",
      "['來', '到', '兩', '扇', '大', '石', '門', '前', '。', '\\n'] \n",
      "\n",
      "第一個目標句子的文本序列：\n",
      "['到', '兩', '扇', '大', '石', '門', '前', '。', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# print 是用來幫你理解 tf.data.Dataset\n",
    "# 的內容，實際上存取資料集非常簡單\n",
    "# 現在先關注下面的 print 結果\n",
    "for b_inp, b_tar in ds.take(1):\n",
    "    d = tokenizer.index_word\n",
    "    print(\"起始句子的 batch：\")\n",
    "    print(b_inp, \"\\n\")\n",
    "    print(\"目標句子的 batch：\")\n",
    "    print(b_tar, \"\\n\")\n",
    "    print(\"-\" * 20, \"\\n\")\n",
    "    \n",
    "    print(\"第一個起始句子的索引序列：\")\n",
    "    first_i = b_inp.numpy()[0]\n",
    "    print(first_i, \"\\n\")\n",
    "    print(\"第一個目標句子的索引序列：\")\n",
    "    first_t = b_tar.numpy()[0]\n",
    "    print(first_t, \"\\n\")\n",
    "    print(\"-\" * 20, \"\\n\")\n",
    "    \n",
    "    print(\"第一個起始句子的文本序列：\")\n",
    "    print([d[i] for i in first_i])\n",
    "    print()\n",
    "    print(\"第一個目標句子的文本序列：\")\n",
    "    print([d[i] for i in first_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了讓你理解資料集回傳的內容，上面用了不少 `print`。但事實上資料集 `ds` 負責的就是每次吐出 128 筆數據的 Tensor，而每筆數據裡頭包含了長度為 10 的輸入・輸出的數字序列（因此維度為 `(128, 10)`）。\n",
    "\n",
    "去除 `print`，你要從資料集 `ds` 取得一個 batch 非常地簡單："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_inp, b_tar in ds.take(1):\n",
    "    # 蒙多想去哪就去哪\n",
    "    # 想怎麼存取 b_iup, b_tar 都可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定義能解決問題的函式集\n",
    "\n",
    "呼！我們花了不少時間在建構資料集，是時候捲起袖子將這些資料丟入模型了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/How_to_Roll_Up_Sleeves_01.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回想資料集內容，你現在應該已經很清楚我們想要模型解決的問題是什麼了：丟入一個數字序列，模型要能產生包含下個時間點的數字序列，越相似越好。\n",
    "\n",
    "如同我們在 [AI 如何找出你的喵](https://demo.leemeng.tw/)裡頭說過的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!quote\n",
    "- 任何類型的神經網路本質上都是一個映射函數。它們會在內部進行一連串特定的數據轉換步驟，想辦法將給定的輸入數據轉換成指定的輸出形式。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們現在要做的就是定義一個神經網路架構，讓這個神經網路（或稱函式）幫我們把輸入的數字序列轉換成對應的輸出序列。\n",
    "\n",
    "我們期待這個模型具有「記憶」，能考慮以前看過的所有歷史資訊，進而產生最有可能的下個中文字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- nlp-kaggle-intro/rnn-animate.gif\n",
    "- 循環神經網路非常適合處理具有順序關係的數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而在[自然語言處理與深度學習入門指南](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)我們就已經談過，循環神經網路中的 LSTM 模型非常適合拿來做這件事情。\n",
    "\n",
    "因此雖然理論上你可以用任意架構的神經網路如基本的前饋神經網路來解決這個問題，使用 LSTM 是一個相對較適合的選擇。在 TensorFlow 裡頭，使用 [Keras API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras) 建立一個神經網路就像在疊疊樂，是一件很輕鬆的事情："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# 使用 keras 建立一個非常簡單的 LSTM 模型\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 詞嵌入層\n",
    "model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=num_words, \n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        batch_input_shape=[\n",
    "            BATCH_SIZE, None]\n",
    "))\n",
    "\n",
    "# LSTM 層\n",
    "model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "    units=RNN_UNITS, \n",
    "    return_sequences=True, \n",
    "    stateful=True, \n",
    "    recurrent_initializer='glorot_uniform'\n",
    "))\n",
    "\n",
    "# 全連接層\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        num_words))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/model_summary.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊我們建立了一個由詞嵌入層、LSTM 層以及全連接層組成的簡單 LSTM 模型。此模型一次吃 128 筆輸入數據，並產出 128 筆 4330 維度的 Tensor。\n",
    "\n",
    "如果你還記得，4330 是天龍八部裡頭所有出現過的中文字的數目。模型輸出的每個維度值，都可以被視為是跟某個詞出現機率成正比的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- lstm-text-generation/antoine-dautry-428776-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，儘管這個神經網路（或稱映射函數）看起來非常有希望能解決我們的序列生成問題，我們並不僅是建立了 1 個幫我們將輸入序列對應到目標序列的映射函數而已。事實上，我們用 `tf.keras` 定義了一個有接近 1,300 萬參數的函式**集合**（Function set）。\n",
    "\n",
    "這跟你看到一個資料集裡頭的特徵 `x` 跟目標值 `y` 成直線，然後想用 `a * x + b = y` 形式去 fit `y` 的道理是一樣的：你相信 `a * x + b = y` 形式的映射函數能幫你把輸入 `x` 有效地對應到目標 `y`，你只是不知道最佳的參數組合 `(a, b)` 是多少罷了；同理，很多研究結果顯示文中定義的 RNN 與 LSTM 能幫我們很好的 model 序列數據，我們只是還不知道最適合天龍八部的參數組合是什麼而已。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- nlp-kaggle-intro/backpropagation-example.gif\n",
    "- 深度學習中我們常使用梯度下降與反向傳播來從函數集合中找出最好的函數（某個特定參數組合的神經網路架構）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參數 `a` 以及 `b` 有無限多種組合，而每一組 `a` 與 `b` 的組合都對應到一個實際的「函數」，也都能幫你把 `x` 乘上 `a` 倍再加上 `b` 後想辦法去接近目標值 `y`，只是每個函數的表現不一而已。而把這些所有可能的函數放在一起，就是所謂的函數集合。\n",
    "\n",
    "只不過針對 `a * x + b = y` 這個簡單例子，我們可以直接用線性代數從整個函式集合裡頭瞬間找出最佳的函數 `f`（即某個參數的組合）；而我們需要透過[梯度下降（Gradient Descent）](https://zh.wikipedia.org/zh-tw/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95)與[反向傳播算法（Backpropagation）](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)來幫我們在茫茫無垠的函式集合（某種神經網路架構如本文定義的 `model` ）裡頭找出一個好的神經網路（某個 1,300 萬個參數的組合）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定義評量函式好壞的指標\n",
    "\n",
    "\n",
    "\n",
    "值得一提的是，透過定義這樣的輸入輸出關係，我們將一個沒有正確解答的任務轉換成監督式學習的任務了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 訓練並選擇出最好的函式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 將函式 / 模型拿來做預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人跟機器的差別：我們理解世界觀，機器理解統計關係\n",
    "我看十遍也寫不出類似的文章，機器看十遍無法理解天龍八部裡頭眾生的愛恨情仇。\n",
    "\n",
    "\n",
    "向金庸致敬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "以下為 tfjs app 部署用的 js。記得每次 `yarn build` 以後都要使用最新的 js "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"{static}tfjs-apps/lstm-text-generation/dist/lstm-text-generation.03657dc5.js\"></script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
